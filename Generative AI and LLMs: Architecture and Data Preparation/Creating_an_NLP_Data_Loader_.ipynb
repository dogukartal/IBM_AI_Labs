{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogukartal/IBM_AI_Labs/blob/main/Generative%20AI%20and%20LLMs%3A%20Architecture%20and%20Data%20Preparation/Generative%20AI%20Architecture/Creating_an_NLP_Data_Loader_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "566d7dfe-ed8c-47a5-9f34-61042ebba142"
      },
      "source": [
        "# Creating an NLP Data Loader\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq torch==1.13.1\n",
        "!pip install -Uqq torchtext==0.14.1"
      ],
      "metadata": {
        "id": "hztraucQbmBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "013ccc67-8680-4afd-bf10-2ac638ac643b"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqq spacy\n",
        "!pip install -Uqq torchdata==0.5.1\n",
        "!pip install -Uqq portalocker>=2.0.0\n",
        "!python -m spacy download en_core_web_sm -qq\n",
        "!python -m spacy download de_core_news_sm -qq\n",
        "!python -m spacy download fr_core_news_sm -qq"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf7c0862-b1b0-4120-a176-a0d223495f7f",
        "outputId": "a1b9f5fa-adf4-426e-ddb6-320d1e604fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14.1\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "print(torchtext.__version__)"
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530327df-269e-4708-a4c4-b49294b9316f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from typing import Iterable, List\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "\n",
        "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Data Set and Data Loader"
      ],
      "metadata": {
        "id": "1HMbUuvfe-3L"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bd14219-34bc-4b00-b6a9-17a54ba8ccdc",
        "outputId": "501a2950-1ddb-4b16-cc5d-c3786bf865c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Soon we must all face the choice between what is right and what is easy.', \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\"]\n",
            "['You are awesome!', 'Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.']\n",
            "[\"Fame's a fickle friend, Harry.\", 'It is our choices, Harry, that show what we truly are, far more than our abilities.']\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
        "    \"Fame's a fickle friend, Harry.\",\n",
        "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
        "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
        "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
        "    \"You are awesome!\"\n",
        "]\n",
        "\n",
        "# Define a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "# Create an instance of your custom dataset\n",
        "custom_dataset = CustomDataset(sentences)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 2\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for batch in dataloader:\n",
        "    print(batch)"
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer and Custom Collate Function"
      ],
      "metadata": {
        "id": "4GwhxmMkhyln"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "171d6f95-efc9-4921-8847-edeac28c8870",
        "outputId": "abd2cc95-fb4a-4ff3-d5ce-66fc7cac8a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Dataset Length: 6\n",
            "Sample Items:\n",
            "Item 1: tensor([11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
            "        43, 61,  9, 44,  0, 14,  9, 33,  1])\n",
            "Item 2: tensor([35,  6, 16,  3, 38, 40,  0,  8,  1])\n",
            "Item 3: tensor([12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
            "        21,  1])\n",
            "Item 4: tensor([54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1])\n",
            "Item 5: tensor([66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
            "         2, 12, 64, 17, 26, 65,  1])\n",
            "Item 6: tensor([19,  4, 25, 20])\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"If you want to know what a man's like, take a good look at how he treats his inferiors, not his equals.\",\n",
        "    \"Fame's a fickle friend, Harry.\",\n",
        "    \"It is our choices, Harry, that show what we truly are, far more than our abilities.\",\n",
        "    \"Soon we must all face the choice between what is right and what is easy.\",\n",
        "    \"Youth can not know how age thinks and feels. But old men are guilty if they forget what it was to be young.\",\n",
        "    \"You are awesome!\"\n",
        "]\n",
        "\n",
        "# Define a custom data set\n",
        "class CustomDataset(Dataset):\n",
        "  # The constructor takes a list of sentences, a tokenizer function, and a vocabulary as input\n",
        "    def __init__(self, sentences, tokenizer, vocab):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(self.sentences[idx])\n",
        "        # Convert tokens to tensor indices using vocab\n",
        "        tensor_indices = [self.vocab[token] for token in tokens]\n",
        "        return torch.tensor(tensor_indices)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, sentences))\n",
        "\n",
        "# Create an instance of your custom data set\n",
        "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
        "\n",
        "print(\"Custom Dataset Length:\", len(custom_dataset))\n",
        "print(\"Sample Items:\")\n",
        "\n",
        "for i in range(6):\n",
        "    sample_item = custom_dataset[i]\n",
        "    print(f\"Item {i + 1}: {sample_item}\")"
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primary purpose collate function is to prepare and format individual data samples (examples) into batches that can be efficiently processed by machine learning models."
      ],
      "metadata": {
        "id": "ZL4IMnxRh_5s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "362be0dd-f306-4c08-8398-3708a210657e",
        "outputId": "584f9c3f-ce9a-43b2-f65a-98e6ab3065e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0],\n",
            "        [66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
            "          2, 12, 64, 17, 26, 65,  1]])\n",
            "tensor([[11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
            "         43, 61,  9, 44,  0, 14,  9, 33,  1],\n",
            "        [35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
            "tensor([[19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0],\n",
            "        [12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
            "         21,  1]])\n"
          ]
        }
      ],
      "source": [
        "# Create a custom collate function\n",
        "def collate_fn(batch):\n",
        "    # pad_sequence takes a batch of sequences as input and pads them to match the length of the longest sequence\n",
        "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "    return padded_batch\n",
        "\n",
        "# Create an instance of your custom data set\n",
        "custom_dataset = CustomDataset(sentences, tokenizer, vocab)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 2\n",
        "\n",
        "# Create a data loader\n",
        "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "for batch in dataloader:\n",
        "  print(batch)"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3734c82-36a2-4ed2-8b65-11f946fac2bc"
      },
      "source": [
        "When `batch_first=True`, output will be in [batch_size x seq_len] shape, otherwise it will be in [seq_len x batch_size] shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d195b725-3629-4442-8c90-d70e66d8d48f",
        "outputId": "06f0795f-8ec4-4b1f-f2c7-0b0983ab2654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['if', 'you', 'want', 'to', 'know', 'what', 'a', 'man', \"'\", 's', 'like', ',', 'take', 'a', 'good', 'look', 'at', 'how', 'he', 'treats', 'his', 'inferiors', ',', 'not', 'his', 'equals', '.']\n",
            "['fame', \"'\", 's', 'a', 'fickle', 'friend', ',', 'harry', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n",
            "['it', 'is', 'our', 'choices', ',', 'harry', ',', 'that', 'show', 'what', 'we', 'truly', 'are', ',', 'far', 'more', 'than', 'our', 'abilities', '.']\n",
            "['soon', 'we', 'must', 'all', 'face', 'the', 'choice', 'between', 'what', 'is', 'right', 'and', 'what', 'is', 'easy', '.', ',', ',', ',', ',']\n",
            "['youth', 'can', 'not', 'know', 'how', 'age', 'thinks', 'and', 'feels', '.', 'but', 'old', 'men', 'are', 'guilty', 'if', 'they', 'forget', 'what', 'it', 'was', 'to', 'be', 'young', '.']\n",
            "['you', 'are', 'awesome', '!', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']\n"
          ]
        }
      ],
      "source": [
        "def collate_fn(batch):\n",
        "    # pad_sequence takes a batch of sequences as input and pads them to match the length of the longest sequence\n",
        "    padded_batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "    return padded_batch\n",
        "\n",
        "# Create a data loader with the custom collate function with batch_first=True,\n",
        "dataloader = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# Iterate through the data loader\n",
        "for batch in dataloader:\n",
        "    for row in batch:\n",
        "        for idx in row:\n",
        "            words = [vocab.get_itos()[idx] for idx in row]\n",
        "        print(words)\n",
        ""
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e7e9a3c-3f27-4a35-b09c-a0bb66e724f4",
        "outputId": "1d0f723c-0cde-427b-b18b-3bb973d7912e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['if', 'fame']\n",
            "['you', \"'\"]\n",
            "['want', 's']\n",
            "['to', 'a']\n",
            "['know', 'fickle']\n",
            "['what', 'friend']\n",
            "['a', ',']\n",
            "['man', 'harry']\n",
            "[\"'\", '.']\n",
            "['s', ',']\n",
            "['like', ',']\n",
            "[',', ',']\n",
            "['take', ',']\n",
            "['a', ',']\n",
            "['good', ',']\n",
            "['look', ',']\n",
            "['at', ',']\n",
            "['how', ',']\n",
            "['he', ',']\n",
            "['treats', ',']\n",
            "['his', ',']\n",
            "['inferiors', ',']\n",
            "[',', ',']\n",
            "['not', ',']\n",
            "['his', ',']\n",
            "['equals', ',']\n",
            "['.', ',']\n",
            "['it', 'soon']\n",
            "['is', 'we']\n",
            "['our', 'must']\n",
            "['choices', 'all']\n",
            "[',', 'face']\n",
            "['harry', 'the']\n",
            "[',', 'choice']\n",
            "['that', 'between']\n",
            "['show', 'what']\n",
            "['what', 'is']\n",
            "['we', 'right']\n",
            "['truly', 'and']\n",
            "['are', 'what']\n",
            "[',', 'is']\n",
            "['far', 'easy']\n",
            "['more', '.']\n",
            "['than', ',']\n",
            "['our', ',']\n",
            "['abilities', ',']\n",
            "['.', ',']\n",
            "['youth', 'you']\n",
            "['can', 'are']\n",
            "['not', 'awesome']\n",
            "['know', '!']\n",
            "['how', ',']\n",
            "['age', ',']\n",
            "['thinks', ',']\n",
            "['and', ',']\n",
            "['feels', ',']\n",
            "['.', ',']\n",
            "['but', ',']\n",
            "['old', ',']\n",
            "['men', ',']\n",
            "['are', ',']\n",
            "['guilty', ',']\n",
            "['if', ',']\n",
            "['they', ',']\n",
            "['forget', ',']\n",
            "['what', ',']\n",
            "['it', ',']\n",
            "['was', ',']\n",
            "['to', ',']\n",
            "['be', ',']\n",
            "['young', ',']\n",
            "['.', ',']\n"
          ]
        }
      ],
      "source": [
        "# Create a custom collate function\n",
        "def collate_fn_false(batch):\n",
        "    # Pad sequences within the batch to have equal lengths\n",
        "    padded_batch = pad_sequence(batch, padding_value=0)\n",
        "    return padded_batch\n",
        "\n",
        "# Create a data loader with the custom collate function with batch_first=True,\n",
        "dataloader_false = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=collate_fn_false)\n",
        "\n",
        "# Iterate through the data loader\n",
        "for seq in dataloader_false:\n",
        "    for row in seq:\n",
        "        #print(row)\n",
        "        words = [vocab.get_itos()[idx] for idx in row]\n",
        "        print(words)"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bb13fd9-b1ea-4aa8-b169-788b818150e3",
        "outputId": "4db8e245-1f07-4649-ea06-1dd4ddebb29a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
            "         43, 61,  9, 44,  0, 14,  9, 33,  1],\n",
            "        [35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
            "Length of sequences in the batch: 27\n",
            "tensor([[12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
            "         21,  1],\n",
            "        [54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
            "          0,  0]])\n",
            "Length of sequences in the batch: 20\n",
            "tensor([[66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
            "          2, 12, 64, 17, 26, 65,  1],\n",
            "        [19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0]])\n",
            "Length of sequences in the batch: 25\n"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "    print(batch)\n",
        "    print(\"Length of sequences in the batch:\",batch.shape[1])"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Collate Function to Tokenize"
      ],
      "metadata": {
        "id": "rZaZco9jkzeL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb016dd6-e4a6-45ed-8fee-96039a5f4411",
        "outputId": "02b668b9-5484-47b5-997d-9999ce1c89d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[66, 29, 14, 13, 10, 22, 60,  7, 37,  1, 28, 51, 48,  4, 42, 11, 59, 39,\n",
            "          2, 12, 64, 17, 26, 65,  1],\n",
            "        [19,  4, 25, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0]])\n",
            "shape of sample 2\n",
            "tensor([[35,  6, 16,  3, 38, 40,  0,  8,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0],\n",
            "        [12,  5, 15, 31,  0,  8,  0, 57, 53,  2, 18, 62,  4,  0, 36, 49, 56, 15,\n",
            "         21,  1]])\n",
            "shape of sample 2\n",
            "tensor([[54, 18, 50, 23, 34, 58, 30, 27,  2,  5, 52,  7,  2,  5, 32,  1,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [11, 19, 63, 17, 13,  2,  3, 47,  6, 16, 45,  0, 55,  3, 41, 46, 24, 10,\n",
            "         43, 61,  9, 44,  0, 14,  9, 33,  1]])\n",
            "shape of sample 2\n"
          ]
        }
      ],
      "source": [
        "# Define a custom data set\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Tokenize each sample in the batch using the specified tokenizer\n",
        "    tensor_batch = []\n",
        "    for sample in batch:\n",
        "        tokens = tokenizer(sample)\n",
        "        # Convert tokens to vocabulary indices and create a tensor for each sample\n",
        "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
        "\n",
        "    # Pad sequences within the batch to have equal lengths using pad_sequence\n",
        "    # batch_first=True ensures that the tensors have shape (batch_size, max_sequence_length)\n",
        "    padded_batch = pad_sequence(tensor_batch, batch_first=True)\n",
        "\n",
        "    # Return the padded batch\n",
        "    return padded_batch\n",
        "\n",
        "custom_dataset=CustomDataset(sentences)\n",
        "\n",
        "# Create a data loader for the custom dataset\n",
        "dataloader = DataLoader(\n",
        "    dataset=custom_dataset,   # Custom PyTorch Dataset containing your data\n",
        "    batch_size=batch_size,     # Number of samples in each mini-batch\n",
        "    shuffle=True,              # Shuffle the data at the beginning of each epoch\n",
        "    collate_fn=collate_fn      # Custom collate function for processing batches\n",
        ")\n",
        "\n",
        "for batch in dataloader:\n",
        "    print(batch)\n",
        "    print(\"shape of sample\",len(batch))"
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84aac6c-5e0e-4390-8707-b765cb754cb4"
      },
      "source": [
        "## French Texts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "001c9be1-4637-418c-8757-fa8a7cd96bb5"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Ceci est une phrase.\",\n",
        "    \"C'est un autre exemple de phrase.\",\n",
        "    \"Voici une troisième phrase.\",\n",
        "    \"Il fait beau aujourd'hui.\",\n",
        "    \"J'aime beaucoup la cuisine française.\",\n",
        "    \"Quel est ton plat préféré ?\",\n",
        "    \"Je t'adore.\",\n",
        "    \"Bon appétit !\",\n",
        "    \"Je suis en train d'apprendre le français.\",\n",
        "    \"Nous devons partir tôt demain matin.\",\n",
        "    \"Je suis heureux.\",\n",
        "    \"Le film était vraiment captivant !\",\n",
        "    \"Je suis là.\",\n",
        "    \"Je ne sais pas.\",\n",
        "    \"Je suis fatigué après une longue journée de travail.\",\n",
        "    \"Est-ce que tu as des projets pour le week-end ?\",\n",
        "    \"Je vais chez le médecin cet après-midi.\",\n",
        "    \"La musique adoucit les mœurs.\",\n",
        "    \"Je dois acheter du pain et du lait.\",\n",
        "    \"Il y a beaucoup de monde dans cette ville.\",\n",
        "    \"Merci beaucoup !\",\n",
        "    \"Au revoir !\",\n",
        "    \"Je suis ravi de vous rencontrer enfin !\",\n",
        "    \"Les vacances sont toujours trop courtes.\",\n",
        "    \"Je suis en retard.\",\n",
        "    \"Félicitations pour ton nouveau travail !\",\n",
        "    \"Je suis désolé, je ne peux pas venir à la réunion.\",\n",
        "    \"À quelle heure est le prochain train ?\",\n",
        "    \"Bonjour !\",\n",
        "    \"C'est génial !\"\n",
        "]"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_fr(batch):\n",
        "    # Pad sequences within the batch to have equal lengths\n",
        "    tensor_batch=[]\n",
        "    for sample in batch:\n",
        "        tokens = tokenizer(sample)\n",
        "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
        "    padded_batch = pad_sequence(tensor_batch,batch_first=True)\n",
        "    return padded_batch\n",
        "\n",
        "# Build tokenizer\n",
        "tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, corpus))\n",
        "\n",
        "# Sort sentences based on their length\n",
        "sorted_data = sorted(corpus, key=lambda x: len(tokenizer(x)))\n",
        "print(sorted_data)\n",
        "\n",
        "dataloader = DataLoader(sorted_data, batch_size=4, shuffle=False, collate_fn=collate_fn_fr)\n",
        "\n",
        "for batch in dataloader:\n",
        "    print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBkJrWU7mi7a",
        "outputId": "3a1eb1e4-9969-4b17-e0a1-6a0cc13c2677"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bonjour !', 'Bon appétit !', 'Merci beaucoup !', 'Au revoir !', \"Je t'adore.\", 'Je suis heureux.', 'Je suis là.', \"C'est génial !\", 'Ceci est une phrase.', 'Voici une troisième phrase.', \"Il fait beau aujourd'hui.\", 'Je ne sais pas.', 'Je suis en retard.', 'Quel est ton plat préféré ?', 'Le film était vraiment captivant !', 'La musique adoucit les mœurs.', 'Félicitations pour ton nouveau travail !', \"J'aime beaucoup la cuisine française.\", 'Nous devons partir tôt demain matin.', 'Les vacances sont toujours trop courtes.', \"C'est un autre exemple de phrase.\", 'Je vais chez le médecin cet après-midi.', 'Je suis ravi de vous rencontrer enfin !', 'À quelle heure est le prochain train ?', \"Je suis en train d'apprendre le français.\", 'Je dois acheter du pain et du lait.', 'Je suis fatigué après une longue journée de travail.', 'Il y a beaucoup de monde dans cette ville.', 'Est-ce que tu as des projets pour le week-end ?', 'Je suis désolé, je ne peux pas venir à la réunion.']\n",
            "tensor([[ 27,   2,   0],\n",
            "        [ 26,  45,   2],\n",
            "        [ 35,   8,   2],\n",
            "        [ 25, 101,   2]])\n",
            "tensor([[  1, 105,  41,   0],\n",
            "        [  1,   3,  76,   0],\n",
            "        [  1,   3,  82,   0],\n",
            "        [ 11,   4,  74,   2]])\n",
            "tensor([[ 28,   4,  10,   9,   0],\n",
            "        [ 38,  10, 107,   9,   0],\n",
            "        [ 12,  69,  51,  49,   0],\n",
            "        [  1,  16, 103,  17,   0]])\n",
            "tensor([[  1,   3,  14, 100,   0,   0],\n",
            "        [ 37,   4,  19,  92,  95,   7],\n",
            "        [ 33,  71, 122, 117,  52,   2],\n",
            "        [ 32,  85,  42,  80,  87,   0]])\n",
            "tensor([[ 30,  18,  19,  88,  21,   2,   0],\n",
            "        [ 31,  43,   8,  15,  57,  73,   0],\n",
            "        [ 36,  62,  90, 110,  60,  83,   0],\n",
            "        [ 34, 112, 104, 106, 108,  56,   0]])\n",
            "tensor([[ 11,   4, 111,  50,  68,   5,   9,   0],\n",
            "        [  1, 113,  55,   6,  86,  53,  47,   0],\n",
            "        [  1,   3,  98,   5, 116,  99,  66,   2],\n",
            "        [120,  97,  75,   4,   6,  93,  20,   7]])\n",
            "tensor([[  1,   3,  14,  20,  58,  44,   6,  72,   0,   0],\n",
            "        [  1,  63,  40,  13,  89,  67,  13,  79,   0,   0],\n",
            "        [  1,   3,  70,  46,  10,  81,  78,   5,  21,   0],\n",
            "        [ 12, 119,  39,   8,   5,  84,  59,  54, 115,   0]])\n",
            "tensor([[ 29,  24,  96, 109,  48,  61,  94,  18,   6, 118,  23,  65,   7],\n",
            "        [  1,   3,  64,  22,  77,  16,  91,  17, 114, 121,  15, 102,   0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f09b4b68-0d42-4846-939e-0d5730777492"
      },
      "source": [
        "## Data Loader for German-English Translation Task\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5c0a728-d757-49d7-82ca-8d3e66e141a2"
      },
      "outputs": [],
      "source": [
        "multi30k.URL[\"train\"] = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/validation.tar.gz\"\n",
        "\n",
        "# Global variables\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# Create an Iterator\n",
        "data_set = iter(train_iter)"
      ],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdfa4d5a-a736-4a7b-870b-4b97e4492125",
        "outputId": "a88d127c-9e15-454d-9a20-8015e5ddd2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1\n",
            "Source (de): Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            "Target (en): Two young, White males are outside near many bushes.\n",
            "Sample 2\n",
            "Source (de): Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
            "Target (en): Several men in hard hats are operating a giant pulley system.\n",
            "Sample 3\n",
            "Source (de): Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
            "Target (en): A little girl climbing into a wooden playhouse.\n",
            "Sample 4\n",
            "Source (de): Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
            "Target (en): A man in a blue shirt is standing on a ladder cleaning a window.\n",
            "Sample 5\n",
            "Source (de): Zwei Männer stehen am Herd und bereiten Essen zu.\n",
            "Target (en): Two men are at the stove preparing food.\n"
          ]
        }
      ],
      "source": [
        "for n in range(5):\n",
        "    # Getting the next pair of source and target sentences from the training data set\n",
        "    src, tgt = next(data_set)\n",
        "\n",
        "    # Printing the source (German) and target (English) sentences\n",
        "    print(f\"Sample {str(n+1)}\")\n",
        "    print(f\"Source ({SRC_LANGUAGE}): {src}\\nTarget ({TGT_LANGUAGE}): {tgt}\")"
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59a23f6e-f5fb-4415-b975-7d99dd67e0f7"
      },
      "source": [
        "### Tokenizer setup\n",
        "\n",
        "The tokenizer, set up using spaCy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c78a1246-21a7-4b07-b217-0ed84e2a5d17"
      },
      "outputs": [],
      "source": [
        "# Making a placeholder dict to store both tokenizers\n",
        "token_transform = {}\n",
        "\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
      ],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a497800-e1a8-49b6-9b6f-8d417c046607",
        "outputId": "0f2edb82-74bb-4e36-8fd1-ded4852e1744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source German (de): Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
            "Target English  (en): A man in green holds a guitar while the other man observes his shirt.\n"
          ]
        }
      ],
      "source": [
        "german, english = next(data_set)\n",
        "print(f\"Source German ({SRC_LANGUAGE}): {german}\\nTarget English  ({TGT_LANGUAGE}): { english }\")"
      ],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "token_transform['de'](german)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ohOkSgiqeCl",
        "outputId": "96cebb65-a872-432b-ac0d-d24a51f5a8c1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ein',\n",
              " 'Mann',\n",
              " 'in',\n",
              " 'grün',\n",
              " 'hält',\n",
              " 'eine',\n",
              " 'Gitarre',\n",
              " ',',\n",
              " 'während',\n",
              " 'der',\n",
              " 'andere',\n",
              " 'Mann',\n",
              " 'sein',\n",
              " 'Hemd',\n",
              " 'ansieht',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_transform['en'](english)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S9WjjvBqcGO",
        "outputId": "b0d9faa6-b1ea-4b93-bd0c-9908db046238"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'man',\n",
              " 'in',\n",
              " 'green',\n",
              " 'holds',\n",
              " 'a',\n",
              " 'guitar',\n",
              " 'while',\n",
              " 'the',\n",
              " 'other',\n",
              " 'man',\n",
              " 'observes',\n",
              " 'his',\n",
              " 'shirt',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Special Tokens"
      ],
      "metadata": {
        "id": "vJ3Iq1F_qvc3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7a84b47-9855-464b-9f79-9137fad3b62d"
      },
      "source": [
        "1. `<unk>`: This token represents \"unknown\" or \"out-of-vocabulary\" words. It is used when a word in the input text is not found in the vocabulary or when dealing with words that are rare or unseen during training. When the model encounters an unknown word, it replaces it with the `<unk>` token.\n",
        "\n",
        "2. `<pad>`: This token represents padding. In sequences of text data, such as sentences or documents, sequences may have different lengths. To create batches of data with uniform dimensions, shorter sequences are often padded with this `<pad>` token to match the length of the longest sequence in the batch.\n",
        "\n",
        "3. `<bos>`: This token represents the \"beginning of sequence.\" It is used to indicate the start of a sentence or sequence of tokens. It helps the model understand the beginning of a text sequence.\n",
        "\n",
        "4. `<eos>`: This token represents the \"end of sequence.\" It is used to indicate the end of a sentence or sequence of tokens. It helps the model recognize the end of a text sequence.\n",
        "\n",
        " Define special symbols and indices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "448a53ad-b770-4f15-8698-4f1f4515b728"
      },
      "outputs": [],
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d831588a-dc84-4fa8-bd40-3f7197c641b0"
      },
      "source": [
        "## Tokens to Indices Transformation (Vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6716a96-17e8-4540-9d30-c4f4b9f6a159"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    # Define a mapping to associate the source and target languages\n",
        "    # with their respective positions in the data samples.\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    # Iterate over each data sample in the provided dataset iterator\n",
        "    for data_sample in data_iter:\n",
        "        # Tokenize the data sample corresponding to the specified language\n",
        "        # and yield the resulting tokens.\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "vocab_transform = {}\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data iterator\n",
        "    train_iterator = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    #To decrease the number of padding tokens, you sort data on the source length to batch similar-length sequences together\n",
        "    sorted_dataset = sorted(train_iterator, key=lambda x: len(x[0].split()))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(sorted_dataset, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)"
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6162114d-4b3e-4c51-a062-2eb06523102a"
      },
      "outputs": [],
      "source": [
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1e02c57-807c-40d3-85da-bf3e6a66a4d6",
        "outputId": "fbb5afc3-c80a-46d0-d410-b22b38f58144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English text string: A man in green holds a guitar while the other man observes his shirt.\n",
            " English sequence: [6, 12, 7, 51, 144, 4, 126, 29, 8, 75, 12, 1748, 27, 23, 5]\n",
            "German text string: Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
            " German sequence: [5, 12, 7, 657, 39, 18, 133, 8, 37, 16, 105, 12, 136, 41, 1779, 4]\n"
          ]
        }
      ],
      "source": [
        "seq_en=vocab_transform['en'](token_transform['en'](english))\n",
        "print(f\"English text string: {english}\\n English sequence: {seq_en}\")\n",
        "\n",
        "seq_de=vocab_transform['de'](token_transform['de'](german))\n",
        "print(f\"German text string: {german}\\n German sequence: {seq_de}\")\n"
      ],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5185e0a-d4c3-418f-8a8c-661d21ac9a54"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7afe60ee-be57-4a28-a2c6-355e47fb4b4e"
      },
      "outputs": [],
      "source": [
        "# function to add BOS/EOS, flip source sentence and create tensor for input sequence indices\n",
        "def tensor_transform_s(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.flip(torch.tensor(token_ids), dims=(0,)),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices (LSTM to perform better)\n",
        "def tensor_transform_t(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))"
      ],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7714d9a-0d1c-4e13-95dc-83fd3cb4b0e2",
        "outputId": "9eab446c-5fa2-48a1-e83e-344ccae6f3ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,    5,   23,   27, 1748,   12,   75,    8,   29,  126,    4,  144,\n",
              "          51,    7,   12,    6,    3])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "seq_en=tensor_transform_s(seq_en)\n",
        "seq_en"
      ],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b362e0e5-1d55-4aea-b6f8-78ae9ab28d33",
        "outputId": "7df927ca-5ae7-4aa3-9c79-9baba0a86df5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,    5,   12,    7,  657,   39,   18,  133,    8,   37,   16,  105,\n",
              "          12,  136,   41, 1779,    4,    3])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "seq_de=tensor_transform_t(seq_de)\n",
        "seq_de"
      ],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10b755dd-7b61-4a19-9641-6ec5a4711fb7"
      },
      "outputs": [],
      "source": [
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "\n",
        "text_transform[SRC_LANGUAGE] = sequential_transforms(token_transform[SRC_LANGUAGE], #Tokenization\n",
        "                                            vocab_transform[SRC_LANGUAGE], #Numericalization\n",
        "                                            tensor_transform_s) # Add BOS/EOS and create tensor\n",
        "\n",
        "text_transform[TGT_LANGUAGE] = sequential_transforms(token_transform[TGT_LANGUAGE], #Tokenization\n",
        "                                            vocab_transform[TGT_LANGUAGE], #Numericalization\n",
        "                                            tensor_transform_t) # Add BOS/EOS and create tensor\n"
      ],
      "execution_count": 39
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "204b086f-6eb7-4f29-976b-eded61eec4ed"
      },
      "source": [
        "## Processing Data in Batches Using Collate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8010ab2d-1c43-4cfe-aec2-2134354a893b"
      },
      "outputs": [],
      "source": [
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_sequences = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
        "        src_sequences = torch.tensor(src_sequences, dtype=torch.int64)\n",
        "        tgt_sequences = text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\"))\n",
        "        tgt_sequences = torch.tensor(tgt_sequences, dtype=torch.int64)\n",
        "        src_batch.append(src_sequences)\n",
        "        tgt_batch.append(tgt_sequences)\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX,batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX,batch_first=True)\n",
        "\n",
        "    return src_batch.to(device), tgt_batch.to(device)"
      ],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81c58104-af75-4865-a229-1bc059cc6b25",
        "outputId": "4abb1b11-1698-4ea3-d220-f52b1dd45142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-7923683dec91>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  src_sequences = torch.tensor(src_sequences, dtype=torch.int64)\n",
            "<ipython-input-40-7923683dec91>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tgt_sequences = torch.tensor(tgt_sequences, dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    2,     3,     1,     1,     1],\n",
              "         [    2,  5510,     3,     1,     1],\n",
              "         [    2,  5510,     3,     1,     1],\n",
              "         [    2,  1701,     8, 12642,     3]], device='cuda:0'),\n",
              " tensor([[   2,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
              "         [   2, 6650, 4623,  259,  172, 9953,  115,  692, 3428,    5,    3],\n",
              "         [   2,  216,  110, 3913, 1650, 3823,   71, 2808, 2187,    5,    3],\n",
              "         [   2,    6, 3398,  202,  109,   37,    3,    1,    1,    1,    1]],\n",
              "        device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_iterator = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "sorted_train_iterator = sorted(train_iterator, key=lambda x: len(x[0].split()))\n",
        "train_dataloader = DataLoader(sorted_train_iterator, batch_size=BATCH_SIZE, collate_fn=collate_fn,drop_last=True)\n",
        "\n",
        "valid_iterator = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "sorted_valid_dataloader = sorted(valid_iterator, key=lambda x: len(x[0].split()))\n",
        "valid_dataloader = DataLoader(sorted_valid_dataloader, batch_size=BATCH_SIZE, collate_fn=collate_fn,drop_last=True)\n",
        "\n",
        "\n",
        "src, trg = next(iter(train_dataloader))\n",
        "src,trg"
      ],
      "execution_count": 41
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "prev_pub_hash": "8c22317e777527276102edc9a5fee16b619deefd7fe94e10f36fb454230c542e",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
