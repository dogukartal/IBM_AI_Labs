{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogukartal/IBM_AI_Labs/blob/main/Generative%20AI%20Language%20Modeling%20with%20Transformers/Data_Loading_and_Text_Processing_for_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "183a8f7e-4a83-42df-af24-2e5a1121a876"
      },
      "source": [
        "# **Data Loading and Text Processing for BERT**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f995e46-bebb-4e8a-87ae-1e2c77eafa2f"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1913a32-c3f8-43f4-9bd0-1d8e0146c672"
      },
      "outputs": [],
      "source": [
        "! pip install -qq torch==2.1.0\n",
        "! pip install -qq portalocker>=2.0.0\n",
        "! pip install -qq torchtext==0.16.0\n",
        "! pip install transformers==4.39.1\n",
        "! pip install pandas"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f016016b-651e-4c95-9d0c-bcbc0d075bec"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.vocab import Vocab\n",
        "from torch import Tensor\n",
        "from torch.nn import Transformer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from itertools import chain\n",
        "from itertools import islice\n",
        "from torchtext.datasets import IMDB\n",
        "from copy import deepcopy\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4338a2cd-bd24-4bb6-b2a4-f6e99037aa28"
      },
      "source": [
        "## Tokenization and Vocabulary Building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bebbed2e-d057-432b-aef3-3d0102ff969b"
      },
      "source": [
        "### Tokenization\n",
        "- The `tokenizer` is initialized to tokenize text using basic English tokenization rules, converting text samples into lists of tokens.\n",
        "- `yield_tokens` is a generator function that iterates through the data, yielding tokenized versions of the text samples. This function facilitates vocabulary building by providing a stream of tokens.\n",
        "- `word_dict` defines special tokens used in text processing, such as padding `[PAD]`, class `[CLS]`, separator `[SEP]`, mask `[MASK]`, and unknown `[UNK]` tokens, with their corresponding indices.\n",
        "- Special symbols and their indices are explicitly defined for clarity and used throughout data preparation.\n",
        "- `text_to_index` and `index_to_en` functions are utility converters. The former converts text into a list of numerical indices based on the vocabulary, and the latter reverses this process, translating a sequence of indices back into readable English text.\n",
        "\n",
        "- **`CLS` (Classification Token)**: This token serves as the Start of Sentence (SOS) marker. It represents the overall meaning of the entire sentence. Commonly used in tasks that require understanding the entire input, like classification.\n",
        "\n",
        "- **`SEP` (Separator Token)**: Used as the End of Sentence (EOS) marker. It also acts as a delimiter in scenarios where a model needs to understand and differentiate between multiple sentences, like in question-answering or sentence-pair tasks.\n",
        "\n",
        "- **`PAD` (Padding Token)**: This token is added to sequences to ensure all inputs are of equal length. During training, it's important to note that the `[PAD]` token, typically with an ID of 0, does not contribute to the gradient calculations.\n",
        "\n",
        "- **`MASK` (Masked Token)**: Utilized for word replacement in tasks like masked language modeling. It allows models to predict the identity of masked-out words, facilitating learning of bidirectional representations.\n",
        "\n",
        "- **`UNK` (Unknown Token)**: Acts as a placeholder for words that are not found in the tokenizer's vocabulary. This token replaces any unknown or out-of-vocabulary item in the input data.\n",
        "\n",
        "These components are foundational for preprocessing text data, ensuring it is in the correct format for model training, including tokenization, numerical conversion, and handling special tokens necessary for models like BERT.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dd4445e-53a7-4396-8de7-d4a729036352"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for label, data_sample in data_iter:\n",
        "        yield tokenizer(data_sample)\n",
        "\n",
        "# Define special symbols and indices\n",
        "PAD_IDX, CLS_IDX, SEP_IDX, MASK_IDX,UNK_IDX = 0, 1, 2, 3, 4\n",
        "\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['[PAD]','[CLS]', '[SEP]','[MASK]','[UNK]']"
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20f9f68d-f5b8-48f8-aa0e-54eadca94b11"
      },
      "source": [
        "### Vocabulary building\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91958ab9-21e9-486d-99b9-044723da1670",
        "outputId": "d5ea6a6e-8660-464f-e501-5b90b3ee7c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'would', 'put', 'this', 'at', 'the', 'top', 'of', 'my', 'list', 'of', 'films', 'in', 'the', 'category', 'of', 'unwatchable', 'trash', '!', 'there']\n"
          ]
        }
      ],
      "source": [
        "# Data Splits\n",
        "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "all_data_iter = chain(train_iter, test_iter)\n",
        "fifth_item_tokens = next(islice(yield_tokens(all_data_iter), 5, None))\n",
        "print(fifth_item_tokens[:20])"
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "871353ae-7d30-462a-aa13-747b0901d815",
        "outputId": "9208c33a-b419-4c20-d1e7-e44691534125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147150\n"
          ]
        }
      ],
      "source": [
        "vocab = build_vocab_from_iterator(yield_tokens(all_data_iter),specials=special_symbols,special_first=True)\n",
        "\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "VOCAB_SIZE=len(vocab)\n",
        "print(VOCAB_SIZE)"
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcece581-c3ba-4987-8760-c039bb230075"
      },
      "outputs": [],
      "source": [
        "# Functions that transform token indices to token texts and the opposite\n",
        "text_to_index = lambda text: [vocab(token) for token in tokenizer(text)]\n",
        "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d4d26ed-6a3d-429b-bf7b-eb52b9134e3a",
        "outputId": "492c1238-b633-426b-91f5-496e2014b7b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". i speculative\n",
            "[16, 12, 149, 119, 11363, 117, 22, 928, 1047, 6, 1251, 7, 96, 42, 99, 12, 30, 1877, 6]\n"
          ]
        }
      ],
      "source": [
        "# Check the mappings\n",
        "seq_en = [0, 1, 2, 3, 4, 5, 6]  # Example input sequence\n",
        "english_sentence = index_to_en(seq_en)\n",
        "seq2=[6,16,26131]\n",
        "english_sentence = index_to_en(seq2)\n",
        "\n",
        "print(english_sentence)\n",
        "\n",
        "text = \"I've seen R-rated films with male nudity. Nowhere, because they don't exist.\"  # Example input text\n",
        "text_to_index = lambda text: [vocab[token] for token in tokenizer(text)]\n",
        "index_sequence = text_to_index(text)\n",
        "\n",
        "print(index_sequence)"
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76196764-1901-4b70-9986-8db92f4f9ea9"
      },
      "source": [
        "## Text Masking and Data Preparation for BERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528bd28b-faa1-41e1-b804-42d57ee4ae57"
      },
      "source": [
        "### Text masking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fb1bde0-6ee8-4960-973a-3acd5085cfae"
      },
      "outputs": [],
      "source": [
        "# Define a function that returns random 0/1 from bernouli distribution for random sampling\n",
        "def bernoulli_true_false(p):\n",
        "    # Create a Bernoulli distribution with probability p\n",
        "    bernoulli_dist = torch.distributions.Bernoulli(torch.tensor([p]))\n",
        "    # Sample from this distribution and convert 1 to True and 0 to False\n",
        "    return bernoulli_dist.sample().item() == 1"
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "721f5cb2-a711-4ded-bdc1-50bcdda20de3"
      },
      "outputs": [],
      "source": [
        "def Masking(token):\n",
        "    # Decide whether to mask this token (20% chance)\n",
        "    mask = bernoulli_true_false(0.2)\n",
        "\n",
        "    # If mask is False, immediately return with '[PAD]' label\n",
        "    if not mask:\n",
        "        return token, '[PAD]'\n",
        "\n",
        "    # If mask is True, proceed with further operations\n",
        "    # Randomly decide on an operation (50% chance each)\n",
        "    random_opp = bernoulli_true_false(0.5)\n",
        "    random_swich = bernoulli_true_false(0.5)\n",
        "\n",
        "    # Case 1: If mask, random_opp, and random_swich are True\n",
        "    if mask and random_opp and random_swich:\n",
        "        # Replace the token with '[MASK]' and set label to a random token\n",
        "        mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
        "        token_ = '[MASK]'\n",
        "\n",
        "    # Case 2: If mask and random_opp are True, but random_swich is False\n",
        "    elif mask and random_opp and not random_swich:\n",
        "        # Leave the token unchanged and set label to the same token\n",
        "        token_ = token\n",
        "        mask_label = token\n",
        "\n",
        "    # Case 3: If mask is True, but random_opp is False\n",
        "    else:\n",
        "        # Replace the token with '[MASK]' and set label to the original token\n",
        "        token_ = '[MASK]'\n",
        "        mask_label = token\n",
        "\n",
        "    return token_, mask_label"
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec29554b-2f8c-4127-b89c-01dd6888924f",
        "outputId": "17ef7456-71f9-4ed2-d202-c7ab4a7ac248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK] apple \t Actual token *apple* is masked with '[MASK]'\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "[MASK] whored \t Actual token *apple* is replaced with random token #whored#\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n",
            "apple [PAD] \t Actual token *apple* is left unchanged\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(100)\n",
        "for l in range(10):\n",
        "  token=\"apple\"\n",
        "  token_,label=Masking(token)\n",
        "  if token==token_ and label==\"[PAD]\":\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is left unchanged\")\n",
        "  elif token_==\"[MASK]\" and label==token:\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is masked with '{token_}'\")\n",
        "  else:\n",
        "    print(token_,label,f\"\\t Actual token *{token}* is replaced with random token #{label}#\")"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd59c1cf-dab6-4bc9-9355-82fd5ab4c644"
      },
      "source": [
        "### Data preparation for MLM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b225b6c6-3d52-4f7e-82f6-149daa4f7977"
      },
      "outputs": [],
      "source": [
        "def prepare_for_mlm(tokens, include_raw_tokens=False):\n",
        "    \"\"\"\n",
        "    Prepares tokenized text for BERT's Masked Language Model (MLM) training.\n",
        "\n",
        "    \"\"\"\n",
        "    bert_input = []  # List to store sentences processed for BERT's MLM\n",
        "    bert_label = []  # List to store labels for each token (mask, random, or unchanged)\n",
        "    raw_tokens_list = []  # List to store raw tokens if needed\n",
        "    current_bert_input = []\n",
        "    current_bert_label = []\n",
        "    current_raw_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # Apply BERT's MLM masking strategy to the token\n",
        "        masked_token, mask_label = Masking(token)\n",
        "\n",
        "        # Append the processed token and its label to the current sentence and label list\n",
        "        current_bert_input.append(masked_token)\n",
        "        current_bert_label.append(mask_label)\n",
        "\n",
        "        # If raw tokens are to be included, append the original token to the current raw tokens list\n",
        "        if include_raw_tokens:\n",
        "            current_raw_tokens.append(token)\n",
        "\n",
        "        # Check if the token is a sentence delimiter (., ?, !)\n",
        "        if token in ['.', '?', '!']:\n",
        "            # If current sentence has more than two tokens, consider it a valid sentence\n",
        "            if len(current_bert_input) > 2:\n",
        "                bert_input.append(current_bert_input)\n",
        "                bert_label.append(current_bert_label)\n",
        "                # If including raw tokens, add the current list of raw tokens to the raw tokens list\n",
        "                if include_raw_tokens:\n",
        "                    raw_tokens_list.append(current_raw_tokens)\n",
        "\n",
        "                # Reset the lists for the next sentence\n",
        "                current_bert_input = []\n",
        "                current_bert_label = []\n",
        "                current_raw_tokens = []\n",
        "            else:\n",
        "                # If the current sentence is too short, discard it and reset lists\n",
        "                current_bert_input = []\n",
        "                current_bert_label = []\n",
        "                current_raw_tokens = []\n",
        "\n",
        "    # Add any remaining tokens as a sentence if there are any\n",
        "    if current_bert_input:\n",
        "        bert_input.append(current_bert_input)\n",
        "        bert_label.append(current_bert_label)\n",
        "        if include_raw_tokens:\n",
        "            raw_tokens_list.append(current_raw_tokens)\n",
        "\n",
        "    # Return the prepared lists for BERT's MLM training\n",
        "    return (bert_input, bert_label, raw_tokens_list) if include_raw_tokens else (bert_input, bert_label)"
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb47107d-b672-41c4-a099-f4447c4e283d",
        "outputId": "112fa258-ddb1-4c3e-bca8-a52bcfe4ffe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without raw tokens: \t  \n",
            " \t original_input is: \t  The sun sets behind the distant mountains. \n",
            " \t bert_input is: \t  [['[MASK]', 'sun', 'sets', 'behind', 'the', '[MASK]', 'mountains', '.']] \n",
            " \t bert_label is: \t  [['the', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'whored', '[PAD]', '[PAD]']]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "With raw tokens: \t  \n",
            " \t original_input is: \t  The sun sets behind the distant mountains. \n",
            " \t bert_input is: \t  [['[MASK]', 'sun', 'sets', 'behind', 'the', '[MASK]', 'mountains', '.']] \n",
            " \t bert_label is: \t  [['the', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'whored', '[PAD]', '[PAD]']] \n",
            " \t raw_tokens_list is: \t  [['the', 'sun', 'sets', 'behind', 'the', 'distant', 'mountains', '.']]\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(100)\n",
        "original_input=\"The sun sets behind the distant mountains.\"\n",
        "tokens=tokenizer(original_input)\n",
        "bert_input, bert_label= prepare_for_mlm(tokens, include_raw_tokens=False)\n",
        "print(\"Without raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label)\n",
        "print(\"-\"*200)\n",
        "torch.manual_seed(100)\n",
        "bert_input, bert_label, raw_tokens_list= prepare_for_mlm(tokens, include_raw_tokens=True)\n",
        "print(\"With raw tokens: \\t \",\"\\n \\t original_input is: \\t \", original_input,\"\\n \\t bert_input is: \\t \", bert_input,\"\\n \\t bert_label is: \\t \", bert_label,\"\\n \\t raw_tokens_list is: \\t \", raw_tokens_list)"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4b1845-ca44-43f3-9381-65dcaff181cb"
      },
      "source": [
        "Therefore, each token in a sentence is labeled, depending on the masking operation that is applied on that token. In this example, the first \"the\" is **masked**, therefore, bert_input is [MASK] and its bert_label is 'The'. Tokens 'sun', 'sets', 'behind' and the last 'the' are not changed, so their corresponding labels are [PAD]. \"distant\" is **masked and replaced with a random token**, therefore, bert_input is [MASK] and its bert_label is 'human-scaled'. Finally, 'mountains' and '.' are **unchanged** so their corresponding labels are [PAD].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "126f4927-6692-456e-b80e-839f871466e6"
      },
      "source": [
        "### Data preparation for NSP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44419e64-e9bb-43a6-8cac-f3aeba65e0fd"
      },
      "outputs": [],
      "source": [
        "def process_for_nsp(input_sentences, input_masked_labels):\n",
        "    \"\"\"\n",
        "    Prepares data for Next Sentence Prediction (NSP) task in BERT training.\n",
        "\n",
        "    Args:\n",
        "    input_sentences (list): List of tokenized sentences.\n",
        "    input_masked_labels (list): Corresponding list of masked labels for the sentences.\n",
        "\n",
        "    Returns:\n",
        "    bert_input (list): List of sentence pairs for BERT input.\n",
        "    bert_label (list): List of masked labels for the sentence pairs.\n",
        "    is_next (list): Binary label list where 1 indicates 'next sentence' and 0 indicates 'not next sentence'.\n",
        "    \"\"\"\n",
        "    if len(input_sentences) < 2:\n",
        "       raise ValueError(\"must have two same number of items.\")\n",
        "\n",
        "\n",
        "    # Verify that both input lists are of the same length and have a sufficient number of sentences\n",
        "    if len(input_sentences) != len(input_masked_labels):\n",
        "        raise ValueError(\"Both lists must have the same number of items.\")\n",
        "\n",
        "    bert_input = []\n",
        "    bert_label = []\n",
        "    is_next = []\n",
        "\n",
        "    available_indices = list(range(len(input_sentences)))\n",
        "\n",
        "    while len(available_indices) >= 2:\n",
        "        if random.random() < 0.5:\n",
        "            # Choose two consecutive sentences to simulate the 'next sentence' scenario\n",
        "            index = random.choice(available_indices[:-1])  # Exclude the last index\n",
        "            # append list and add  '[CLS]' and  '[SEP]' tokens\n",
        "            bert_input.append([['[CLS]']+input_sentences[index]+ ['[SEP]'],input_sentences[index + 1]+ ['[SEP]']])\n",
        "            bert_label.append([['[PAD]']+input_masked_labels[index]+['[PAD]'], input_masked_labels[index + 1]+ ['[PAD]']])\n",
        "            is_next.append(1)  # Label 1 indicates these sentences are consecutive\n",
        "\n",
        "            # Remove the used indices\n",
        "            available_indices.remove(index)\n",
        "            if index + 1 in available_indices:\n",
        "                available_indices.remove(index + 1)\n",
        "        else:\n",
        "            # Choose two random distinct sentences to simulate the 'not next sentence' scenario\n",
        "            indices = random.sample(available_indices, 2)\n",
        "            bert_input.append([['[CLS]']+input_sentences[indices[0]]+['[SEP]'],input_sentences[indices[1]]+ ['[SEP]']])\n",
        "            bert_label.append([['[PAD]']+input_masked_labels[indices[0]]+['[PAD]'], input_masked_labels[indices[1]]+['[PAD]']])\n",
        "            is_next.append(0)  # Label 0 indicates these sentences are not consecutive\n",
        "\n",
        "            # Remove the used indices\n",
        "            available_indices.remove(indices[0])\n",
        "            available_indices.remove(indices[1])\n",
        "\n",
        "\n",
        "\n",
        "    return bert_input, bert_label, is_next"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ff2c96b-3c6c-4b47-848b-b308b12b9c7d",
        "outputId": "8ec24815-1ed3-49ab-a8be-beb888165de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Input:\n",
            "[['[CLS]', 'she', 'enjoys', 'reading', 'books', '[SEP]'], ['he', 'likes', 'playing', 'guitar', '[SEP]']]\n",
            "BERT Label:\n",
            "[['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['he', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
            "Is Next:  [1]\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "BERT Input:\n",
            "[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]\n",
            "BERT Label:\n",
            "[['[PAD]', 'he', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
            "Is Next:  [0]\n"
          ]
        }
      ],
      "source": [
        "# Flatten the tensor\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "# Sample input sentences\n",
        "input_sentences = [[\"i\", \"love\", \"apples\"], [\"she\", \"enjoys\", \"reading\", \"books\"], [\"he\", \"likes\", \"playing\", \"guitar\"]]\n",
        "# Create masked labels for the sentences\n",
        "input_masked_labels=[]\n",
        "for sentence in input_sentences:\n",
        "  _, current_masked_label= prepare_for_mlm(sentence, include_raw_tokens=False)\n",
        "  input_masked_labels.append(flatten(current_masked_label))\n",
        "# Create NSP pairs and labels\n",
        "random.seed(100)\n",
        "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
        "\n",
        "# Print the output\n",
        "print(\"BERT Input:\")\n",
        "for pair in bert_input:\n",
        "    print(pair)\n",
        "print(\"BERT Label:\")\n",
        "for pair in bert_label:\n",
        "    print(pair)\n",
        "print(\"Is Next: \", is_next)\n",
        "print(\"-\"*200)\n",
        "random.seed(1000)\n",
        "bert_input, bert_label, is_next = process_for_nsp(input_sentences, input_masked_labels)\n",
        "\n",
        "# Print the output\n",
        "print(\"BERT Input:\")\n",
        "for pair in bert_input:\n",
        "    print(pair)\n",
        "print(\"BERT Label:\")\n",
        "for pair in bert_label:\n",
        "    print(pair)\n",
        "print(\"Is Next: \", is_next)\n"
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8564f17-e410-44e1-ac1a-58184722a390"
      },
      "source": [
        "These two examples demonstrate how sentence pairs are randomly created from the sentence bank and labeled for NSP task. Special symbols [CLS] and [SEP] are first added to the input sentences. BERT label is created using the `prepare_for_mlm` function. In the first example, the second sentence follows the first sentence. Therefore, `Is Next` is 1. In the second example, the second sentence does not follow the first sentence. So, `Is Next` is 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bbd1b3e-5e83-45e6-88e0-c0c8f7b0e203"
      },
      "source": [
        "### Finalizing BERT inputs\n",
        "\n",
        "`prepare_bert_final_inputs` consolidates the prepared data for MLM and NSP into a format suitable for BERT training, including converting tokens to indices, padding sequences for uniform length, and generating segment labels to distinguish between pairs of sentences. This function is the final step in preparing data for BERT, ensuring it is in the correct format for effective model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6e87ce1-c9c8-44fd-a752-844f8b1027dc"
      },
      "outputs": [],
      "source": [
        "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts,to_tenor=True):\n",
        "    \"\"\"\n",
        "    Prepare the final input lists for BERT training.\n",
        "    \"\"\"\n",
        "    def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
        "        pair=deepcopy(pair_)\n",
        "        max_len = max(len(pair[0]), len(pair[1]))\n",
        "        #append [PAD] to each sentence in the pair till the maximum length reaches\n",
        "        pair[0].extend([pad] * (max_len - len(pair[0])))\n",
        "        pair[1].extend([pad] * (max_len - len(pair[1])))\n",
        "        return pair[0], pair[1]\n",
        "\n",
        "    #flatten the tensor\n",
        "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "    #transform tokens to vocab indices\n",
        "    tokens_to_index=lambda tokens: [vocab[token] for token in tokens]\n",
        "\n",
        "    bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
        "\n",
        "    for bert_input, bert_label,is_next in zip(bert_inputs, bert_labels,is_nexts):\n",
        "        # Create segment labels for each pair of sentences\n",
        "        segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
        "\n",
        "        # Zero-pad the bert_input and bert_label and segment_label\n",
        "        bert_input_padded = zero_pad_list_pair(bert_input)\n",
        "        bert_label_padded = zero_pad_list_pair(bert_label)\n",
        "        segment_label_padded = zero_pad_list_pair(segment_label,pad=0)\n",
        "\n",
        "        #convert to tensors\n",
        "        if to_tenor:\n",
        "\n",
        "            # Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
        "            bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)),dtype=torch.int64))\n",
        "            #bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
        "            bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)),dtype=torch.int64))\n",
        "            segment_labels_final.append(torch.tensor(flatten(segment_label_padded),dtype=torch.int64))\n",
        "            is_nexts_final.append(is_next)\n",
        "\n",
        "        else:\n",
        "          # Flatten the padded inputs and labels\n",
        "            bert_inputs_final.append(flatten(bert_input_padded))\n",
        "            bert_labels_final.append(flatten(bert_label_padded))\n",
        "            segment_labels_final.append(flatten(segment_label_padded))\n",
        "            is_nexts_final.append(is_next)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final\n"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04af43a1-5b09-4c6f-9b1e-c94c1a1cb526",
        "outputId": "cacbd6d0-3977-49f0-b0be-3e2f215d0aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:\t\t [[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]] \n",
            "inputs_final:\t [tensor([    1,    33,  1155,   404,  4833,     2,    16,   123, 14227,     2,     0,     0])] \n",
            "bert labels final:\t [tensor([ 0, 33,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])] \n",
            "segment labels final:\t [tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0])] \n",
            "is nexts final:\t [0]\n"
          ]
        }
      ],
      "source": [
        "bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final=prepare_bert_final_inputs(bert_input, bert_label, is_next,to_tenor=True)\n",
        "torch.set_printoptions(linewidth=10000)# this assures that whole output is printed in one line\n",
        "print(\"input:\\t\\t\",bert_input,\"\\ninputs_final:\\t\",bert_inputs_final,\"\\nbert labels final:\\t\",bert_labels_final,\"\\nsegment labels final:\\t\",segment_labels_final,\"\\nis nexts final:\\t\",is_nexts_final)"
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ad3f891-d8ba-4540-993c-95974cb6cae3"
      },
      "source": [
        "Sentences are zero-padded and each token is mapped to its vocab index(`[CLS]`>>1, `he`>>33, ..., `[SEP]`>>2,`[PAD]`>>0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b624823-b958-4b54-8d8c-198877192d12"
      },
      "source": [
        "Mask labels are also padded and mapped to vocab indices. In this case, all tokens are **unchanged** except the token, `he` which is masked:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1564c48e-d7a3-412d-b281-de75da4fe651",
        "outputId": "4fc8abe0-36da-4d38-e922-6811232e9bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:\t\t [[['[CLS]', 'he', 'likes', 'playing', 'guitar', '[SEP]'], ['i', 'love', 'apples', '[SEP]']]] \n",
            "mask_label:\t [[['[PAD]', 'he', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]']]] \n",
            "labels_final: \t [tensor([ 0, 33,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])]\n"
          ]
        }
      ],
      "source": [
        "print(\"input:\\t\\t\",bert_input,\"\\nmask_label:\\t\",bert_label, \"\\nlabels_final: \\t\",bert_labels_final)"
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0485df0b-2aa9-4db9-9583-373bb902b472"
      },
      "source": [
        "Finally, segment labels are created, where tokens of the first sentence are labeled with 1, tokens of the second sentence are labeled with 2 and zero-paddings are labeled with 0.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3bd6466-657e-47da-b114-e903840d514e",
        "outputId": "709414bf-7a37-4bac-cc0f-137853d65cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "inputs_final:\t [tensor([    1,    33,  1155,   404,  4833,     2,    16,   123, 14227,     2,     0,     0])] \n",
            "segment_labels:\t [tensor([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0])]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\ninputs_final:\\t\",bert_inputs_final,\"\\nsegment_labels:\\t\",segment_labels_final)"
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686000c5-cab7-49b0-b2b9-8822919079b7"
      },
      "source": [
        "## Preparing the Dataset and Training\n",
        "\n",
        "- A CSV file is created to store the data set prepared for BERT training and testing. Each row contains the original text, BERT inputs, labels, segment labels, and the NSP task label.\n",
        "- The data from the IMDB data set is tokenized, processed for MLM, and then for NSP. The results are formatted and written to the CSV file, providing a comprehensive data set for BERT model training.\n",
        "\n",
        "This process is critical for ensuring the data is in the right format for effective training of BERT on the IMDB data set, focusing on understanding text context and relationships between sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8b2dc5b-65f6-4d40-a065-7dc2ed92318b"
      },
      "outputs": [],
      "source": [
        "csv_file_path ='train_bert_data_new.csv'\n",
        "\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    csv_writer = csv.writer(file)\n",
        "    csv_writer.writerow(['Original Text', 'BERT Input', 'BERT Label', 'Segment Label', 'Is Next'])\n",
        "\n",
        "    # Wrap train_iter with tqdm for a progress bar\n",
        "    for n, (_, sample) in enumerate(tqdm(train_iter, desc=\"Processing samples\")):\n",
        "        # Tokenize the sample input\n",
        "        tokens = tokenizer(sample)\n",
        "        # Create MLM inputs and labels\n",
        "        bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens=False)\n",
        "        if len(bert_input) < 2:\n",
        "            continue\n",
        "        # Create NSP pairs, token labels, and is_next label\n",
        "        bert_inputs, bert_labels, is_nexts = process_for_nsp(bert_input, bert_label)\n",
        "        # add zero-paddings, map tokens to vocab indices and create segment labels\n",
        "        bert_inputs, bert_labels, segment_labels, is_nexts = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)\n",
        "        # convert tensors to lists, convert lists to JSON-formatted strings\n",
        "        for bert_input, bert_label, segment_label, is_next in zip(bert_inputs, bert_labels, segment_labels, is_nexts):\n",
        "            bert_input_str = json.dumps(bert_input.tolist())\n",
        "            bert_label_str = json.dumps(bert_label.tolist())\n",
        "            segment_label_str = ','.join(map(str, segment_label.tolist()))\n",
        "            # Write the data to a CSV file row-by-row\n",
        "            csv_writer.writerow([sample, bert_input_str, bert_label_str, segment_label_str, is_next])"
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "prev_pub_hash": "6c8fa70732ed928bd648243a39ecd88f18424f64edfa13622ab688097a48508d",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}