{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogukartal/IBM_AI_Labs/blob/main/Generative%20AI%20Language%20Modeling%20with%20Transformers/Pre_training_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5aa69e7-58dd-4c30-b6fa-5fe5ec35dda5"
      },
      "source": [
        "# **Pretraining BERT Models**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95196ccd-cf36-4036-96c3-baf87e2d3ac2"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d408a425-49fc-45d2-a17b-8f5874b48bae"
      },
      "outputs": [],
      "source": [
        "!pip install -qq torch==2.1.0\n",
        "!pip install -Uqq portalocker>=2.0.0\n",
        "!pip install -qq torchtext==0.16.0"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ccf938-e66f-4bd3-802d-fb9e14d640a8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import Tensor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "from transformers import BertTokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torchtext.vocab import Vocab,build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import IMDB\n",
        "import random\n",
        "from itertools import chain\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc750c5f-7aaa-4cd5-afef-b5217df8d765"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4541471a-2824-4939-9eed-5e2625683e2f"
      },
      "source": [
        "### Pretraining Objectives\n",
        "\n",
        "1. Masked Language Modeling (MLM):\n",
        "   Masked language modeling involves randomly masking some words in a sentence and training the model to predict the masked words based on the context provided by the surrounding words(i.e., words that appear either before or after the masked word). The objective is to enable the model to learn contextual understanding and fill in missing information.\n",
        "\n",
        "   Here's how MLM works:\n",
        "   - Given an input sentence, a certain percentage of the words are randomly chosen and replaced with a special [MASK] token.\n",
        "   - The model's task is to predict the original words that were masked, given the context of the surrounding words.\n",
        "   - During training, the model learns to understand the relationship between the masked words and the rest of the sentence, effectively capturing the contextual information.\n",
        "\n",
        "2. Next Sentence Prediction (NSP):\n",
        "   Next sentence prediction involves training the model to predict whether two sentences are consecutive in the original text or randomly chosen from the corpus. This objective helps the model learn sentence-level relationships and understand the coherence between sentences.\n",
        "\n",
        "   Here's how NSP works:\n",
        "   - Given a pair of sentences, the model is trained to predict whether the second sentence follows the first sentence in the original text or if it is randomly selected from the corpus.\n",
        "   - The model learns to capture the relationships between sentences and understand the flow of information in the text.\n",
        "\n",
        "   NSP is particularly useful for tasks that involve understanding the relationship between multiple sentences, such as question answering or document classification. By training the model to predict the coherence of sentence pairs, it learns to capture the semantic connections between them.\n",
        "\n",
        "It's important to note that different pretrained models may use variations or combinations of these objectives, depending on the specific architecture and training setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de5fcef5-16c0-406d-9d8a-eabfdaea807d"
      },
      "source": [
        "## Loading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e65cd8c-a13b-43c2-9c24-6d6539ba8d33"
      },
      "outputs": [],
      "source": [
        "!wget -O BERT_dataset.zip https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/bZaoQD52DcMpE7-kxwAG8A.zip\n",
        "!unzip BERT_dataset.zip"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba92ec93-579d-4b6e-9b42-4e0287759673"
      },
      "outputs": [],
      "source": [
        "# Create a torch Dataset using the CSV file\n",
        "class BERTCSVDataset(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.data = pd.read_csv(filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        try:\n",
        "            bert_input = torch.tensor(json.loads(row['BERT Input']), dtype=torch.long)\n",
        "            bert_label = torch.tensor(json.loads(row['BERT Label']), dtype=torch.long)\n",
        "            segment_label = torch.tensor([int(x) for x in row['Segment Label'].split(',')], dtype=torch.long)\n",
        "            is_next = torch.tensor(row['Is Next'], dtype=torch.long)\n",
        "            original_text = row['Original Text']  # If you want to use it\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON for row {idx}: {e}\")\n",
        "            print(\"BERT Input:\", row['BERT Input'])\n",
        "            print(\"BERT Label:\", row['BERT Label'])\n",
        "            # Handle the error\n",
        "            return None  # or some default values\n",
        "        return bert_input, bert_label, segment_label, is_next  # Include original_text if needed"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9d66907-0443-418b-a39b-2242a169cf5f"
      },
      "outputs": [],
      "source": [
        "PAD_IDX = 0\n",
        "\n",
        "def collate_batch(batch):\n",
        "    bert_inputs_batch, bert_labels_batch, segment_labels_batch, is_nexts_batch = [], [], [], []\n",
        "\n",
        "    for bert_input, bert_label, segment_label, is_next in batch:\n",
        "        # Convert each sequence to a tensor and append to the respective list\n",
        "        bert_inputs_batch.append(torch.tensor(bert_input, dtype=torch.long))\n",
        "        bert_labels_batch.append(torch.tensor(bert_label, dtype=torch.long))\n",
        "        segment_labels_batch.append(torch.tensor(segment_label, dtype=torch.long))\n",
        "        is_nexts_batch.append(is_next)\n",
        "\n",
        "    # Pad the sequences in the batch\n",
        "    bert_inputs_final = pad_sequence(bert_inputs_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    bert_labels_final = pad_sequence(bert_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    segment_labels_final = pad_sequence(segment_labels_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    is_nexts_batch = torch.tensor(is_nexts_batch, dtype=torch.long)\n",
        "\n",
        "    return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_batch"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdfe134a-7bf1-44c1-ad93-739571bbad69"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dataset_path = './bert_dataset/bert_train_data.csv'\n",
        "test_dataset_path = './bert_dataset/bert_test_data.csv'\n",
        "\n",
        "train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "566caa29-8b15-425e-8876-e10693dc1cd5"
      },
      "source": [
        "## Model creation\n",
        "\n",
        "In BERT, there are three types of embedings\n",
        "\n",
        "1. Token Embedding: Token embedding is the initial representation of each token in a BERT model. It maps each token to a dense vector representation of a fixed size, typically referred to as the embedding size. The token embedding layer in BERT learns the contextual representations of the input tokens. These embeddings capture the semantic meaning of the tokens and their relationships with other tokens in the context.\n",
        "\n",
        "2. Positional Embedding: BERT is a transformer-based model that processes the input tokens in parallel. However, since transformers don't inherently capture the order of tokens, positional embedding is used to inject positional information into the model. It adds a vector representation to each token that encodes its position in the input sequence. The positional embedding allows BERT to understand the sequential order of the tokens and capture their relative positions.\n",
        "\n",
        "3. Segment Embedding: BERT can handle sentence pairs or sequences that have distinct segments or parts. To differentiate between different segments, such as sentences or document sections, segment embedding is used. It assigns a unique vector representation to each segment or part of the input. The segment embeddings help BERT understand the relationships between different segments and capture the context within and between them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5480d0e-f62b-4e4d-9305-1f6657381ff1"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 10\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Create a positional encoding matrix as per the Transformer paper's formula\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: torch.Tensor):\n",
        "        # Apply the positional encodings to the input token embeddings\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6958c741-8b55-45f9-ab96-71da0665cf8a"
      },
      "outputs": [],
      "source": [
        "class BERTEmbedding (nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, emb_size, dropout=0.1, train=True):\n",
        "        super().__init__()\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
        "        self.segment_embedding = nn.Embedding(3, emb_size)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, bert_inputs, segment_labels=False):\n",
        "        my_embeddings=self.token_embedding(bert_inputs)\n",
        "        if self.train:\n",
        "          x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
        "        else:\n",
        "          x = my_embeddings + self.positional_encoding(my_embeddings)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8edb155a-c381-4dfb-8302-f1f27770e4ab"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE=147161\n",
        "batch = 2\n",
        "count = 0\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load sample batches from dataloader\n",
        "for batch in train_dataloader:\n",
        "  bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "  count += 1\n",
        "  if count == 5:\n",
        "      break"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_inputs, bert_labels, segment_labels, is_nexts = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "PSh0pd5xmodS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ece720da-5ee5-4f0e-ac08-67b7de692e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "679ff9b8-9b1b-47e2-9bae-5a9fdd505e40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "bert_inputs.shape"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57ec6847-0668-4246-8a00-efc6612696d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fea074-beec-4f4e-92c3-54b3741d42ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    1,     5,     3,    64, 20802,    43,   334,   121,     3,    64,\n",
              "           20,    78,    44,   226,  1221,  1741,    11,    14,     6,     2,\n",
              "            0,    64,  1258,    38,    10,     3,   101,  7953,  1268,  1886,\n",
              "         1209,  8608,   134,  1069,    25,    49,    20,  1747,    41,   354,\n",
              "            6,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "bert_inputs[:,0]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66a6950e-f2ec-44e7-a9d7-3362cbb59413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52facfc-7fa8-4abb-c7b2-1a77f283c7e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "segment_labels.shape"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b599885-b09f-4349-8f59-6f19f0ad53de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ee4798-b237-444f-f746-28ff8d5645a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "segment_labels[:,0]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c0cdfd7-2ffc-4e22-bbfa-d40316d50f41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257dcaae-08dd-4d7e-f110-e00d3faa87bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of token embeddings: torch.Size([100, 3, 10]) \n",
            "\n",
            "Token Embeddings for the 0th token of the first sample: tensor([ 0.9975,  2.7258, -2.7573,  3.3481,  2.5082,  1.8394, -0.5717, -1.3693,\n",
            "        -4.1012, -0.3087], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Token Embeddings for the 1th token of the first sample: tensor([-4.4976,  2.6946, -6.5365, -4.9356, -0.0905,  1.2110,  0.0913,  0.7686,\n",
            "        -3.9067,  1.4908], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Token Embeddings for the 2th token of the first sample: tensor([-3.1621, -0.4848, -6.7954,  6.0018,  1.2483,  0.8448, -1.7277, -2.5739,\n",
            "        -3.6704,  1.4464], device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the TokenEmbedding\n",
        "token_embedding = TokenEmbedding(VOCAB_SIZE, emb_size=EMBEDDING_DIM).to(device)\n",
        "\n",
        "# Get the token embeddings for a sample input\n",
        "t_embeddings = token_embedding(bert_inputs)\n",
        "\n",
        "#Each token is transformed into a tensor of size emb_size\n",
        "print(f\"Dimensions of token embeddings: {t_embeddings.size()} \\n\") # Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "\n",
        "#Check the embedded vectors for first 3 tokens of the first sample in the batch\n",
        "# you get embeddings[i,0,:] where i refers to the i'th token of the first sample in the batch (b=0)\n",
        "for i in range(3):\n",
        "    print(f\"Token Embeddings for the {i}th token of the first sample: {t_embeddings[i,0,:]}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da14e50-f006-45c5-a9b2-99a457c27d7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419e1146-4f0f-4f73-e681-a79a2c7308c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of positionally encoded tokens: torch.Size([100, 3, 10])\n",
            "Positional Embeddings for the 0th token of the first sample: tensor([ 0.9975,  3.7258, -2.7573,  4.3481,  2.5082,  2.8394, -0.5717, -0.3693,\n",
            "        -4.1012,  0.6913], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Positional Embeddings for the 1th token of the first sample: tensor([-3.6561,  3.2349, -6.3786, -3.9482, -0.0653,  2.2107,  0.0953,  1.7686,\n",
            "        -3.9061,  2.4908], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Positional Embeddings for the 2th token of the first sample: tensor([-2.2528, -0.9009, -6.4837,  6.9520,  1.2985,  1.8435, -1.7198, -1.5739,\n",
            "        -3.6692,  2.4464], device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "positional_encoding = PositionalEncoding(emb_size=EMBEDDING_DIM,dropout=0).to(device)\n",
        "\n",
        "# Apply positional encoding to token embeddings\n",
        "p_embedding = positional_encoding(t_embeddings)\n",
        "\n",
        "print(f\"Dimensions of positionally encoded tokens: {p_embedding.size()}\")# Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "\n",
        "#Check the positional encoded vectors for first 3 tokens of the first sample in the batch\n",
        "# you get encoded_tokens[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"Positional Embeddings for the {i}th token of the first sample: {p_embedding[i,0,:]}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c782e8b-8669-4edd-b12a-a81854b5ddbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b3ac74-4f34-46c5-c9c8-d4235006d822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of segment embedding: torch.Size([100, 3, 10])\n",
            "Segment Embeddings for the 0th token of the first sample: tensor([ 0.6989, -0.1497,  1.6185,  0.6163,  0.9998,  0.5078, -0.2371,  0.4028,\n",
            "        -0.2353,  1.0584], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Segment Embeddings for the 1th token of the first sample: tensor([ 0.6989, -0.1497,  1.6185,  0.6163,  0.9998,  0.5078, -0.2371,  0.4028,\n",
            "        -0.2353,  1.0584], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Segment Embeddings for the 2th token of the first sample: tensor([ 0.6989, -0.1497,  1.6185,  0.6163,  0.9998,  0.5078, -0.2371,  0.4028,\n",
            "        -0.2353,  1.0584], device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "segment_embedding = nn.Embedding(3, EMBEDDING_DIM).to(device)\n",
        "\n",
        "s_embedding = segment_embedding(segment_labels)\n",
        "print(f\"Dimensions of segment embedding: {s_embedding.size()}\")# Expected: (sequence_length, batch_size, EMBEDDING_DIM)\n",
        "\n",
        "#Check the Segment Embedding vectors for first 3 tokens of the first sample in the batch\n",
        "# you get segment_embedded[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"Segment Embeddings for the {i}th token of the first sample: {s_embedding[i,0,:]}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22277f67-02e8-4654-8fe5-7e0d29736728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4dd57ad-b433-40e0-a078-b1df9716b176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of token + position + segment encoded tokens: torch.Size([100, 3, 10])\n",
            "BERT_Embedding for 0th token: tensor([ 2.6939,  6.3020, -3.8962,  8.3125,  6.0162,  5.1866, -1.3804, -1.3358,\n",
            "        -8.4376,  1.4409], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "BERT_Embedding for 1th token: tensor([ -7.4547,   5.7798, -11.2967,  -8.2675,   0.8440,   3.9294,  -0.0505,\n",
            "          2.9400,  -8.0481,   5.0401], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "BERT_Embedding for 2th token: tensor([ -4.7160,  -1.5354, -11.6607,  13.5701,   3.5465,   3.1960,  -3.6846,\n",
            "         -3.7450,  -7.5749,   4.9512], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#Create the combined embedding vectors\n",
        "bert_embeddings = t_embeddings + p_embedding + s_embedding\n",
        "print(f\"Dimensions of token + position + segment encoded tokens: {bert_embeddings.size()}\")\n",
        "\n",
        "#Check the BERT Embedding vectors for first 3 tokens of the first sample in the batch\n",
        "# you get bert_embeddings[i,0,:] where i refers to the i'th token of the first sample(b=0) in the batch\n",
        "for i in range(3):\n",
        "    print(f\"BERT_Embedding for {i}th token: {bert_embeddings[i,0,:]}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29aad99a-7a46-4a76-ae5a-d58487234b34"
      },
      "outputs": [],
      "source": [
        "class BERT(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, dropout=0.1):\n",
        "        \"\"\"\n",
        "        vocab_size: The size of the vocabulary.\n",
        "        d_model: The size of the embeddings (hidden size).\n",
        "        n_layers: The number of Transformer layers.\n",
        "        heads: The number of attention heads in each Transformer layer.\n",
        "        dropout: The dropout rate applied to embeddings and Transformer layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.heads = heads\n",
        "\n",
        "        # Embedding layer that combines token embeddings and segment embeddings\n",
        "        self.bert_embedding = BERTEmbedding(vocab_size, d_model, dropout)\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        # Linear layer for Next Sentence Prediction\n",
        "        self.nextsentenceprediction = nn.Linear(d_model, 2)\n",
        "\n",
        "        # Linear layer for Masked Language Modeling\n",
        "        self.masked_language = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, bert_inputs, segment_labels):\n",
        "        \"\"\"\n",
        "        bert_inputs: Input tokens.\n",
        "        segment_labels: Segment IDs for distinguishing different segments in the input.\n",
        "        mask: Attention mask to prevent attention to padding tokens.\n",
        "\n",
        "        return: Predictions for next sentence task and masked language modeling task.\n",
        "        \"\"\"\n",
        "\n",
        "        padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
        "        # Generate embeddings from input tokens and segment labels\n",
        "        my_bert_embedding = self.bert_embedding(bert_inputs, segment_labels)\n",
        "\n",
        "        # Pass embeddings through the Transformer encoder\n",
        "        transformer_encoder_output = self.transformer_encoder(my_bert_embedding,src_key_padding_mask=padding_mask)\n",
        "\n",
        "\n",
        "        next_sentence_prediction = self.nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
        "\n",
        "\n",
        "        # Masked Language Modeling: Predict all tokens in the sequence\n",
        "        masked_language = self.masked_language(transformer_encoder_output)\n",
        "\n",
        "        return  next_sentence_prediction, masked_language"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f18c66c-9552-4dfd-b76e-e5dc44588c22"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 10\n",
        "\n",
        "# Define parameters\n",
        "vocab_size = 147161  # Replace VOCAB_SIZE with your vocabulary size\n",
        "d_model = EMBEDDING_DIM  # Replace EMBEDDING_DIM with your embedding dimension\n",
        "n_layers = 2  # Number of Transformer layers\n",
        "initial_heads = 12 # Initial number of attention heads\n",
        "initial_heads = 2\n",
        "# Ensure the number of heads is a factor of the embedding dimension\n",
        "heads = initial_heads - d_model % initial_heads\n",
        "\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Create an instance of the BERT model\n",
        "model = BERT(vocab_size, d_model, n_layers, heads, dropout).to(device)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bae977ac-6cff-4766-9d74-35978261015a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894c8608-f583-43d2-f07e-d4ee1be7d13b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "padding_mask = (bert_inputs == PAD_IDX).transpose(0, 1)\n",
        "padding_mask.shape"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "640285a0-20fc-4425-aa71-38184837b40d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1627c6-52c8-4ab9-d2f1-b83fcc026ae1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 3, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=heads, dropout=dropout,batch_first=False).to(device)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers).to(device)\n",
        "# Pass embeddings through the Transformer encoder\n",
        "transformer_encoder_output = transformer_encoder(bert_embeddings,src_key_padding_mask=padding_mask)\n",
        "transformer_encoder_output.shape"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a93b136-b685-459a-b14e-1be6757dc0de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c18ea58-83e1-4274-f3e4-92d5269367cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NSP Output Shape: torch.Size([3, 2])\n"
          ]
        }
      ],
      "source": [
        "nextsentenceprediction = nn.Linear(d_model, 2).to(device)\n",
        "nsp = nextsentenceprediction(transformer_encoder_output[ 0,:])\n",
        "#logits for NSP task\n",
        "print(f\"NSP Output Shape: {nsp.shape}\")  # Expected shape: (batch_size, 2)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccf3187d-42fa-43b2-9299-ea4293270159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142b46c0-bab0-4650-996d-2c359596bda8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLM Output Shape: torch.Size([100, 3, 147161])\n"
          ]
        }
      ],
      "source": [
        "masked_language = nn.Linear(d_model, vocab_size).to(device)\n",
        "# Masked Language Modeling: Predict all tokens in the sequence\n",
        "mlm = masked_language(transformer_encoder_output)\n",
        "#logits for MLM task\n",
        "print(f\"MLM Output Shape: {mlm.shape}\")  # Expected shape: (seq_length, batch_size, vocab_size)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902b0c90-a1fb-439a-ae8d-f9788456cc6c"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "After creating the BERT model, the next step is training and evaluating its performance. To facilitate this, an `evaluate` function is defined with the following steps:\n",
        "\n",
        "1. Loss Function: The CrossEntropyLoss function is defined to calculate the loss between predicted and actual values.\n",
        "\n",
        "2. Function Arguments: The function takes arguments including the dataloader, model, loss function, and device.\n",
        "\n",
        "3. Evaluation Mode: The BERT model is put into evaluation mode using `model.eval()`, disabling dropout and training-specific behaviors. Variables are initialized to track the total loss, total next sentence loss, total mask loss, and total number of batches.\n",
        "\n",
        "4. Evaluation Loop: The function iterates through the batches in the provided dataloader.\n",
        "\n",
        "5. Forward Pass: A forward pass is performed with the BERT model to obtain predictions for the next sentence and masked language tasks.\n",
        "\n",
        "6. Loss Calculation: The losses for the next sentence and masked language tasks are calculated, and then summed up to obtain the total loss for the batch.\n",
        "\n",
        "7. Average Loss Calculation: The average loss, average next sentence loss, and average mask loss are calculated by dividing the total losses by the total number of batches.\n",
        "\n",
        "\n",
        "The `evaluate` function is used not only for evaluating the BERT model's performance but also during the training phase to assess the model's progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4816b08-5d18-4fbe-a37f-d289fbbd3570"
      },
      "outputs": [],
      "source": [
        "PAD_IDX=0\n",
        "loss_fn_mlm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)# The loss function must ignore PAD tokens and only calculates loss for the masked tokens\n",
        "loss_fn_nsp = nn.CrossEntropyLoss()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e42e6437-de51-4c21-9f8c-12b2af62f926"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader=test_dataloader, model=model, loss_fn_mlm=loss_fn_mlm, loss_fn_nsp=loss_fn_nsp, device=device):\n",
        "    model.eval()  # Turn off dropout and other training-specific behaviors\n",
        "\n",
        "    total_loss = 0\n",
        "    total_next_sentence_loss = 0\n",
        "    total_mask_loss = 0\n",
        "    total_batches = 0\n",
        "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
        "        for batch in dataloader:\n",
        "            bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
        "\n",
        "            # Calculate loss for next sentence prediction\n",
        "            # Ensure is_nexts is of the correct shape for CrossEntropyLoss\n",
        "            next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts.view(-1))\n",
        "\n",
        "            # Calculate loss for predicting masked tokens\n",
        "            # Flatten both masked_language predictions and bert_labels to match CrossEntropyLoss input requirements\n",
        "            mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
        "\n",
        "            # Sum up the two losses\n",
        "            loss = next_loss + mask_loss\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "            else:\n",
        "                total_loss += loss.item()\n",
        "                total_next_sentence_loss += next_loss.item()\n",
        "                total_mask_loss += mask_loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / (total_batches + 1)\n",
        "    avg_next_sentence_loss = total_next_sentence_loss / (total_batches + 1)\n",
        "    avg_mask_loss = total_mask_loss / (total_batches + 1)\n",
        "\n",
        "    print(f\"Average Loss: {avg_loss:.4f}, Average Next Sentence Loss: {avg_next_sentence_loss:.4f}, Average Mask Loss: {avg_mask_loss:.4f}\")\n",
        "    return avg_loss"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1578cfd6-6382-4d8b-b2c4-11fffd452311"
      },
      "source": [
        "## Training\n",
        "The training process for the BERT model involves the following steps:\n",
        "\n",
        "1. Optimizer Definition: Before training starts, an optimizer is defined for training the BERT model. In this case, the Adam optimizer is used.\n",
        "\n",
        "2. Training Loop: Within each epoch, the training data is iterated through in batches.\n",
        "\n",
        "3. Forward Pass: For each batch, a forward pass is performed, where the BERT model predicts the next sentence and masked language tasks.\n",
        "\n",
        "4. Loss Calculation and Parameter Update: The loss is calculated based on the predicted and actual values. The model's parameters are then updated through backpropagation and gradient clipping.\n",
        "\n",
        "5. Epoch Evaluation: After each epoch, the average training loss is printed. The model's performance on the test set is evaluated. Additionally, the model is saved after each epoch.\n",
        "\n",
        "These steps are repeated for multiple epochs to train the BERT model and monitor its progress over time.\n",
        "\n",
        "**NOTE: The current DataLoader is quite huge, and it will take several hours for the model to train with such a huge dataset. Hence, below is the randomly sampled dataset (which is relatively small which still takes 1-2 hours to run) from IMDB to make the process faster. If you want to train the model on the entire dataset, skip the next cell and directly run the training cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10cbe33a-3f7b-4bd6-8837-a5f20bbb1609"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 3\n",
        "\n",
        "train_dataset_path = './bert_dataset/bert_train_data_sampled.csv'\n",
        "test_dataset_path = './bert_dataset/bert_test_data_sampled.csv'\n",
        "\n",
        "train_dataset = BERTCSVDataset(train_dataset_path)\n",
        "test_dataset = BERTCSVDataset(test_dataset_path)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deda6fc2-ba34-485e-92c4-e6cc3e6f2251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748e8251-7f6c-49eb-c093-2fa2e9ed94e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Epoch 1:   0%|          | 0/3334 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:   0%|          | 6/3334 [00:00<00:56, 58.67it/s]\u001b[A\n",
            "Epoch 1:   0%|          | 12/3334 [00:00<00:58, 56.58it/s]\u001b[A\n",
            "Epoch 1:   1%|          | 18/3334 [00:00<00:59, 56.05it/s]\u001b[A\n",
            "Epoch 1:   1%|          | 24/3334 [00:00<00:58, 56.95it/s]\u001b[A\n",
            "Epoch 1:   1%|          | 30/3334 [00:00<00:59, 55.88it/s]\u001b[A\n",
            "Epoch 1:   1%|          | 36/3334 [00:00<01:00, 54.45it/s]\u001b[A\n",
            "Epoch 1:   1%|▏         | 42/3334 [00:00<00:59, 55.70it/s]\u001b[A\n",
            "Epoch 1:   1%|▏         | 48/3334 [00:00<01:00, 54.36it/s]\u001b[A\n",
            "Epoch 1:   2%|▏         | 54/3334 [00:00<00:59, 55.44it/s]\u001b[A\n",
            "Epoch 1:   2%|▏         | 61/3334 [00:01<00:57, 57.00it/s]\u001b[A\n",
            "Epoch 1:   2%|▏         | 67/3334 [00:01<00:57, 56.56it/s]\u001b[A\n",
            "Epoch 1:   2%|▏         | 73/3334 [00:01<00:58, 55.96it/s]\u001b[A\n",
            "Epoch 1:   2%|▏         | 80/3334 [00:01<00:55, 58.77it/s]\u001b[A\n",
            "Epoch 1:   3%|▎         | 87/3334 [00:01<00:54, 59.79it/s]\u001b[A\n",
            "Epoch 1:   3%|▎         | 93/3334 [00:01<00:54, 59.26it/s]\u001b[A\n",
            "Epoch 1:   3%|▎         | 99/3334 [00:01<00:54, 59.36it/s]\u001b[A\n",
            "Epoch 1:   3%|▎         | 106/3334 [00:01<00:54, 59.35it/s]\u001b[A\n",
            "Epoch 1:   3%|▎         | 112/3334 [00:01<00:56, 56.81it/s]\u001b[A\n",
            "Epoch 1:   4%|▎         | 118/3334 [00:02<00:55, 57.56it/s]\u001b[A\n",
            "Epoch 1:   4%|▎         | 124/3334 [00:02<00:56, 57.00it/s]\u001b[A\n",
            "Epoch 1:   4%|▍         | 130/3334 [00:02<00:55, 57.59it/s]\u001b[A\n",
            "Epoch 1:   4%|▍         | 136/3334 [00:02<00:54, 58.21it/s]\u001b[A\n",
            "Epoch 1:   4%|▍         | 142/3334 [00:02<00:57, 55.48it/s]\u001b[A\n",
            "Epoch 1:   4%|▍         | 149/3334 [00:02<00:54, 58.42it/s]\u001b[A\n",
            "Epoch 1:   5%|▍         | 155/3334 [00:02<00:55, 56.86it/s]\u001b[A\n",
            "Epoch 1:   5%|▍         | 161/3334 [00:02<00:55, 56.89it/s]\u001b[A\n",
            "Epoch 1:   5%|▌         | 167/3334 [00:02<01:03, 49.73it/s]\u001b[A\n",
            "Epoch 1:   5%|▌         | 173/3334 [00:03<01:00, 52.28it/s]\u001b[A\n",
            "Epoch 1:   5%|▌         | 179/3334 [00:03<01:00, 51.84it/s]\u001b[A\n",
            "Epoch 1:   6%|▌         | 185/3334 [00:03<01:00, 51.76it/s]\u001b[A\n",
            "Epoch 1:   6%|▌         | 191/3334 [00:03<00:58, 53.33it/s]\u001b[A\n",
            "Epoch 1:   6%|▌         | 198/3334 [00:03<00:55, 56.09it/s]\u001b[A\n",
            "Epoch 1:   6%|▌         | 204/3334 [00:03<00:55, 56.25it/s]\u001b[A\n",
            "Epoch 1:   6%|▋         | 210/3334 [00:03<00:55, 56.39it/s]\u001b[A\n",
            "Epoch 1:   6%|▋         | 216/3334 [00:03<00:54, 57.07it/s]\u001b[A\n",
            "Epoch 1:   7%|▋         | 222/3334 [00:03<00:54, 56.80it/s]\u001b[A\n",
            "Epoch 1:   7%|▋         | 228/3334 [00:04<00:57, 54.29it/s]\u001b[A\n",
            "Epoch 1:   7%|▋         | 234/3334 [00:04<00:56, 55.32it/s]\u001b[A\n",
            "Epoch 1:   7%|▋         | 241/3334 [00:04<00:53, 57.56it/s]\u001b[A\n",
            "Epoch 1:   7%|▋         | 248/3334 [00:04<00:51, 60.35it/s]\u001b[A\n",
            "Epoch 1:   8%|▊         | 255/3334 [00:04<00:51, 59.49it/s]\u001b[A\n",
            "Epoch 1:   8%|▊         | 262/3334 [00:04<00:50, 60.66it/s]\u001b[A\n",
            "Epoch 1:   8%|▊         | 269/3334 [00:04<00:51, 60.03it/s]\u001b[A\n",
            "Epoch 1:   8%|▊         | 276/3334 [00:04<00:51, 59.66it/s]\u001b[A\n",
            "Epoch 1:   8%|▊         | 282/3334 [00:04<00:54, 55.68it/s]\u001b[A\n",
            "Epoch 1:   9%|▊         | 289/3334 [00:05<00:53, 57.43it/s]\u001b[A\n",
            "Epoch 1:   9%|▉         | 295/3334 [00:05<00:53, 56.70it/s]\u001b[A\n",
            "Epoch 1:   9%|▉         | 301/3334 [00:05<00:54, 55.82it/s]\u001b[A\n",
            "Epoch 1:   9%|▉         | 308/3334 [00:05<00:52, 57.60it/s]\u001b[A\n",
            "Epoch 1:   9%|▉         | 314/3334 [00:05<00:52, 57.44it/s]\u001b[A\n",
            "Epoch 1:  10%|▉         | 320/3334 [00:05<00:52, 57.93it/s]\u001b[A\n",
            "Epoch 1:  10%|▉         | 326/3334 [00:05<00:51, 58.07it/s]\u001b[A\n",
            "Epoch 1:  10%|▉         | 332/3334 [00:05<00:53, 56.01it/s]\u001b[A\n",
            "Epoch 1:  10%|█         | 339/3334 [00:05<00:51, 58.42it/s]\u001b[A\n",
            "Epoch 1:  10%|█         | 345/3334 [00:06<00:51, 58.05it/s]\u001b[A\n",
            "Epoch 1:  11%|█         | 352/3334 [00:06<00:49, 60.52it/s]\u001b[A\n",
            "Epoch 1:  11%|█         | 359/3334 [00:06<00:48, 61.02it/s]\u001b[A\n",
            "Epoch 1:  11%|█         | 367/3334 [00:06<00:44, 66.30it/s]\u001b[A\n",
            "Epoch 1:  11%|█         | 374/3334 [00:06<00:43, 67.34it/s]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 381/3334 [00:06<00:44, 66.50it/s]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 388/3334 [00:06<00:44, 66.11it/s]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 395/3334 [00:06<00:44, 65.54it/s]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 402/3334 [00:06<00:45, 65.14it/s]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 409/3334 [00:07<00:44, 65.08it/s]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 417/3334 [00:07<00:43, 67.59it/s]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 424/3334 [00:07<00:44, 65.54it/s]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 431/3334 [00:07<00:44, 65.52it/s]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 438/3334 [00:07<00:43, 66.43it/s]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 445/3334 [00:07<00:44, 64.65it/s]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 452/3334 [00:07<00:43, 65.63it/s]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 459/3334 [00:07<00:43, 65.90it/s]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 466/3334 [00:07<00:45, 63.64it/s]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 474/3334 [00:08<00:43, 66.39it/s]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 481/3334 [00:08<00:44, 64.49it/s]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 489/3334 [00:08<00:42, 67.11it/s]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 496/3334 [00:08<00:43, 64.76it/s]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 504/3334 [00:08<00:42, 66.72it/s]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 512/3334 [00:08<00:40, 70.19it/s]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 520/3334 [00:08<00:41, 67.61it/s]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 527/3334 [00:08<00:42, 66.59it/s]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 534/3334 [00:08<00:43, 64.12it/s]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 541/3334 [00:09<00:44, 63.25it/s]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 548/3334 [00:09<00:43, 63.85it/s]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 555/3334 [00:09<00:45, 61.20it/s]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 562/3334 [00:09<00:44, 62.07it/s]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 569/3334 [00:09<00:44, 62.65it/s]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 576/3334 [00:09<00:42, 64.36it/s]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 584/3334 [00:09<00:40, 67.29it/s]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 591/3334 [00:09<00:40, 68.05it/s]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 598/3334 [00:09<00:42, 64.43it/s]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 605/3334 [00:10<00:42, 63.58it/s]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 612/3334 [00:10<00:43, 63.20it/s]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 619/3334 [00:10<00:42, 63.74it/s]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 626/3334 [00:10<00:41, 65.39it/s]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 633/3334 [00:10<00:44, 60.27it/s]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 640/3334 [00:10<00:43, 61.63it/s]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 647/3334 [00:10<00:44, 60.26it/s]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 655/3334 [00:10<00:42, 63.47it/s]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 662/3334 [00:10<00:42, 62.87it/s]\u001b[A\n",
            "Epoch 1:  20%|██        | 669/3334 [00:11<00:43, 61.26it/s]\u001b[A\n",
            "Epoch 1:  20%|██        | 677/3334 [00:11<00:40, 65.17it/s]\u001b[A\n",
            "Epoch 1:  21%|██        | 684/3334 [00:11<00:41, 64.55it/s]\u001b[A\n",
            "Epoch 1:  21%|██        | 691/3334 [00:11<00:42, 62.76it/s]\u001b[A\n",
            "Epoch 1:  21%|██        | 698/3334 [00:11<00:42, 62.29it/s]\u001b[A\n",
            "Epoch 1:  21%|██        | 705/3334 [00:11<00:41, 63.72it/s]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 712/3334 [00:11<00:40, 64.71it/s]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 719/3334 [00:11<00:41, 63.33it/s]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 726/3334 [00:11<00:44, 58.64it/s]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 734/3334 [00:12<00:41, 62.58it/s]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 741/3334 [00:12<00:40, 63.50it/s]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 749/3334 [00:12<00:38, 66.69it/s]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 756/3334 [00:12<00:38, 66.94it/s]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 763/3334 [00:12<00:37, 67.73it/s]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 771/3334 [00:12<00:37, 68.96it/s]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 778/3334 [00:12<00:36, 69.08it/s]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 785/3334 [00:12<00:37, 67.31it/s]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 792/3334 [00:12<00:37, 67.73it/s]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 799/3334 [00:13<00:37, 67.37it/s]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 806/3334 [00:13<00:37, 66.85it/s]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 813/3334 [00:13<00:37, 67.19it/s]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 820/3334 [00:13<00:37, 67.55it/s]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 827/3334 [00:13<00:36, 68.25it/s]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 834/3334 [00:13<00:36, 67.60it/s]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 841/3334 [00:13<00:36, 67.70it/s]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 848/3334 [00:13<00:36, 67.93it/s]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 855/3334 [00:13<00:37, 66.32it/s]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 862/3334 [00:13<00:37, 66.60it/s]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 869/3334 [00:14<00:37, 66.26it/s]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 876/3334 [00:14<00:36, 66.56it/s]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 883/3334 [00:14<00:36, 67.50it/s]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 890/3334 [00:14<00:36, 66.43it/s]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 897/3334 [00:14<00:45, 53.47it/s]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 903/3334 [00:14<00:49, 49.26it/s]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 910/3334 [00:14<00:46, 52.68it/s]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 917/3334 [00:14<00:43, 55.37it/s]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 925/3334 [00:15<00:40, 58.77it/s]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 932/3334 [00:15<00:39, 60.71it/s]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 939/3334 [00:15<00:40, 59.62it/s]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 947/3334 [00:15<00:38, 62.02it/s]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 955/3334 [00:15<00:36, 65.00it/s]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 962/3334 [00:15<00:38, 61.67it/s]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 969/3334 [00:15<00:38, 61.52it/s]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 976/3334 [00:15<00:37, 62.31it/s]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 983/3334 [00:16<00:40, 58.44it/s]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 990/3334 [00:16<00:39, 59.48it/s]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 997/3334 [00:16<00:40, 57.14it/s]\u001b[A\n",
            "Epoch 1:  30%|███       | 1004/3334 [00:16<00:40, 58.24it/s]\u001b[A\n",
            "Epoch 1:  30%|███       | 1010/3334 [00:16<00:39, 58.66it/s]\u001b[A\n",
            "Epoch 1:  30%|███       | 1016/3334 [00:16<00:39, 58.43it/s]\u001b[A\n",
            "Epoch 1:  31%|███       | 1023/3334 [00:16<00:38, 60.06it/s]\u001b[A\n",
            "Epoch 1:  31%|███       | 1030/3334 [00:16<00:38, 59.51it/s]\u001b[A\n",
            "Epoch 1:  31%|███       | 1036/3334 [00:16<00:40, 57.41it/s]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 1042/3334 [00:17<00:40, 56.70it/s]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 1048/3334 [00:17<00:39, 57.47it/s]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 1054/3334 [00:17<00:41, 55.58it/s]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 1060/3334 [00:17<00:40, 55.85it/s]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 1066/3334 [00:17<00:41, 55.00it/s]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 1073/3334 [00:17<00:38, 58.38it/s]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 1080/3334 [00:17<00:38, 58.19it/s]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 1086/3334 [00:17<00:38, 58.60it/s]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 1092/3334 [00:17<00:41, 53.74it/s]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 1099/3334 [00:18<00:40, 55.70it/s]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 1105/3334 [00:18<00:39, 55.87it/s]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 1111/3334 [00:18<00:41, 53.92it/s]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 1117/3334 [00:18<00:40, 54.63it/s]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 1123/3334 [00:18<00:41, 53.29it/s]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 1129/3334 [00:18<00:41, 53.12it/s]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 1136/3334 [00:18<00:38, 56.90it/s]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 1142/3334 [00:18<00:38, 57.08it/s]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 1148/3334 [00:18<00:39, 55.86it/s]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 1154/3334 [00:19<00:38, 56.04it/s]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 1161/3334 [00:19<00:37, 58.62it/s]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 1167/3334 [00:19<00:37, 58.15it/s]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 1173/3334 [00:19<00:40, 53.99it/s]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 1179/3334 [00:19<00:41, 52.00it/s]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 1185/3334 [00:19<00:39, 54.09it/s]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 1191/3334 [00:19<00:38, 55.06it/s]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 1197/3334 [00:19<00:38, 55.49it/s]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 1203/3334 [00:19<00:38, 55.05it/s]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 1209/3334 [00:20<00:39, 53.15it/s]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 1215/3334 [00:20<00:39, 54.20it/s]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 1222/3334 [00:20<00:37, 56.84it/s]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 1229/3334 [00:20<00:36, 58.09it/s]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 1235/3334 [00:20<00:36, 57.12it/s]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 1241/3334 [00:20<00:37, 55.31it/s]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 1247/3334 [00:20<00:37, 56.34it/s]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 1254/3334 [00:20<00:36, 57.48it/s]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 1260/3334 [00:20<00:36, 56.71it/s]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 1267/3334 [00:21<00:35, 58.94it/s]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 1273/3334 [00:21<00:35, 57.43it/s]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 1279/3334 [00:21<00:36, 56.79it/s]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 1286/3334 [00:21<00:34, 58.88it/s]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 1293/3334 [00:21<00:34, 59.39it/s]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 1299/3334 [00:21<00:34, 59.13it/s]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 1305/3334 [00:21<00:35, 57.39it/s]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 1311/3334 [00:21<00:35, 57.13it/s]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 1317/3334 [00:21<00:37, 54.14it/s]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 1323/3334 [00:22<00:36, 55.43it/s]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 1330/3334 [00:22<00:34, 57.79it/s]\u001b[A\n",
            "Epoch 1:  40%|████      | 1338/3334 [00:22<00:32, 60.79it/s]\u001b[A\n",
            "Epoch 1:  40%|████      | 1345/3334 [00:22<00:33, 59.52it/s]\u001b[A\n",
            "Epoch 1:  41%|████      | 1352/3334 [00:22<00:33, 59.64it/s]\u001b[A\n",
            "Epoch 1:  41%|████      | 1358/3334 [00:22<00:35, 56.27it/s]\u001b[A\n",
            "Epoch 1:  41%|████      | 1364/3334 [00:22<00:35, 55.43it/s]\u001b[A\n",
            "Epoch 1:  41%|████      | 1370/3334 [00:22<00:34, 56.53it/s]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 1376/3334 [00:22<00:34, 57.25it/s]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 1382/3334 [00:23<00:33, 57.74it/s]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 1389/3334 [00:23<00:32, 60.66it/s]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 1396/3334 [00:23<00:32, 60.55it/s]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 1403/3334 [00:23<00:33, 57.34it/s]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 1409/3334 [00:23<00:40, 47.07it/s]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 1415/3334 [00:23<00:42, 45.56it/s]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 1420/3334 [00:23<00:42, 45.36it/s]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 1426/3334 [00:23<00:39, 48.80it/s]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 1433/3334 [00:24<00:35, 53.13it/s]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 1441/3334 [00:24<00:32, 58.73it/s]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 1448/3334 [00:24<00:30, 61.50it/s]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 1455/3334 [00:24<00:33, 55.93it/s]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 1461/3334 [00:24<00:40, 46.56it/s]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 1467/3334 [00:24<00:39, 47.30it/s]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 1475/3334 [00:24<00:34, 54.27it/s]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 1481/3334 [00:24<00:33, 54.58it/s]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 1488/3334 [00:25<00:32, 57.53it/s]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 1495/3334 [00:25<00:30, 60.81it/s]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 1502/3334 [00:25<00:29, 61.51it/s]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 1510/3334 [00:25<00:27, 65.47it/s]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 1517/3334 [00:25<00:28, 62.85it/s]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 1524/3334 [00:25<00:28, 63.15it/s]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 1531/3334 [00:25<00:27, 64.62it/s]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 1538/3334 [00:25<00:28, 63.19it/s]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 1545/3334 [00:25<00:28, 63.87it/s]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 1552/3334 [00:26<00:27, 65.44it/s]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 1559/3334 [00:26<00:28, 63.24it/s]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 1566/3334 [00:26<00:27, 65.00it/s]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 1573/3334 [00:26<00:26, 66.00it/s]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 1580/3334 [00:26<00:26, 66.76it/s]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 1587/3334 [00:26<00:27, 64.40it/s]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 1594/3334 [00:26<00:26, 64.51it/s]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 1601/3334 [00:26<00:26, 64.64it/s]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 1608/3334 [00:26<00:26, 65.22it/s]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 1615/3334 [00:27<00:25, 66.50it/s]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 1622/3334 [00:27<00:25, 66.19it/s]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 1630/3334 [00:27<00:25, 67.30it/s]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 1637/3334 [00:27<00:26, 64.78it/s]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 1644/3334 [00:27<00:26, 64.07it/s]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 1652/3334 [00:27<00:24, 68.43it/s]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 1659/3334 [00:27<00:25, 64.92it/s]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 1666/3334 [00:27<00:25, 64.36it/s]\u001b[A\n",
            "Epoch 1:  50%|█████     | 1673/3334 [00:27<00:26, 63.43it/s]\u001b[A\n",
            "Epoch 1:  50%|█████     | 1680/3334 [00:28<00:25, 64.80it/s]\u001b[A\n",
            "Epoch 1:  51%|█████     | 1687/3334 [00:28<00:26, 63.14it/s]\u001b[A\n",
            "Epoch 1:  51%|█████     | 1694/3334 [00:28<00:25, 63.32it/s]\u001b[A\n",
            "Epoch 1:  51%|█████     | 1702/3334 [00:28<00:24, 67.22it/s]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 1709/3334 [00:28<00:24, 66.23it/s]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 1716/3334 [00:28<00:24, 66.74it/s]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 1723/3334 [00:28<00:24, 66.99it/s]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 1730/3334 [00:28<00:23, 67.63it/s]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 1737/3334 [00:28<00:24, 65.56it/s]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 1744/3334 [00:28<00:25, 63.59it/s]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 1751/3334 [00:29<00:24, 63.62it/s]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 1758/3334 [00:29<00:24, 65.23it/s]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 1765/3334 [00:29<00:24, 64.24it/s]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 1772/3334 [00:29<00:25, 61.74it/s]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 1780/3334 [00:29<00:23, 65.52it/s]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 1789/3334 [00:29<00:22, 69.09it/s]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 1796/3334 [00:29<00:22, 68.04it/s]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 1803/3334 [00:29<00:23, 66.32it/s]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 1810/3334 [00:30<00:23, 64.99it/s]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 1818/3334 [00:30<00:22, 67.65it/s]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 1825/3334 [00:30<00:22, 68.12it/s]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 1833/3334 [00:30<00:21, 70.19it/s]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 1841/3334 [00:30<00:21, 69.30it/s]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 1848/3334 [00:30<00:21, 68.17it/s]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 1855/3334 [00:30<00:22, 65.98it/s]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 1862/3334 [00:30<00:22, 66.14it/s]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 1869/3334 [00:30<00:22, 66.19it/s]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 1876/3334 [00:30<00:22, 64.41it/s]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 1883/3334 [00:31<00:22, 65.86it/s]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 1890/3334 [00:31<00:21, 65.89it/s]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 1897/3334 [00:31<00:21, 65.83it/s]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 1904/3334 [00:31<00:21, 65.26it/s]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 1911/3334 [00:31<00:21, 65.89it/s]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 1918/3334 [00:31<00:21, 65.47it/s]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 1926/3334 [00:31<00:21, 66.30it/s]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 1933/3334 [00:31<00:20, 66.81it/s]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 1940/3334 [00:31<00:21, 63.59it/s]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 1947/3334 [00:32<00:21, 64.06it/s]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 1954/3334 [00:32<00:22, 61.42it/s]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 1961/3334 [00:32<00:22, 60.57it/s]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 1968/3334 [00:32<00:22, 60.01it/s]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 1975/3334 [00:32<00:22, 60.60it/s]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 1982/3334 [00:32<00:22, 60.38it/s]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 1990/3334 [00:32<00:21, 63.67it/s]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 1997/3334 [00:32<00:21, 61.69it/s]\u001b[A\n",
            "Epoch 1:  60%|██████    | 2004/3334 [00:33<00:21, 60.58it/s]\u001b[A\n",
            "Epoch 1:  60%|██████    | 2011/3334 [00:33<00:21, 61.06it/s]\u001b[A\n",
            "Epoch 1:  61%|██████    | 2018/3334 [00:33<00:22, 58.56it/s]\u001b[A\n",
            "Epoch 1:  61%|██████    | 2025/3334 [00:33<00:22, 58.93it/s]\u001b[A\n",
            "Epoch 1:  61%|██████    | 2032/3334 [00:33<00:21, 60.11it/s]\u001b[A\n",
            "Epoch 1:  61%|██████    | 2039/3334 [00:33<00:21, 61.35it/s]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 2046/3334 [00:33<00:20, 62.79it/s]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 2053/3334 [00:33<00:20, 62.74it/s]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 2060/3334 [00:33<00:21, 59.72it/s]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 2067/3334 [00:34<00:20, 62.44it/s]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 2074/3334 [00:34<00:19, 64.12it/s]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 2081/3334 [00:34<00:19, 63.65it/s]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 2088/3334 [00:34<00:19, 62.49it/s]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 2095/3334 [00:34<00:19, 62.51it/s]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 2102/3334 [00:34<00:20, 61.47it/s]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 2109/3334 [00:34<00:19, 62.53it/s]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 2116/3334 [00:34<00:20, 59.71it/s]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 2123/3334 [00:34<00:20, 58.43it/s]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 2129/3334 [00:35<00:21, 57.14it/s]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 2137/3334 [00:35<00:19, 62.12it/s]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 2144/3334 [00:35<00:19, 62.47it/s]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 2151/3334 [00:35<00:19, 61.93it/s]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 2158/3334 [00:35<00:19, 61.64it/s]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 2165/3334 [00:35<00:20, 58.42it/s]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 2172/3334 [00:35<00:19, 59.91it/s]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 2179/3334 [00:35<00:19, 58.47it/s]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 2185/3334 [00:36<00:19, 58.69it/s]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 2192/3334 [00:36<00:18, 60.76it/s]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 2199/3334 [00:36<00:18, 59.85it/s]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 2206/3334 [00:36<00:18, 61.13it/s]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 2213/3334 [00:36<00:18, 61.15it/s]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 2220/3334 [00:36<00:17, 62.47it/s]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 2227/3334 [00:36<00:18, 59.68it/s]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 2234/3334 [00:36<00:17, 61.14it/s]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 2241/3334 [00:36<00:18, 60.01it/s]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 2248/3334 [00:37<00:18, 59.04it/s]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 2254/3334 [00:37<00:18, 57.35it/s]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 2260/3334 [00:37<00:18, 57.98it/s]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 2266/3334 [00:37<00:19, 56.21it/s]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 2272/3334 [00:37<00:19, 55.70it/s]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 2278/3334 [00:37<00:18, 56.40it/s]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 2285/3334 [00:37<00:17, 58.33it/s]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 2291/3334 [00:37<00:18, 57.69it/s]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 2297/3334 [00:37<00:18, 56.01it/s]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 2304/3334 [00:38<00:17, 57.37it/s]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 2310/3334 [00:38<00:18, 56.73it/s]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 2316/3334 [00:38<00:18, 55.15it/s]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 2323/3334 [00:38<00:17, 57.43it/s]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 2329/3334 [00:38<00:17, 56.22it/s]\u001b[A\n",
            "Epoch 1:  70%|███████   | 2335/3334 [00:38<00:18, 53.72it/s]\u001b[A\n",
            "Epoch 1:  70%|███████   | 2342/3334 [00:38<00:17, 55.35it/s]\u001b[A\n",
            "Epoch 1:  70%|███████   | 2349/3334 [00:38<00:17, 57.66it/s]\u001b[A\n",
            "Epoch 1:  71%|███████   | 2355/3334 [00:38<00:17, 55.35it/s]\u001b[A\n",
            "Epoch 1:  71%|███████   | 2361/3334 [00:39<00:18, 53.87it/s]\u001b[A\n",
            "Epoch 1:  71%|███████   | 2367/3334 [00:39<00:17, 54.59it/s]\u001b[A\n",
            "Epoch 1:  71%|███████   | 2373/3334 [00:39<00:17, 54.45it/s]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 2379/3334 [00:39<00:18, 52.62it/s]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 2386/3334 [00:39<00:17, 54.90it/s]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 2392/3334 [00:39<00:17, 54.25it/s]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 2398/3334 [00:39<00:16, 55.12it/s]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 2404/3334 [00:39<00:16, 55.74it/s]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 2410/3334 [00:39<00:17, 54.18it/s]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 2416/3334 [00:40<00:16, 55.46it/s]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 2423/3334 [00:40<00:15, 58.94it/s]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 2429/3334 [00:40<00:15, 59.00it/s]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 2437/3334 [00:40<00:14, 62.82it/s]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 2444/3334 [00:40<00:13, 63.67it/s]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 2451/3334 [00:40<00:13, 63.69it/s]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 2458/3334 [00:40<00:13, 64.31it/s]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 2465/3334 [00:40<00:13, 64.47it/s]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 2472/3334 [00:40<00:13, 64.81it/s]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 2479/3334 [00:41<00:13, 62.89it/s]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 2486/3334 [00:41<00:13, 63.82it/s]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 2494/3334 [00:41<00:12, 66.38it/s]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 2501/3334 [00:41<00:12, 66.16it/s]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 2508/3334 [00:41<00:13, 62.50it/s]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 2515/3334 [00:41<00:13, 62.51it/s]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 2522/3334 [00:41<00:12, 63.37it/s]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 2529/3334 [00:41<00:12, 64.47it/s]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 2537/3334 [00:41<00:12, 63.85it/s]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 2545/3334 [00:42<00:12, 65.40it/s]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 2552/3334 [00:42<00:11, 65.51it/s]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 2559/3334 [00:42<00:11, 66.47it/s]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 2566/3334 [00:42<00:11, 66.02it/s]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 2573/3334 [00:42<00:11, 65.58it/s]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 2580/3334 [00:42<00:11, 66.59it/s]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 2587/3334 [00:42<00:11, 65.27it/s]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 2594/3334 [00:42<00:11, 64.39it/s]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 2601/3334 [00:42<00:11, 62.71it/s]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 2608/3334 [00:43<00:11, 62.57it/s]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 2615/3334 [00:43<00:11, 64.10it/s]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 2623/3334 [00:43<00:10, 66.71it/s]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 2630/3334 [00:43<00:10, 66.07it/s]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 2638/3334 [00:43<00:10, 69.50it/s]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 2645/3334 [00:43<00:10, 67.69it/s]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 2653/3334 [00:43<00:10, 68.08it/s]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 2660/3334 [00:43<00:10, 65.71it/s]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 2667/3334 [00:43<00:10, 64.57it/s]\u001b[A\n",
            "Epoch 1:  80%|████████  | 2674/3334 [00:44<00:10, 64.47it/s]\u001b[A\n",
            "Epoch 1:  80%|████████  | 2682/3334 [00:44<00:09, 66.62it/s]\u001b[A\n",
            "Epoch 1:  81%|████████  | 2689/3334 [00:44<00:09, 67.32it/s]\u001b[A\n",
            "Epoch 1:  81%|████████  | 2697/3334 [00:44<00:09, 68.55it/s]\u001b[A\n",
            "Epoch 1:  81%|████████  | 2704/3334 [00:44<00:09, 66.66it/s]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 2711/3334 [00:44<00:09, 62.67it/s]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 2718/3334 [00:44<00:10, 58.40it/s]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 2724/3334 [00:44<00:10, 58.62it/s]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 2731/3334 [00:44<00:09, 60.57it/s]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 2739/3334 [00:45<00:09, 64.64it/s]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 2746/3334 [00:45<00:09, 64.50it/s]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 2753/3334 [00:45<00:08, 65.17it/s]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 2761/3334 [00:45<00:08, 66.93it/s]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 2768/3334 [00:45<00:08, 66.41it/s]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 2775/3334 [00:45<00:08, 66.20it/s]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 2783/3334 [00:45<00:08, 67.78it/s]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 2790/3334 [00:45<00:08, 66.07it/s]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 2797/3334 [00:45<00:08, 64.04it/s]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 2804/3334 [00:46<00:08, 64.87it/s]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 2811/3334 [00:46<00:08, 63.92it/s]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 2818/3334 [00:46<00:07, 64.71it/s]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 2825/3334 [00:46<00:08, 63.22it/s]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 2832/3334 [00:46<00:08, 62.74it/s]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 2839/3334 [00:46<00:07, 62.22it/s]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 2847/3334 [00:46<00:07, 66.97it/s]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 2855/3334 [00:46<00:07, 68.31it/s]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 2862/3334 [00:46<00:07, 63.55it/s]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 2869/3334 [00:47<00:07, 61.40it/s]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 2876/3334 [00:47<00:07, 61.69it/s]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 2883/3334 [00:47<00:07, 61.85it/s]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 2891/3334 [00:47<00:07, 63.04it/s]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 2898/3334 [00:47<00:07, 61.22it/s]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 2905/3334 [00:47<00:06, 61.90it/s]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 2912/3334 [00:47<00:06, 63.31it/s]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 2919/3334 [00:47<00:06, 64.76it/s]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 2926/3334 [00:47<00:06, 62.39it/s]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 2933/3334 [00:48<00:06, 61.04it/s]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 2940/3334 [00:48<00:06, 63.29it/s]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 2948/3334 [00:48<00:05, 65.24it/s]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 2955/3334 [00:48<00:06, 62.13it/s]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 2962/3334 [00:48<00:05, 62.15it/s]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 2969/3334 [00:48<00:05, 63.28it/s]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 2976/3334 [00:48<00:05, 64.55it/s]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 2983/3334 [00:48<00:05, 65.17it/s]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 2990/3334 [00:48<00:05, 64.70it/s]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 2997/3334 [00:49<00:05, 64.15it/s]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 3004/3334 [00:49<00:05, 64.53it/s]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 3011/3334 [00:49<00:05, 63.56it/s]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 3018/3334 [00:49<00:04, 64.22it/s]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 3025/3334 [00:49<00:04, 64.26it/s]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 3032/3334 [00:49<00:04, 64.53it/s]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 3039/3334 [00:49<00:04, 64.63it/s]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 3047/3334 [00:49<00:04, 66.68it/s]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 3054/3334 [00:49<00:04, 64.41it/s]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 3061/3334 [00:50<00:04, 60.98it/s]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 3068/3334 [00:50<00:04, 61.76it/s]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 3075/3334 [00:50<00:04, 60.64it/s]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 3082/3334 [00:50<00:04, 61.49it/s]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 3089/3334 [00:50<00:03, 62.12it/s]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 3096/3334 [00:50<00:03, 61.15it/s]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 3103/3334 [00:50<00:03, 61.03it/s]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 3110/3334 [00:50<00:03, 61.10it/s]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 3117/3334 [00:51<00:03, 59.07it/s]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 3124/3334 [00:51<00:03, 60.38it/s]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 3131/3334 [00:51<00:04, 46.77it/s]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 3137/3334 [00:51<00:05, 35.44it/s]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 3142/3334 [00:51<00:06, 30.19it/s]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 3146/3334 [00:52<00:06, 28.58it/s]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 3150/3334 [00:52<00:06, 29.68it/s]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 3156/3334 [00:52<00:05, 35.28it/s]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 3162/3334 [00:52<00:04, 39.25it/s]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 3167/3334 [00:52<00:04, 38.13it/s]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 3172/3334 [00:52<00:05, 28.72it/s]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 3176/3334 [00:53<00:06, 24.71it/s]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 3179/3334 [00:53<00:06, 23.46it/s]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 3183/3334 [00:53<00:05, 25.70it/s]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 3186/3334 [00:53<00:05, 26.35it/s]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 3192/3334 [00:53<00:04, 32.58it/s]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 3199/3334 [00:53<00:03, 40.72it/s]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 3205/3334 [00:53<00:02, 43.49it/s]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 3210/3334 [00:53<00:02, 42.35it/s]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 3215/3334 [00:54<00:03, 35.08it/s]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 3219/3334 [00:54<00:04, 28.15it/s]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 3223/3334 [00:54<00:04, 26.44it/s]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 3226/3334 [00:54<00:04, 26.21it/s]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 3231/3334 [00:54<00:03, 30.29it/s]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 3236/3334 [00:54<00:02, 34.74it/s]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 3242/3334 [00:54<00:02, 40.49it/s]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 3249/3334 [00:55<00:01, 46.39it/s]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 3255/3334 [00:55<00:01, 48.12it/s]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 3262/3334 [00:55<00:01, 52.28it/s]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 3269/3334 [00:55<00:01, 56.98it/s]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 3275/3334 [00:55<00:01, 55.63it/s]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 3281/3334 [00:55<00:00, 56.64it/s]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 3287/3334 [00:55<00:00, 55.65it/s]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 3294/3334 [00:55<00:00, 57.32it/s]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 3300/3334 [00:55<00:00, 56.71it/s]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 3306/3334 [00:56<00:00, 56.07it/s]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 3312/3334 [00:56<00:00, 56.35it/s]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 3318/3334 [00:56<00:00, 56.51it/s]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 3324/3334 [00:56<00:00, 55.51it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 3334/3334 [00:56<00:00, 58.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average training loss: 24.9981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epochs: 100%|██████████| 1/1 [01:07<00:00, 67.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 12.5464, Average Next Sentence Loss: 0.7335, Average Mask Loss: 11.8129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "# Training loop setup\n",
        "num_epochs = 1\n",
        "total_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "# Define the number of warmup steps, e.g., 10% of total\n",
        "warmup_steps = int(total_steps * 0.1)\n",
        "\n",
        "# Create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=warmup_steps,\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "# Lists to store losses for plotting\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
        "        bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        next_sentence_prediction, masked_language = model(bert_inputs, segment_labels)\n",
        "\n",
        "        next_loss = loss_fn_nsp(next_sentence_prediction, is_nexts)\n",
        "        mask_loss = loss_fn_mlm(masked_language.view(-1, masked_language.size(-1)), bert_labels.view(-1))\n",
        "\n",
        "        loss = next_loss + mask_loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update the learning rate\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            continue\n",
        "        else:\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader) + 1\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1} - Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation after each epoch\n",
        "    eval_loss = evaluate(test_dataloader, model, loss_fn_nsp, loss_fn_mlm, device)\n",
        "    eval_losses.append(eval_loss)"
      ],
      "execution_count": 115
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c9b1055-22e9-432d-b94a-dd325c772ab2"
      },
      "source": [
        "To evaluate the performance of a pretrained BERT model in predicting whether a second sentence follows the first, a function called `predict_nsp` is defined. The function operates as follows:\n",
        "\n",
        "1. Tokenization: The input sentences are tokenized using the `tokenizer.encode_plus` method, which returns a dictionary of tokenized inputs. These tokenized inputs are then converted into tensors and moved to the appropriate device for processing.\n",
        "\n",
        "2. Prediction: The BERT model is utilized to make predictions by passing the token and segment tensors as input.\n",
        "\n",
        "3. Logits Manipulation: The first element of the logits tensor is selected and unsqueezed to add an extra dimension, resulting in a shape of `[1, 2]`.\n",
        "\n",
        "4. Probability and Prediction: The logits are passed through a softmax function to obtain probabilities, and the prediction is obtained by taking the argmax.\n",
        "\n",
        "5. Result Interpretation: The prediction is interpreted and returned as a string, indicating whether the second sentence follows the first or not.\n",
        "\n",
        "6. Example Usage: An example usage of the `predict_nsp` function is provided, demonstrating how two example sentences can be passed to the function along with a BERT model and tokenizer. The result is printed, indicating whether the second sentence follows the first based on the model's prediction.\n",
        "\n",
        "By utilizing the `predict_nsp` function, the performance of the pretrained BERT model in determining the relationship between two sentences can be assessed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "448af968-38ca-4daa-885b-11910334f901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "a4e9be2d0b8347eaae8899b0571b6b16",
            "8ff84b99446a4a40958f84c97912145c",
            "943b7e197b4743ec9deee6f34422dd69",
            "01154f9f367a4cdebdcab91defd64b17",
            "4c512b3149064de4a87784a5cf9da96d",
            "130450a39f0940df81f604edb9ba418e",
            "2090eec6d14a416dbe81cfe08e734e76",
            "137f57bfce8e482c926146dc742af4c1",
            "0648a652497543b28875429411f8a003",
            "037847ad8751451f898e7e4ebc041622",
            "67ea382277804945bdd4f75288eec1cd",
            "60220e17973f4064a504f4a7a3f6b2f8",
            "e0f397955f62444db5337a81cb3993fc",
            "54076c4514bc496d96cfb943eba72657",
            "4d2a00c5060f4d9f84437eecb9a6c0ab",
            "bd54acc0daa14e39af2d56c2b95a4405",
            "2ac22c3ed50049dca3212e683774bbf2",
            "5acf9db8c5c841648c1c7a9a7a9b05b8",
            "d28ffa11bcb54a92af41d40a0bdd7359",
            "3ccab5210f1f40c8befd78c9a12d7499",
            "e56ef62dc52643fab7be17d87db23ff5",
            "dc5d97b79a194e53951866c9c20b5d8d",
            "eca9d00905da44b5848c994e95717607",
            "a645b7bbeca145309965bc5a03ee7abb",
            "0e6957c91c324be692a065407208c49d",
            "2558a0319efe4c079e534fde8a1d6e38",
            "561b2ca527484d09b216e9c5b280ea2b",
            "f127fd1ba4c54d1992c4529a9a965fb4",
            "46c49cfe08aa45dfa808724142aa8b66",
            "60fcc551bdef4b358287f22b26c6b3eb",
            "ed6e556b4e964775913a2949a0c17527",
            "1d67795622234b6aa0f5a38c6915e28a",
            "20730557c3274baab2ae442f90f4d9f5",
            "6c008efb74c4431c8dbfa28289060fdc",
            "16e9553333be4bc3be711601e68eff6c",
            "5dec4a8a0a46429aab98da4ed263c49c",
            "bd59ece138d0480e9b11605919751aeb",
            "f0b896a8d2664f339aaf244b6478c096",
            "4828ac81c21c48878758ff0fdbfb7991",
            "e55832ea48a644c090cb9d154ddb0036",
            "08b043c4c67647ec8f03ec62caed9b4b",
            "2b96c9dd0d044946b4bbff1dfbd0c85e",
            "9ef0846b58294f4fa9aef1397cc71b79",
            "3af3e03393f6475ea1376a22efeb61cf"
          ]
        },
        "outputId": "cf35c4a7-dbca-4c74-d323-a959b72667bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4e9be2d0b8347eaae8899b0571b6b16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60220e17973f4064a504f4a7a3f6b2f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eca9d00905da44b5848c994e95717607"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c008efb74c4431c8dbfa28289060fdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second sentence does not follow the first\n"
          ]
        }
      ],
      "source": [
        "# Initialize the tokenizer with the BERT model's vocabulary\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "def predict_nsp(sentence1, sentence2, model, tokenizer):\n",
        "    # Tokenize sentences with special tokens\n",
        "    tokens = tokenizer.encode_plus(sentence1, sentence2, return_tensors=\"pt\")\n",
        "    tokens_tensor = tokens[\"input_ids\"].to(device)\n",
        "    segment_tensor = tokens[\"token_type_ids\"].to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        # Assuming the model returns NSP predictions first\n",
        "        nsp_prediction, _ = model(tokens_tensor, segment_tensor)\n",
        "        # Select the first element (first sequence) of the logits tensor\n",
        "        first_logits = nsp_prediction[0].unsqueeze(0)  # Adds an extra dimension, making it [1, 2]\n",
        "        logits = torch.softmax(first_logits, dim=1)\n",
        "        prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Interpret the prediction\n",
        "    return \"Second sentence follows the first\" if prediction == 1 else \"Second sentence does not follow the first\"\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"The cat sat on the mat.\"\n",
        "sentence2 = \"It was a sunny day\"\n",
        "\n",
        "print(predict_nsp(sentence1, sentence2, model, tokenizer))"
      ],
      "execution_count": 112
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b98f1b91-e9bb-4eb7-9bad-bf9bbbc9d05f"
      },
      "source": [
        "A function is defined to perform Masked Language Modeling (MLM) using a pretrained BERT model. The function operates as follows:\n",
        "\n",
        "1. Tokenization: The input sentence is tokenized using the tokenizer and converted into token IDs, including the special tokens. The tokenized sentence is stored in the `tokens_tensor` variable.\n",
        "\n",
        "2. Segment Labels: Dummy segment labels filled with zeros are created and stored as `segment_labels`.\n",
        "\n",
        "3. Prediction: The BERT model is used to make predictions by passing the token tensor and segment labels as input. The MLM logits are extracted as `predictions`.\n",
        "\n",
        "4. Mask Token Index: The position of the [MASK] token is identified using the `nonzero` method and stored in the `mask_token_index` variable. Note that all tokens except the mask token are zero-padded.\n",
        "\n",
        "5. Predicted Index: The predicted index for the [MASK] token is obtained by taking the argmax of the MLM logits at the corresponding position.\n",
        "\n",
        "6. Token Conversion: The predicted index is converted back to a token using the `convert_ids_to_tokens` method of the tokenizer.\n",
        "\n",
        "7. Replaced Sentence: The original sentence is replaced with the predicted token at the position of the [MASK] token, resulting in the predicted sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77e726f1-7cc4-4e4c-8631-cd7dfa85caff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb25fc6-8141-4b36-efdd-12a7a7f039cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sat on the [unused4].\n"
          ]
        }
      ],
      "source": [
        "def predict_mlm(sentence, model, tokenizer):\n",
        "    # Tokenize the input sentence and convert to token IDs, including special tokens\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    tokens_tensor = inputs.input_ids.to(device)\n",
        "\n",
        "    # Create dummy segment labels filled with zeros, assuming it's needed by your model\n",
        "    segment_labels = torch.zeros_like(tokens_tensor).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model, now correctly handling the output tuple\n",
        "        output_tuple = model(tokens_tensor, segment_labels)\n",
        "\n",
        "        # Assuming the second element of the tuple contains the MLM logits\n",
        "        predictions = output_tuple[1]  # Adjusted based on your model's output\n",
        "\n",
        "        # Identify the position of the [MASK] token\n",
        "        mask_token_index = (tokens_tensor == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "        # Get the predicted index for the [MASK] token from the MLM logits\n",
        "        predicted_index = torch.argmax(predictions[0, mask_token_index.item(), :], dim=-1)\n",
        "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
        "\n",
        "        # Replace [MASK] in the original sentence with the predicted token\n",
        "        predicted_sentence = sentence.replace(tokenizer.mask_token, predicted_token, 1)\n",
        "\n",
        "    return predicted_sentence\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentence = \"The cat sat on the [MASK].\"\n",
        "print(predict_mlm(sentence, model, tokenizer))"
      ],
      "execution_count": 122
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "prev_pub_hash": "f81e7c06d2b733d2f221088b8dc36a4290af8f068916988a507edf37176e5278",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4e9be2d0b8347eaae8899b0571b6b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ff84b99446a4a40958f84c97912145c",
              "IPY_MODEL_943b7e197b4743ec9deee6f34422dd69",
              "IPY_MODEL_01154f9f367a4cdebdcab91defd64b17"
            ],
            "layout": "IPY_MODEL_4c512b3149064de4a87784a5cf9da96d"
          }
        },
        "8ff84b99446a4a40958f84c97912145c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_130450a39f0940df81f604edb9ba418e",
            "placeholder": "​",
            "style": "IPY_MODEL_2090eec6d14a416dbe81cfe08e734e76",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "943b7e197b4743ec9deee6f34422dd69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137f57bfce8e482c926146dc742af4c1",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0648a652497543b28875429411f8a003",
            "value": 48
          }
        },
        "01154f9f367a4cdebdcab91defd64b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_037847ad8751451f898e7e4ebc041622",
            "placeholder": "​",
            "style": "IPY_MODEL_67ea382277804945bdd4f75288eec1cd",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.27kB/s]"
          }
        },
        "4c512b3149064de4a87784a5cf9da96d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130450a39f0940df81f604edb9ba418e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2090eec6d14a416dbe81cfe08e734e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "137f57bfce8e482c926146dc742af4c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0648a652497543b28875429411f8a003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "037847ad8751451f898e7e4ebc041622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67ea382277804945bdd4f75288eec1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60220e17973f4064a504f4a7a3f6b2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0f397955f62444db5337a81cb3993fc",
              "IPY_MODEL_54076c4514bc496d96cfb943eba72657",
              "IPY_MODEL_4d2a00c5060f4d9f84437eecb9a6c0ab"
            ],
            "layout": "IPY_MODEL_bd54acc0daa14e39af2d56c2b95a4405"
          }
        },
        "e0f397955f62444db5337a81cb3993fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ac22c3ed50049dca3212e683774bbf2",
            "placeholder": "​",
            "style": "IPY_MODEL_5acf9db8c5c841648c1c7a9a7a9b05b8",
            "value": "vocab.txt: 100%"
          }
        },
        "54076c4514bc496d96cfb943eba72657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d28ffa11bcb54a92af41d40a0bdd7359",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ccab5210f1f40c8befd78c9a12d7499",
            "value": 231508
          }
        },
        "4d2a00c5060f4d9f84437eecb9a6c0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e56ef62dc52643fab7be17d87db23ff5",
            "placeholder": "​",
            "style": "IPY_MODEL_dc5d97b79a194e53951866c9c20b5d8d",
            "value": " 232k/232k [00:00&lt;00:00, 3.00MB/s]"
          }
        },
        "bd54acc0daa14e39af2d56c2b95a4405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac22c3ed50049dca3212e683774bbf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5acf9db8c5c841648c1c7a9a7a9b05b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d28ffa11bcb54a92af41d40a0bdd7359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ccab5210f1f40c8befd78c9a12d7499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e56ef62dc52643fab7be17d87db23ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc5d97b79a194e53951866c9c20b5d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eca9d00905da44b5848c994e95717607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a645b7bbeca145309965bc5a03ee7abb",
              "IPY_MODEL_0e6957c91c324be692a065407208c49d",
              "IPY_MODEL_2558a0319efe4c079e534fde8a1d6e38"
            ],
            "layout": "IPY_MODEL_561b2ca527484d09b216e9c5b280ea2b"
          }
        },
        "a645b7bbeca145309965bc5a03ee7abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f127fd1ba4c54d1992c4529a9a965fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_46c49cfe08aa45dfa808724142aa8b66",
            "value": "tokenizer.json: 100%"
          }
        },
        "0e6957c91c324be692a065407208c49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60fcc551bdef4b358287f22b26c6b3eb",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed6e556b4e964775913a2949a0c17527",
            "value": 466062
          }
        },
        "2558a0319efe4c079e534fde8a1d6e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d67795622234b6aa0f5a38c6915e28a",
            "placeholder": "​",
            "style": "IPY_MODEL_20730557c3274baab2ae442f90f4d9f5",
            "value": " 466k/466k [00:00&lt;00:00, 8.14MB/s]"
          }
        },
        "561b2ca527484d09b216e9c5b280ea2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f127fd1ba4c54d1992c4529a9a965fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c49cfe08aa45dfa808724142aa8b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60fcc551bdef4b358287f22b26c6b3eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed6e556b4e964775913a2949a0c17527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d67795622234b6aa0f5a38c6915e28a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20730557c3274baab2ae442f90f4d9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c008efb74c4431c8dbfa28289060fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16e9553333be4bc3be711601e68eff6c",
              "IPY_MODEL_5dec4a8a0a46429aab98da4ed263c49c",
              "IPY_MODEL_bd59ece138d0480e9b11605919751aeb"
            ],
            "layout": "IPY_MODEL_f0b896a8d2664f339aaf244b6478c096"
          }
        },
        "16e9553333be4bc3be711601e68eff6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4828ac81c21c48878758ff0fdbfb7991",
            "placeholder": "​",
            "style": "IPY_MODEL_e55832ea48a644c090cb9d154ddb0036",
            "value": "config.json: 100%"
          }
        },
        "5dec4a8a0a46429aab98da4ed263c49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b043c4c67647ec8f03ec62caed9b4b",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b96c9dd0d044946b4bbff1dfbd0c85e",
            "value": 570
          }
        },
        "bd59ece138d0480e9b11605919751aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ef0846b58294f4fa9aef1397cc71b79",
            "placeholder": "​",
            "style": "IPY_MODEL_3af3e03393f6475ea1376a22efeb61cf",
            "value": " 570/570 [00:00&lt;00:00, 17.8kB/s]"
          }
        },
        "f0b896a8d2664f339aaf244b6478c096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4828ac81c21c48878758ff0fdbfb7991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e55832ea48a644c090cb9d154ddb0036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08b043c4c67647ec8f03ec62caed9b4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b96c9dd0d044946b4bbff1dfbd0c85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ef0846b58294f4fa9aef1397cc71b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3af3e03393f6475ea1376a22efeb61cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}