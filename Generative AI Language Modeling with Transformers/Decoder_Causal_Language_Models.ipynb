{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogukartal/IBM_AI_Labs/blob/main/Generative%20AI%20Language%20Modeling%20with%20Transformers/Decoder_Causal_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ec00cbc-2930-4668-b90a-578059b61462"
      },
      "source": [
        "# Decoder Causal Language Models\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60c67577-0c70-482f-98a1-742bcde71eb5"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9e79798-ec0c-45e9-8047-6dd995f9335e"
      },
      "outputs": [],
      "source": [
        "!pip install -qq torch==2.0.0\n",
        "!pip install -Uqq portalocker>=2.0.0\n",
        "!pip install -qq torchtext==0.15.1\n",
        "!pip install -qq torchdata==0.6.0\n",
        "!pip install -qq matplotlib\n",
        "!pip install -qq transformers"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45a902d6-0ebe-4819-956e-309b73a1d51a"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from typing import Iterable, List\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.datasets import IMDB,PennTreebank\n",
        "from transformers import GPT2Tokenizer\n",
        "import time\n",
        "from torch.optim import Adam\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a92dc50e-c18e-490f-a642-9be373666919",
        "outputId": "fd20f5bd-4dfe-43bf-ca8a-b5fa3c9428ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546c1de7-0a6c-427e-b66b-9491efcd6be0"
      },
      "source": [
        "## Text pipeline\n",
        "### Dataset\n",
        "\n",
        "Some datasets:\n",
        "* [PennTreebank](https://pytorch.org/text/0.8.1/datasets.html#penntreebank)\n",
        "* [WikiText-2](https://pytorch.org/text/0.8.1/datasets.html#wikitext-2)\n",
        "* [WikiText103](https://pytorch.org/text/0.8.1/datasets.html#wikitext103)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cf30594-8511-4440-979f-6af034232162",
        "outputId": "edfa341b-c286-46d0-c5d8-4bc080fbdb0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Load the dataset\n",
        "train_iter, val_iter = IMDB()\n",
        "\n",
        "data_itr = iter(train_iter)\n",
        "\n",
        "next(data_itr)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d569e994-da64-481c-8fba-b3849b2ce707"
      },
      "source": [
        "\n",
        "### Preprocessing data\n",
        "\n",
        "- **Special Symbols and Indices**: Initializes special tokens (`<unk>`, `<pad>`, and an empty string for EOS) with their corresponding indices (`0`, `1`, and `2`).\n",
        "    - `UNK_IDX`: Index for unknown words.\n",
        "    - `PAD_IDX`: Index used for padding shorter sentences in a batch to ensure uniform length.\n",
        "    - `EOS_IDX`: Index representing the end of a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17361237-1632-4e09-ba03-657ece1acde8"
      },
      "outputs": [],
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6d22ab3-0dcc-4245-897c-984348b9b577"
      },
      "outputs": [],
      "source": [
        "# Generator function iterating through a dataset , tokenizing each data sample and yields one tokenized sample at a time\n",
        "def yield_tokens(data_iter):\n",
        "\n",
        "    for _,data_sample in data_iter:\n",
        "        yield  tokenizer(data_sample)\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Constructs a vocabulary from the tokenized dataset\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)\n",
        "\n",
        "# Sets a default index for tokens not found in the vocabulary\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "# Converts a given text into a sequence of indices based on the built vocabulary\n",
        "text_to_index = lambda text: [vocab(token) for token in tokenizer(text)]\n",
        "\n",
        "# Transforms a sequence of indices back into a readable string\n",
        "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "61c0231e-79be-4c5a-9aad-a541d5880e3b",
        "outputId": "5d0f7ba3-2b6c-4ac5-8983-2222948f1e94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<unk> <pad> <|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Check\n",
        "index_to_en(torch.tensor([0,1,2]))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3957c8f1-8652-4ff2-ab96-dc1b1e551dbf"
      },
      "source": [
        "### Collate function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe6060e5-9d06-4cae-958e-16c4da49f104"
      },
      "outputs": [],
      "source": [
        "# This function generates a random text sample(src_sequence) and its subsequent sequence(tgt_sequence) from a given text for language model training\n",
        "def get_sample(block_size, text):\n",
        "    # Determine the length of the input text\n",
        "    sample_leg = len(text)\n",
        "    # Calculate the stopping point for randomly selecting a sample\n",
        "    # This ensures the selected sample doesn't exceed the text length\n",
        "    random_sample_stop = sample_leg - block_size\n",
        "\n",
        "\n",
        "    # Check if a random sample can be taken (if the text is longer than block_size)\n",
        "    if random_sample_stop >= 1:\n",
        "        # Randomly select a starting point for the sample\n",
        "        random_start = torch.randint(low=0, high=random_sample_stop, size=(1,)).item()\n",
        "        # Define the endpoint of the sample\n",
        "        stop = random_start + block_size\n",
        "\n",
        "        # Create the input and target sequences\n",
        "        src_sequence = text[random_start:stop]\n",
        "        tgt_sequence = text[random_start + 1:stop + 1]\n",
        "\n",
        "    # Handle the case where the text length is exactly equal or less the block size\n",
        "    elif random_sample_stop <= 0:\n",
        "        # Start from the beginning and use the entire text\n",
        "        random_start = 0\n",
        "        stop = sample_leg\n",
        "        src_sequence = text[random_start:stop]\n",
        "        tgt_sequence = text[random_start + 1:stop]\n",
        "        # Append an empty string to maintain sequence alignment\n",
        "        tgt_sequence.append( '<|endoftext|>')\n",
        "\n",
        "    return src_sequence, tgt_sequence"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14c5f767-7115-42f3-9751-999e26681c53",
        "outputId": "dada669b-bf19-4a54-975a-46c96be766c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'rented',\n",
              " 'i',\n",
              " 'am',\n",
              " 'curious-yellow',\n",
              " 'from',\n",
              " 'my',\n",
              " 'video',\n",
              " 'store',\n",
              " 'because']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "BATCH_SIZE = 1\n",
        "\n",
        "batch_of_tokens = []\n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  _, text = next(iter(train_iter))\n",
        "  batch_of_tokens.append(tokenizer(text))\n",
        "\n",
        "text = batch_of_tokens[0][0:100]\n",
        "text[0:10]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba4dbf92-7ba9-447a-a8f3-c5c8ecef139f",
        "outputId": "0d82d598-c8bb-4ded-9ea5-7fcbc5cb330f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src:  ['1967', '.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was']\n",
            "tgt:  ['.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized']\n"
          ]
        }
      ],
      "source": [
        "block_size = 10\n",
        "src_sequences, tgt_sequence = get_sample(block_size, text)\n",
        "\n",
        "# Check if it is shifted\n",
        "print(\"src: \",src_sequences)\n",
        "print(\"tgt: \",tgt_sequence)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efe96c91-c68f-4a86-824b-83fe4144ec1f",
        "outputId": "6ab9cd48-eb36-4fb2-f381-c3cee1da2e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "Source Sequence (Text): ['and', 'race', 'issues', 'in', 'the', 'united', 'states', '.', 'in', 'between']\n",
            "Source Sequence (Indices): [7, 1610, 1462, 14, 4, 2671, 1768, 3, 14, 259]\n",
            "Source Sequence (Shape): torch.Size([10])\n",
            "Target Sequence (Text): ['race', 'issues', 'in', 'the', 'united', 'states', '.', 'in', 'between', 'asking']\n",
            "Target Sequence (Indices): [1610, 1462, 14, 4, 2671, 1768, 3, 14, 259, 1743]\n",
            "Target Sequence (Shape): torch.Size([10])\n",
            "Sample 1:\n",
            "Source Sequence (Text): ['named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about']\n",
            "Source Sequence (Indices): [831, 6788, 49, 518, 10, 901, 287, 68, 59, 52]\n",
            "Source Sequence (Shape): torch.Size([10])\n",
            "Target Sequence (Text): ['lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life']\n",
            "Target Sequence (Indices): [6788, 49, 518, 10, 901, 287, 68, 59, 52, 161]\n",
            "Target Sequence (Shape): torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "## Creates batches of source (`src_batch`) and target (`tgt_batch`) sequences from a dataset for training NLP models\n",
        "# Initialize empty lists to store source and target sequences\n",
        "src_batch, tgt_batch = [], []\n",
        "\n",
        "# Define the batch size\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "# Loop to create batches of source and target sequences\n",
        "for i in range(BATCH_SIZE):\n",
        "    # Retrieve the next data point from the training iterator\n",
        "    _,text = next(iter(train_iter))\n",
        "\n",
        "    # Generate source and target sequences using the get_sample function\n",
        "    src_sequence_text, tgt_sequence_text = get_sample(block_size, tokenizer(text))\n",
        "\n",
        "    # Convert source and target sequences to tokenized vocabulary indices\n",
        "    src_sequence_indices = vocab(src_sequence_text)\n",
        "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
        "\n",
        "    # Convert the sequences to PyTorch tensors with dtype int64\n",
        "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
        "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
        "\n",
        "    # Append the source and target sequences to their respective batches\n",
        "    src_batch.append(src_sequence)\n",
        "    tgt_batch.append(tgt_sequence)\n",
        "\n",
        "    # Print the output for every 2nd sample (adjust as needed)\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(\"Source Sequence (Text):\", src_sequence_text)\n",
        "    print(\"Source Sequence (Indices):\", src_sequence_indices)\n",
        "    print(\"Source Sequence (Shape):\", src_sequence.shape)\n",
        "    print(\"Target Sequence (Text):\", tgt_sequence_text)\n",
        "    print(\"Target Sequence (Indices):\", tgt_sequence_indices)\n",
        "    print(\"Target Sequence (Shape):\", tgt_sequence.shape)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f34d6059-0234-471b-b8c7-aa19ba229cc0"
      },
      "outputs": [],
      "source": [
        "BLOCK_SIZE = 30\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for _,_textt in batch:\n",
        "      src_sequence,tgt_sequence = get_sample(BLOCK_SIZE,tokenizer(_textt))\n",
        "      src_sequence = vocab(src_sequence)\n",
        "      tgt_sequence = vocab(tgt_sequence)\n",
        "      src_sequence = torch.tensor(src_sequence, dtype=torch.int64)\n",
        "      tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)\n",
        "      src_batch.append(src_sequence)\n",
        "      tgt_batch.append(tgt_sequence)\n",
        "\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)\n",
        "\n",
        "    return src_batch.to(DEVICE), tgt_batch.to(DEVICE)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eefff862-bed3-48ed-b310-bb1df5b9b532"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1\n",
        "dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(val_iter , batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf6d15b-f5d4-4f7e-86cf-a42de1888c0e"
      },
      "source": [
        "### Iterating through data samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f70d43d-dc81-40d1-b3e5-0e5ace6d1faa",
        "outputId": "c31446ec-44ee-4359-e9dc-f5c04014d83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 0\n",
            "sorce: a child of five for more then ten minutes . princess is lovely , but should be tongueless , cause actress don ' t know how to carry a role\n",
            "\n",
            "\n",
            "target: child of five for more then ten minutes . princess is lovely , but should be tongueless , cause actress don ' t know how to carry a role .\n",
            "\n",
            "\n",
            "sample 1\n",
            "sorce: articulate the overwhelming power of this 90-minute waste of time if i were having a three-way with jessica alba and jessica biel in front of a tv and blood of\n",
            "\n",
            "\n",
            "target: the overwhelming power of this 90-minute waste of time if i were having a three-way with jessica alba and jessica biel in front of a tv and blood of the\n",
            "\n",
            "\n",
            "sample 2\n",
            "sorce: when ww3 was a real possibility - nay probability - this film would have terrified me , but after the cold war ended so had the dangers of nuclear war\n",
            "\n",
            "\n",
            "target: ww3 was a real possibility - nay probability - this film would have terrified me , but after the cold war ended so had the dangers of nuclear war which\n",
            "\n",
            "\n",
            "sample 3\n",
            "sorce: hey , stunt men deserve their moment of glory always . solid support comes from george ' gabby ' hayes and wayne as usual has much screen charisma , particularly\n",
            "\n",
            "\n",
            "target: , stunt men deserve their moment of glory always . solid support comes from george ' gabby ' hayes and wayne as usual has much screen charisma , particularly when\n",
            "\n",
            "\n",
            "sample 4\n",
            "sorce: time constraints definitely show more than anything else . there is solid directing as always from carpenter yet there is a quality to the writing and whole production itself that\n",
            "\n",
            "\n",
            "target: constraints definitely show more than anything else . there is solid directing as always from carpenter yet there is a quality to the writing and whole production itself that gives\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = iter(dataloader)\n",
        "\n",
        "for sample in range(5):\n",
        "  src, trt = next(dataset)\n",
        "  print(\"sample\", sample)\n",
        "  print(\"sorce:\", index_to_en(src))\n",
        "  print(\"\\n\")\n",
        "  print(\"target:\", index_to_en(trt))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5e763bf-76c5-4c13-aa32-68152cb286f2",
        "outputId": "18238aaa-7c82-490e-a6a0-7cf14d0d29ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 1])\n",
            "torch.Size([30, 1])\n",
            "to\n",
            "my\n"
          ]
        }
      ],
      "source": [
        "for  src, trt in dataset:\n",
        "    print(trt.shape)\n",
        "    print(src.shape)\n",
        "    print(index_to_en(src[0,:]))\n",
        "    print(index_to_en(trt[0,:]))\n",
        "    break"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbe663d4-7d7e-4617-ad9d-332a4fd5401d",
        "outputId": "8f5b7dc4-4fa2-4f65-f719-aac9e5202c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source: to my house last night . i feared the worst knowing its reputation , and it was as god-awful as i ' d anticipated . this is a mexican-made mess\n",
            "target: my house last night . i feared the worst knowing its reputation , and it was as god-awful as i ' d anticipated . this is a mexican-made mess ,\n"
          ]
        }
      ],
      "source": [
        "print(\"source:\", index_to_en(src))\n",
        "print(\"target:\", index_to_en(trt))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4471574-186d-46a0-b174-25380db4998b"
      },
      "source": [
        "### Masking\n",
        "\n",
        "In transformers, masking is crucial for ensuring certain positions are not attended to. The function ```generate_square_subsequent_mask``` produces an upper triangular matrix, which ensures that during decoding, a token can't attend to future tokens of target.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "680443b5-88f4-4e5f-8952-b9784ff512a0"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz, device=DEVICE):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9569b23-1d9a-4502-b79a-731327390676"
      },
      "source": [
        "The ```create_mask function```, on the other hand, generates source masks, based on the provided source sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f93066e9-66df-4c87-afd6-bce3dc5c9b4a"
      },
      "outputs": [],
      "source": [
        "def create_mask(src,device=DEVICE):\n",
        "    src_seq_len = src.shape[0]\n",
        "    src_mask = generate_square_subsequent_mask(src_seq_len)\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, src_padding_mask"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e6a1c53-5d5f-49a9-b83c-4c619cba1595"
      },
      "outputs": [],
      "source": [
        "#Replace first four tokens with PAD token so we can also check how pad tokens are masked using padding_mask\n",
        "src[0:4] = PAD_IDX"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf2807c-3379-430e-85bd-c0394f4c15d9",
        "outputId": "cf094d34-660e-4870-fe0d-a19c920467f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    1],\n",
              "        [    1],\n",
              "        [    1],\n",
              "        [    1],\n",
              "        [  320],\n",
              "        [    3],\n",
              "        [   13],\n",
              "        [ 9212],\n",
              "        [    4],\n",
              "        [  153],\n",
              "        [ 1622],\n",
              "        [  112],\n",
              "        [ 2371],\n",
              "        [    5],\n",
              "        [    7],\n",
              "        [   12],\n",
              "        [   18],\n",
              "        [   23],\n",
              "        [ 4310],\n",
              "        [   23],\n",
              "        [   13],\n",
              "        [    8],\n",
              "        [  216],\n",
              "        [ 6340],\n",
              "        [    3],\n",
              "        [   15],\n",
              "        [   11],\n",
              "        [    6],\n",
              "        [32578],\n",
              "        [  582]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "mask, padding_mask = create_mask(src)\n",
        "src"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba85fee-a8da-4b96-a230-75c9b8f2551f",
        "outputId": "2a2482b2-826e-4d99-8a7e-36c306537461"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "mask"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cd0f84d-a47b-4576-87bd-46bfba49d200",
        "outputId": "0d4fa8b9-7c3e-4835-c9e5-af64e75d0baf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True,  True,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "padding_mask"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "760a4664-e012-4427-81da-c222065cb3e1"
      },
      "source": [
        "### Positional encoding\n",
        "\n",
        "GPT uses trainable positional encodings unlike fixed positional encodings such as sinusoidal encodings.\n",
        "\n",
        "Trainable positional encodings are implemented as a set of learnable parameters, one for each position in the input sequence. These parameters have the same dimensionality as the token embeddings. During training, the model updates the positional encoding parameters along with the other model parameters to capture the positional information more effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e39da0b-4c51-4edb-94ce-a38eddba5879"
      },
      "outputs": [],
      "source": [
        "# add positional information to the input tokens\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84868b45-d835-46cd-814a-4785ecd78fc5"
      },
      "source": [
        "### Token embedding\n",
        "Token embedding, also known as word embedding or word representation, is a way to convert words or tokens from a text corpus into numerical vectors in a continuous vector space where the numerical values represent various linguistic properties of the word, such as its meaning, context, or relationships with other words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0b7d4f7-499b-46ad-8f9a-704b0074592c"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95a364db-ac82-4e3b-8037-f0b9e1cc9069"
      },
      "source": [
        "## Custom GPT model architecture\n",
        "\n",
        "- **Initialization (`__init__`)**: The constructor takes several parameters including `embed_size`, `vocab_size`, `num_heads`, `num_layers`, `max_seq_len`, and `dropout`. It initializes the embedding layer, positional encoding, transformer encoder layers, and a linear layer (`lm_head`) for generating logits over the vocabulary.\n",
        "\n",
        "- **Weight initialization (`init_weights`)**: This method initializes the weights of the model for better training convergence. The Xavier uniform initialization is used, which is a common practice for initializing weights in deep learning.\n",
        "\n",
        "- **Decoder (`decoder`)**: Although named `decoder`, this method currently functions as the forward pass through the transformer encoder layers, followed by the generation of logits for the language modeling task. It handles the addition of positional encodings to the embeddings and applies a mask if necessary.\n",
        "\n",
        "- **Forward pass (`forward`)**: This method is similar to the `decoder` method and defines the forward computation of the model. It processes the input through embedding layers, positional encoding, transformer encoder layers, and produces the final output using the `lm_head`.\n",
        "\n",
        "- **Mask generation**: Both `decoder` and `forward` methods contain logic to generate a square causal mask if no source mask is provided. This mask ensures that the prediction for a position does not depend on the future tokens in the sequence, which is important for the autoregressive nature of GPT models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a9ca68c-b3f9-4d8b-a186-8a14ebfb50b2"
      },
      "outputs": [],
      "source": [
        "class CustomGPTModel(nn.Module):\n",
        "    def __init__(self, embed_size,vocab_size, num_heads, num_layers, max_seq_len=500,dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.init_weights()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n",
        "\n",
        "        print(\"Embedding size: \", embed_size)\n",
        "\n",
        "        # Remaining layers are part of the TransformerDecoder\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.embed_size = embed_size\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def init_weights(self):\n",
        "      for p in self.parameters():\n",
        "          if p.dim() > 1:\n",
        "              nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_mask(src,device=DEVICE):\n",
        "        src_seq_len = src.shape[0]\n",
        "        src_mask = nn.Transformer.generate_square_subsequent_mask(src_seq_len)\n",
        "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "        return src_mask,src_padding_mask\n",
        "\n",
        "    def decoder(self, x,src_mask):\n",
        "        seq_length = x.size(0)\n",
        "\n",
        "        # Add positional embeddings to the input embeddings\n",
        "        x = self.embed(x)* math.sqrt(self.embed_size)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask, src_padding_mask = create_mask(x)\n",
        "\n",
        "        output = self.transformer_encoder(x, src_mask)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    def forward(self,x,src_mask=None,key_padding_mask=None):\n",
        "\n",
        "        seq_length = x.size(0)\n",
        "\n",
        "        # Add positional embeddings to the input embeddings\n",
        "        x = self.embed(x)* math.sqrt(self.embed_size) #src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask, src_padding_mask = create_mask(x)\n",
        "\n",
        "        output = self.transformer_encoder(x, src_mask,key_padding_mask)\n",
        "        x = self.lm_head(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22345aac-e5c0-4c76-8116-fd328a56e308"
      },
      "source": [
        "### Model configuration and initialization\n",
        "\n",
        "- `ntokens`: The total number of unique tokens in the vocabulary, which the model will use to represent words.\n",
        "- `emsize`: The size of each embedding vector. In this model, each word will be represented by a 200-dimensional vector.\n",
        "- `nlayers`: The number of transformer encoder layers in the model. We are using two layers in this configuration.\n",
        "- `nhead`: The number of attention heads in the multi-head attention mechanism. The model will use two attention heads.\n",
        "- `dropout`: A regularization technique where randomly selected neurons are ignored during training to prevent overfitting. Here, we set the dropout probability to 0.2.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "298592a0-20aa-407d-9dad-9b888b7ddf16",
        "outputId": "9fd255b6-c4f2-4728-9f77-f93d9541dd1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding size:  200\n"
          ]
        }
      ],
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "\n",
        "model = CustomGPTModel(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size=ntokens,dropout=dropout).to(DEVICE)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19f6e28-fe0d-4bf6-af56-c168061f08fa"
      },
      "source": [
        "### Prompting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "316103c4-74f0-4de3-bba9-c0964d73d564"
      },
      "outputs": [],
      "source": [
        "def encode_prompt(prompt, block_size=BLOCK_SIZE):\n",
        "    # Handle None prompt\n",
        "    while prompt is None:\n",
        "        prompt = input(\"Sorry, prompt cannot be empty. Please enter a valid prompt: \")\n",
        "\n",
        "    tokens = tokenizer(prompt)\n",
        "    number_of_tokens = len(tokens)\n",
        "\n",
        "    # Handle long prompts\n",
        "    if number_of_tokens > block_size:\n",
        "        tokens = tokens[-block_size:]  # Keep last block_size characters\n",
        "\n",
        "    prompt_indices = vocab(tokens)\n",
        "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
        "    return prompt_encoded"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c758203-9ca1-4deb-b6e0-aa5d81b529cc",
        "outputId": "9b770d60-b132-4c96-fcc6-067aae076377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, prompt cannot be empty. Please enter a valid prompt: Hello\n",
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(index_to_en(encode_prompt(None)))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cad5de1f-669a-4863-ae05-9e3bff42ff4c",
        "outputId": "54d0c71d-ba0f-49d9-ca54-7f3f790a4e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a prompt to get model generate next words .\n"
          ]
        }
      ],
      "source": [
        "print(index_to_en(encode_prompt(\"This is a prompt to get model generate next words.\" ) ))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b18fc9-91b2-4fd0-8284-d35360bd6427",
        "outputId": "e759ef4c-0c5d-4088-f809-bc293c0731d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   15],\n",
              "        [   11],\n",
              "        [    6],\n",
              "        [33700],\n",
              "        [   10],\n",
              "        [   86],\n",
              "        [ 2076],\n",
              "        [ 5673],\n",
              "        [  388],\n",
              "        [  665],\n",
              "        [    3]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "prompt_encoded = encode_prompt(\"This is a prompt to get model generate next words.\").to(DEVICE)\n",
        "prompt_encoded"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed567a7b-cc37-427d-8718-d13d50c349ad"
      },
      "outputs": [],
      "source": [
        "logits = model.decoder(prompt_encoded,src_mask=None).to(DEVICE)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7c1f19a-6754-493f-87d0-f7116e2998c3",
        "outputId": "76b65159-9056-417f-e91a-28117708d574"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 1, 68813])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# 11 tokens per output, single batch, logit values\n",
        "logits.shape"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94cbbbc4-d392-477c-a36d-33a947b4c2d8",
        "outputId": "68cc4728-ec47-4610-c4ee-eaae2c42ea37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 11, 68813])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "logits = logits.transpose(0, 1)\n",
        "logits.shape"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab9b43ad-c8ff-4ca3-b756-ec2a500e2097",
        "outputId": "360b9e07-dc2c-4229-8cf1-805921ef2bad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 68813])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "logit_preiction = logits[:,-1]\n",
        "logit_preiction.shape"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f57f2501-7384-4a30-b529-049d1223b266",
        "outputId": "dd6d827a-3d7d-48ea-86e5-36adeaccdb35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([56984], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "_, next_word_index = torch.max(logit_preiction, dim=1)\n",
        "next_word_index"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7f935791-63c0-444c-a1c8-513621c4b7aa",
        "outputId": "6c407d79-e6fe-4109-c47e-a1a70d33778a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'original\\x97is'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "index_to_en(next_word_index)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0683d8a9-055e-4919-a975-0da34b3395c6"
      },
      "source": [
        "## Autoregressive text generation\n",
        "In decoder models, we simply append the output to the input to generate the next response. We stop this process when we encounter the end-of-sequence tag <|endoftext|> or if the input becomes too large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ef17c1-4b49-41b2-a1b2-07d36bb64b48"
      },
      "outputs": [],
      "source": [
        "prompt = \"this is the beginning of\""
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00b0e276-8cd2-406a-a8a6-9970445a86f7",
        "outputId": "d5d35932-d7ab-4239-d76b-2be0ce518c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device for prompt_encoded: torch.Size([5, 1])\n"
          ]
        }
      ],
      "source": [
        "prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
        "print(\"Device for prompt_encoded:\", prompt_encoded.shape)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8ed2f0-3b69-4c6c-8333-9e33e3ec2789"
      },
      "outputs": [],
      "source": [
        "max_new_tokens = 10"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e0bc025-8801-44c3-aa35-8ccdfdad59fa",
        "outputId": "11b90881-61fa-47a4-bda6-172e0e699b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Shape of logits at step 0: torch.Size([1, 5, 68813])\n",
            "Shape of logit_prediction at step 0: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 0: torch.Size([1, 1])\n",
            "Sequence for step 0: ['this', 'is', 'the', 'beginning', 'of', 'establishing']\n",
            "Shape of prompt_encoded after concatenation at step 0: torch.Size([6, 1])\n",
            " \n",
            "Shape of logits at step 1: torch.Size([1, 6, 68813])\n",
            "Shape of logit_prediction at step 1: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 1: torch.Size([1, 1])\n",
            "Sequence for step 1: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful']\n",
            "Shape of prompt_encoded after concatenation at step 1: torch.Size([7, 1])\n",
            " \n",
            "Shape of logits at step 2: torch.Size([1, 7, 68813])\n",
            "Shape of logit_prediction at step 2: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 2: torch.Size([1, 1])\n",
            "Sequence for step 2: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful', 'barrages']\n",
            "Shape of prompt_encoded after concatenation at step 2: torch.Size([8, 1])\n",
            " \n",
            "Shape of logits at step 3: torch.Size([1, 8, 68813])\n",
            "Shape of logit_prediction at step 3: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 3: torch.Size([1, 1])\n",
            "Sequence for step 3: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful', 'barrages', 'aegerter']\n",
            "Shape of prompt_encoded after concatenation at step 3: torch.Size([9, 1])\n",
            " \n",
            "Shape of logits at step 4: torch.Size([1, 9, 68813])\n",
            "Shape of logit_prediction at step 4: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 4: torch.Size([1, 1])\n",
            "Sequence for step 4: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful', 'barrages', 'aegerter', 'lo-calorie']\n",
            "Shape of prompt_encoded after concatenation at step 4: torch.Size([10, 1])\n",
            " \n",
            "Shape of logits at step 5: torch.Size([1, 10, 68813])\n",
            "Shape of logit_prediction at step 5: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 5: torch.Size([1, 1])\n",
            "Sequence for step 5: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful', 'barrages', 'aegerter', 'lo-calorie', 'bleach']\n",
            "Shape of prompt_encoded after concatenation at step 5: torch.Size([11, 1])\n",
            " \n",
            "Shape of logits at step 6: torch.Size([1, 11, 68813])\n",
            "Shape of logit_prediction at step 6: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 6: torch.Size([1, 1])\n",
            "Sequence for step 6: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful', 'barrages', 'aegerter', 'lo-calorie', 'bleach', 'allows']\n",
            "Shape of prompt_encoded after concatenation at step 6: torch.Size([12, 1])\n",
            " \n",
            "Shape of logits at step 7: torch.Size([1, 12, 68813])\n",
            "Shape of logit_prediction at step 7: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 7: torch.Size([1, 1])\n",
            "Sequence for step 7: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful', 'barrages', 'aegerter', 'lo-calorie', 'bleach', 'allows', 'spiritualism']\n",
            "Shape of prompt_encoded after concatenation at step 7: torch.Size([13, 1])\n",
            " \n",
            "Shape of logits at step 8: torch.Size([1, 13, 68813])\n",
            "Shape of logit_prediction at step 8: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 8: torch.Size([1, 1])\n",
            "Sequence for step 8: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful', 'barrages', 'aegerter', 'lo-calorie', 'bleach', 'allows', 'spiritualism', 'slavery']\n",
            "Shape of prompt_encoded after concatenation at step 8: torch.Size([14, 1])\n",
            " \n",
            "Shape of logits at step 9: torch.Size([1, 14, 68813])\n",
            "Shape of logit_prediction at step 9: torch.Size([1, 68813])\n",
            "Shape of next_token_encoded at step 9: torch.Size([1, 1])\n",
            "Sequence for step 9: ['this', 'is', 'the', 'beginning', 'of', 'establishing', 'unsuspensful', 'barrages', 'aegerter', 'lo-calorie', 'bleach', 'allows', 'spiritualism', 'slavery', 'end-credits']\n",
            "Shape of prompt_encoded after concatenation at step 9: torch.Size([15, 1])\n"
          ]
        }
      ],
      "source": [
        "for i in range(max_new_tokens):\n",
        "    logits = model.decoder(prompt_encoded,src_mask=None)\n",
        "    logits = logits.transpose(0, 1)\n",
        "    print(\" \")\n",
        "    print(f\"Shape of logits at step {i}: {logits.shape}\")\n",
        "\n",
        "    logit_preiction = logits[:, -1]\n",
        "    print(f\"Shape of logit_prediction at step {i}: {logit_preiction.shape}\")\n",
        "\n",
        "    next_token_encoded = torch.argmax(logit_preiction, dim=-1).reshape(-1, 1)\n",
        "    print(f\"Shape of next_token_encoded at step {i}: {next_token_encoded.shape}\")\n",
        "\n",
        "    prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0).to(DEVICE)\n",
        "    print(f\"Sequence for step {i}: {[index_to_en(j) for j in prompt_encoded]}\")\n",
        "    print(f\"Shape of prompt_encoded after concatenation at step {i}: {prompt_encoded.shape}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e530b11-1f07-44dd-b873-24e9084227a1",
        "outputId": "d5068197-8ac9-4fa0-ca2c-ba634d59a18f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<|endoftext|>' ]\n",
        "BLOCK_SIZE"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dbdbe43-80b4-4576-a5f4-dce9dfe6696b"
      },
      "outputs": [],
      "source": [
        "# Auto-regressive Language Model text generation\n",
        "def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
        "    # Move model to the specified device (e.g., GPU or CPU)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # Encode the input prompt using the provided encode_prompt function\n",
        "    prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
        "    tokens = []\n",
        "\n",
        "    # Generate new tokens up to max_new_tokens\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Decode the encoded prompt using the model's decoder\n",
        "        logits = model(prompt_encoded,src_mask=None,key_padding_mask=None)\n",
        "\n",
        "        # Transpose the logits to bring the sequence length to the first dimension\n",
        "        logits = logits.transpose(0, 1)\n",
        "\n",
        "        # Select the logits of the last token in the sequence\n",
        "        logit_prediction = logits[:, -1]\n",
        "\n",
        "        # Choose the most probable next token from the logits(greedy decoding)\n",
        "        next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
        "\n",
        "        # If the next token is the end-of-sequence (EOS) token, stop generation\n",
        "        if next_token_encoded.item() == EOS_IDX:\n",
        "            break\n",
        "\n",
        "        # Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
        "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
        "\n",
        "        # Convert the next token index to a token string using the vocabulary\n",
        "        # Move the tensor back to CPU for vocab lookup if needed\n",
        "        token_id = next_token_encoded.to('cpu').item()\n",
        "        tokens.append(vocab.get_itos()[token_id])\n",
        "\n",
        "    # Join the generated tokens into a single string and return\n",
        "    return ' '.join(tokens)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "38b3b708-5f4d-40e0-9370-1fa235af43a3",
        "outputId": "ae9aa3b2-a130-4e26-ae15-19707534faa7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'haggis- hadha hurl-buckets mid-flight gregarious spookiness alerting classes millionaire- entanglements amoeba-like >>ff no-good remo stuck automotive neuroticism middlebrow long-anticipated stridence blithe postmark falak service moroccan poe bad--wrong includes newbies sartre'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "generate(model,prompt=\"this is the beginning of\",max_new_tokens=30,vocab=vocab,tokenizer=tokenizer)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83f9dac-8b4a-4303-a673-038e7008b7cb"
      },
      "source": [
        "### Decoding the differences: Training vs. inference\n",
        "\n",
        "The key difference between the training and inference stages lies in the inputs to the decoder. During training, the decoder benefits from exposure to the ground truth--receiving the exact target sequence tokens incrementally through a technique known as \"teacher forcing.\"\n",
        "\n",
        "To start the training, first create a Cross Entropy Loss object. The loss will not consider PAD tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30a8856d-604c-4165-9df3-80ede571643d",
        "outputId": "261fb458-2f5e-4409-f721-100a0e227a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 1, 68813])\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Create mask\n",
        "src, tgt=next(iter(dataloader))\n",
        "mask, padding_mask = create_mask(src)\n",
        "\n",
        "logits = model(src,src_mask=mask,key_padding_mask=padding_mask)\n",
        "print(logits.shape)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4835a2b-4fa7-48b6-9477-c3dca2916bd5",
        "outputId": "490f3880-172f-48bc-c17c-41eb783ada81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape:  torch.Size([30, 1, 68813])\n",
            "Source shape:  tensor([[  127],\n",
            "        [  269],\n",
            "        [    3],\n",
            "        [   23],\n",
            "        [   20],\n",
            "        [   93],\n",
            "        [ 2541],\n",
            "        [    9],\n",
            "        [    4],\n",
            "        [66930],\n",
            "        [  166],\n",
            "        [   95],\n",
            "        [ 2195],\n",
            "        [    8],\n",
            "        [  621],\n",
            "        [    5],\n",
            "        [   13],\n",
            "        [   91],\n",
            "        [   64],\n",
            "        [   77],\n",
            "        [    6],\n",
            "        [11306],\n",
            "        [  436],\n",
            "        [   16],\n",
            "        [   70],\n",
            "        [   23],\n",
            "        [   85],\n",
            "        [  788],\n",
            "        [   10],\n",
            "        [   12]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(\"Output shape: \", logits.shape)\n",
        "print(\"Source shape: \", src)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09bb3ac0-f376-4ed2-9694-0d4982d2522d",
        "outputId": "a2b90552-9720-4380-95b3-900a34dd8990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 1])\n"
          ]
        }
      ],
      "source": [
        "tgt\n",
        "print(tgt.shape)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a3fad72-cf09-4b16-9f33-60b9c2ab4a7c",
        "outputId": "99476d7b-07ae-4a30-ad21-927766737b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 68813])\n",
            "torch.Size([30])\n"
          ]
        }
      ],
      "source": [
        "print(logits.reshape(-1, logits.shape[-1]).shape)\n",
        "print(tgt.reshape(-1).shape)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b47d710-6857-4d8a-b0b5-bfdb8dc2e19a",
        "outputId": "56c3e71c-4c99-47aa-8eef-31c94a356b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36.47765350341797\n"
          ]
        }
      ],
      "source": [
        "# Calculate the loss\n",
        "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
        "print(loss.item())"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdd9edee-3c36-4949-b548-5d091f8b47c5"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, eval_data) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for src,tgt in eval_data:\n",
        "            tgt = tgt.to(DEVICE)\n",
        "            #seq_len = src.size(0)\n",
        "            logits = model(src,src_mask=None,key_padding_mask=None)\n",
        "            total_loss +=  loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1)).item()\n",
        "    return total_loss / (len(list(eval_data)) - 1)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21e32895-a050-4885-a1df-fc3e85c58c9b",
        "outputId": "2d254b79-918a-490a-faf1-4619cdb2aa3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35.02987586185881"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "evaluate(model, val_dataloader)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e971190-28ec-4dc7-a34f-4dfd5b730ee4"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "- **Optimizer**: Initializes an ADAM optimizer.\n",
        "\n",
        "Within the `train` function:\n",
        "- The model is set to train mode, which enables dropout and batch normalization layers.\n",
        "- A loop iterates over the training data, which is loaded in batches. For each batch:\n",
        "    - The source (`src`) and target (`tgt`) sequences are extracted.\n",
        "    - The model performs a forward pass to get logits.\n",
        "    - The logits are reshaped for loss calculation.\n",
        "    - The loss is computed using `loss_fn`, which likely refers to a loss function such as cross-entropy that measures the difference between the predicted logits and the target sequences.\n",
        "- Gradient clipping is applied to prevent exploding gradients, which is common in training deep neural networks.\n",
        "- The optimizer updates the model parameters based on the computed gradients.\n",
        "\n",
        "Logging occurs every `10000` steps, or when reaching a specific batch (batch `42060` is hardcoded as an example). During logging:\n",
        "\n",
        "- The average loss and the perplexity (a measure of how well the probability model predicts a sample) are calculated and printed, providing insights into the model's performance.\n",
        "- The elapsed time per batch since the last log interval is measured and reported, giving an indication of training efficiency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ef958e-2723-4f57-9d96-39d6a74f270c"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
        "\n",
        "def train(model: nn.Module,train_data) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 10000\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_batches = len(list(train_data)) // block_size\n",
        "    for batch,srctgt in enumerate(train_data):\n",
        "        src= srctgt[0]\n",
        "        tgt= srctgt[1]\n",
        "        logits = model(src,src_mask=None)\n",
        "        logits_flat = logits.reshape(-1, logits.shape[-1])\n",
        "        loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            #cur_loss = total_loss / log_interval\n",
        "            cur_loss = total_loss / batch\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch//block_size:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "319e701c-03b3-423d-9715-bd8214791a8f",
        "outputId": "7a71bf5c-ea0f-4fe6-8d6b-bab17b8b08f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.08 | loss  8.33 | ppl  4147.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 165.01s | valid loss  8.18 | valid ppl  3586.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.05 | loss  8.22 | ppl  3718.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 165.08s | valid loss  8.18 | valid ppl  3574.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.07 | loss  8.22 | ppl  3705.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 165.28s | valid loss  8.10 | valid ppl  3309.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.07 | loss  8.22 | ppl  3704.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 165.62s | valid loss  8.17 | valid ppl  3531.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.10 | loss  8.21 | ppl  3683.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 165.95s | valid loss  8.26 | valid ppl  3851.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.06 | loss  8.21 | ppl  3677.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 165.27s | valid loss  8.25 | valid ppl  3810.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.06 | loss  8.21 | ppl  3665.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 165.51s | valid loss  8.19 | valid ppl  3609.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.07 | loss  8.21 | ppl  3676.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 165.40s | valid loss  8.23 | valid ppl  3751.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.10 | loss  8.22 | ppl  3703.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 165.64s | valid loss  8.09 | valid ppl  3271.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.15 | loss  8.22 | ppl  3716.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 165.98s | valid loss  8.27 | valid ppl  3904.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.07 | loss  8.22 | ppl  3729.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 165.29s | valid loss  8.11 | valid ppl  3327.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.10 | loss  8.22 | ppl  3707.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 165.69s | valid loss  8.18 | valid ppl  3567.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.06 | loss  8.20 | ppl  3640.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 165.20s | valid loss  8.22 | valid ppl  3730.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.06 | loss  8.21 | ppl  3690.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 165.78s | valid loss  8.15 | valid ppl  3469.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.06 | loss  8.21 | ppl  3687.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 165.42s | valid loss  8.34 | valid ppl  4205.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.08 | loss  8.20 | ppl  3639.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 165.58s | valid loss  8.28 | valid ppl  3954.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.06 | loss  8.22 | ppl  3706.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 165.08s | valid loss  8.18 | valid ppl  3568.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.10 | loss  8.21 | ppl  3689.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 166.58s | valid loss  8.29 | valid ppl  3969.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.13 | loss  8.21 | ppl  3663.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 166.58s | valid loss  8.27 | valid ppl  3921.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.09 | loss  8.21 | ppl  3689.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 165.73s | valid loss  8.23 | valid ppl  3770.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.09 | loss  8.22 | ppl  3701.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 165.58s | valid loss  8.29 | valid ppl  3986.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.08 | loss  8.22 | ppl  3721.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 165.25s | valid loss  8.22 | valid ppl  3728.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.09 | loss  8.22 | ppl  3703.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 165.97s | valid loss  8.25 | valid ppl  3827.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.08 | loss  8.22 | ppl  3701.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 165.48s | valid loss  8.25 | valid ppl  3845.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.06 | loss  8.21 | ppl  3675.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 166.08s | valid loss  8.27 | valid ppl  3896.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.17 | loss  8.21 | ppl  3670.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 166.44s | valid loss  8.18 | valid ppl  3579.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.06 | loss  8.23 | ppl  3744.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 165.20s | valid loss  8.26 | valid ppl  3879.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.07 | loss  8.21 | ppl  3678.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 165.56s | valid loss  8.14 | valid ppl  3413.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.09 | loss  8.22 | ppl  3696.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 166.08s | valid loss  8.13 | valid ppl  3390.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |  1000/ 1250 batches | lr 0.0100 | ms/batch  7.10 | loss  8.21 | ppl  3680.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 165.50s | valid loss  8.23 | valid ppl  3749.30\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 30\n",
        "Train_losses = []\n",
        "Val_losses = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_loss = train(model,dataloader)\n",
        "    val_loss = evaluate(model, val_dataloader)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    Train_losses.append(train_loss)\n",
        "    Val_losses.append(val_loss)\n",
        "\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'model_best_val_loss.pt')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5397d22-19e8-4b4d-8227-14c87610a7a8"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of epochs (assuming the lengths of train_losses and val_losses are equal)\n",
        "num_epochs = len(Train_losses)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot the training losses\n",
        "ax.plot(range(num_epochs), Train_losses, label='Training Loss', color='blue')\n",
        "\n",
        "# Plot the validation losses\n",
        "ax.plot(range(num_epochs), Val_losses, label='Validation Loss', color='orange')\n",
        "\n",
        "# Set the x-axis label\n",
        "ax.set_xlabel('Epoch')\n",
        "\n",
        "# Set the y-axis label\n",
        "ax.set_ylabel('Loss')\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Training and Validation Losses')\n",
        "\n",
        "# Add a legend to the plot\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ff6dbd2-c755-46b1-9f4f-6618f387dc4d"
      },
      "source": [
        "![loss_gpt.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/V1Fda63Q4CrNfgT5g1HfVQ.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26a91ef5-c4e4-4c42-aab3-54a98e57b353"
      },
      "source": [
        "## Loading the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1a70bd6-3ce9-46e5-90bf-be3e37d28bed"
      },
      "outputs": [],
      "source": [
        "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kyn1_OsXrzjef0xihlsXmg.pt'\n",
        "#model.load_state_dict(torch.load('kyn1_OsXrzjef0xihlsXmg.pt',map_location=torch.device('cpu')))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4129217d-dbdc-49b5-af66-51671eb30158"
      },
      "outputs": [],
      "source": [
        "print(generate(model,prompt=\"the movie was\",max_new_tokens=10,vocab=vocab,tokenizer=tokenizer))"
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "prev_pub_hash": "e1b85295509065c068b2e670cf2ca79050c48a451b3cf21a0f6a0bb73a97d7f6",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}