{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogukartal/IBM_AI_Labs/blob/main/Generative%20AI%20and%20LLMs%3A%20Architecture%20and%20Data%20Preparation/Generative%20AI%20Architecture/Implementing_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "725b9b41-50df-4bc3-a172-a27fabeaa304"
      },
      "source": [
        "# **Implementing Tokenization**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e81d347-18c6-4254-95e6-569a88327b8f"
      },
      "source": [
        "* [`nltk`](https://www.nltk.org/) or natural language toolkit, will be employed for data management tasks. It offers comprehensive tools and resources for processing natural language text, making it a valuable choice for tasks such as text preprocessing and analysis.\n",
        "\n",
        "* [`spaCy`](https://spacy.io/) is an open-source software library for advanced natural language processing in Python. spaCy is renowned for its speed and accuracy in processing large volumes of text data.\n",
        "\n",
        "* [`BertTokenizer`](https://huggingface.co/docs/transformers/main_classes/tokenizer#berttokenizer) is part of the Hugging Face Transformers library, a popular library for working with state-of-the-art pre-trained language models. BertTokenizer is specifically designed for tokenizing text according to the BERT model's specifications.\n",
        "\n",
        "* [`XLNetTokenizer`](https://huggingface.co/docs/transformers/main_classes/tokenizer#xlnettokenizer) is another component of the Hugging Face Transformers library. It is tailored for tokenizing text in alignment with the XLNet model's requirements.\n",
        "\n",
        "* [`torchtext`](https://pytorch.org/text/stable/index.html) It is part of the PyTorch ecosystem, to handle various natural language processing tasks. It  simplifies the process of working with text data and provides functionalities for data preprocessing, tokenization, vocabulary management, and batching.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b642c149-4d55-4534-a459-8fd821e58398"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install sentencepiece\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "!pip install torchtext==0.15.2"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8deadd60-85a4-490f-9105-896045f2b378",
        "outputId": "055b5d4e-ae66-4dbd-f512-9c0978c05936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk # Natural Language Tool Kit\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import spacy # Advanced Natural Language Processing\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from transformers import BertTokenizer\n",
        "from transformers import XLNetTokenizer\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import warnings\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cd56ce4-56b6-47b8-8c94-ef0205714ae0"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c3ed0d9-78bb-4d0d-8009-f182846ffc67"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%202.png\" width=\"50%\" alt=\"Image Description\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "369e68f8-0e7d-4b90-9bc2-7b9de6615300"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%201.png\" width=\"50%\" alt=\"Image Description\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf6bc820-c172-4746-90f5-96f98539d805"
      },
      "source": [
        "## Word-based tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2840771d-e823-4301-a299-1468aeacbe4b",
        "outputId": "e4df99bc-d1e5-4e92-f0fc-4c82bd8fa16e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization', '.']\n"
          ]
        }
      ],
      "source": [
        "# nltk splits the text based on words\n",
        "text = \"This is a sample sentence for word tokenization.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0862f481-4f58-454f-854a-668c1a9666c3",
        "outputId": "d7464c46-f669-4264-8305-2e5fd4953baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
          ]
        }
      ],
      "source": [
        "# Tokenization of negative possesions\n",
        "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9100e8dc-350b-49b3-93a5-d05aea52d194",
        "outputId": "2904ed5d-672b-4932-e51c-3f20f715afc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['I', 'could', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', 'Do', \"n't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n",
            "I PRON nsubj\n",
            "could AUX aux\n",
            "n't PART neg\n",
            "help VERB ROOT\n",
            "the DET det\n",
            "dog NOUN dobj\n",
            ". PUNCT punct\n",
            "Ca AUX aux\n",
            "n't PART neg\n",
            "you PRON nsubj\n",
            "do VERB ROOT\n",
            "it PRON dobj\n",
            "? PUNCT punct\n",
            "Do AUX aux\n",
            "n't PART neg\n",
            "be AUX ROOT\n",
            "afraid ADJ acomp\n",
            "if SCONJ mark\n",
            "you PRON nsubj\n",
            "are AUX advcl\n",
            ". PUNCT punct\n"
          ]
        }
      ],
      "source": [
        "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
        "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Making a list of the tokens and priting the list\n",
        "token_list = [token.text for token in doc]\n",
        "print(\"Tokens:\", token_list)\n",
        "\n",
        "# Showing token details\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b274cb8b-580c-4b84-80ab-458f019285e9"
      },
      "outputs": [],
      "source": [
        "# Words with similar meanings will be assigned different IDs, resulting in them being treated as entirely separate words with distinct meanings\n",
        "text = \"Unicorns are real. I saw a unicorn yesterday.\"\n",
        "token = word_tokenize(text)\n",
        "print(token)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e78705d-77dc-4934-a882-78e55126f71d"
      },
      "source": [
        "## Subword-based tokenizer\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%203.png\" width=\"50%\" alt=\"Image Description\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "528eef1b-7b5d-4ffb-b519-cb4f41445ab0"
      },
      "source": [
        "### WordPiece\n",
        "\n",
        "Initially, WordPiece initializes its vocabulary to include every character present in the training data and progressively learns a specified number of merge rules.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "057f57b4-86a8-4179-8133-f3e0f3a653fd",
        "outputId": "c93d1884-7874-41d5-b3ed-6e70ee8b4dd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ibm', 'taught', 'me', 'token', '##ization', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.tokenize(\"IBM taught me tokenization.\")"
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1292b5e-03a3-4037-8a92-f40bec9c5e39"
      },
      "source": [
        "- 'ibm': \"IBM\" is tokenized as 'ibm'. BERT converts tokens into lowercase, as it does not retain the case information when using the \"bert-base-uncased\" model.\n",
        "- 'taught', 'me', '.': These tokens are the same as the original words or punctuation, just lowercased (except punctuation).\n",
        "- 'token', '##ization': \"Tokenization\" is broken into two tokens. \"Token\" is a whole word, and \"##ization\" is a part of the original word. The \"##\" indicates that \"ization\" should be connected back to \"token\" when detokenizing (transforming tokens back to words).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dbd36ee-2611-42dd-befb-c93061d3d08e"
      },
      "source": [
        "### Unigram and SentencePiece\n",
        "\n",
        "Unigram is a method for breaking words or text into smaller pieces. It accomplishes this by starting with a large list of possibilities and gradually narrowing it down based on how frequently those pieces appear in the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33753d73-adf9-4d3b-9375-9b96851228e5",
        "outputId": "c4a9f844-16f7-4aea-b977-d1aec8beb41b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁IBM', '▁taught', '▁me', '▁token', 'ization', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "tokenizer.tokenize(\"IBM taught me tokenization.\")"
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e162664-a19e-4312-a445-67b02329c85f"
      },
      "source": [
        "- '▁IBM': The \"▁\" (often referred to as \"whitespace character\") before \"IBM\" indicates that this token is preceded by a space in the original text. \"IBM\" is kept as is because it's recognized as a whole token by XLNet and it preserves the casing because you are using the \"xlnet-base-cased\" model.\n",
        "- '▁taught', '▁me', '▁token': Similarly, these tokens are prefixed with \"▁\" to indicate they are new words preceded by a space in the original text, preserving the word as a whole and maintaining the original casing.\n",
        "- 'ization': Unlike \"BertTokenizer,\" \"XLNetTokenizer\" does not use \"##\" to indicate subword tokens. \"ization\" appears as its own token without a prefix because it directly follows the preceding word \"token\" without a space in the original text.\n",
        "- '.': The period is tokenized as a separate token since punctuation is treated separately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf5fe166-722d-4dbf-8a54-f569901be051"
      },
      "source": [
        "## Tokenization with PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ee3095-1a72-459e-bdd2-e38fa9742d1e",
        "outputId": "e7a2a6c3-9c0d-406c-ed73-30e745d602de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['introduction', 'to', 'nlp']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "dataset = [\n",
        "    (1,\"Introduction to NLP\"),\n",
        "    (2,\"Basics of PyTorch\"),\n",
        "    (1,\"NLP Techniques for Text Classification\"),\n",
        "    (3,\"Named Entity Recognition with PyTorch\"),\n",
        "    (3,\"Sentiment Analysis using PyTorch\"),\n",
        "    (3,\"Machine Translation with PyTorch\"),\n",
        "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
        "    (1,\" Machine Translation with NLP \"),\n",
        "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokenizer(dataset[0][1])"
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "897a2e8b-b74e-4d4f-8b15-5f36e87f3230"
      },
      "source": [
        "## Token indices\n",
        "The **```build_vocab_from_iterator```** function, when applied to a list of tokens, assigns a unique index to each token based on its position in the vocabulary. These indices serve as a way to represent the tokens in a numerical format that can be easily processed by machine learning models.\n",
        "\n",
        "For example, given a vocabulary with tokens [\"apple\", \"banana\", \"orange\"], the corresponding indices might be [0, 1, 2], where \"apple\" is represented by index 0, \"banana\" by index 1, and \"orange\" by index 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2bf4b13-5700-4516-a412-fe4fe465f129"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(data_iter):\n",
        "    for  _,text in data_iter:\n",
        "        yield tokenizer(text)"
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0726676a-5af9-4905-9837-254c07e92417"
      },
      "outputs": [],
      "source": [
        "my_iterator = yield_tokens(dataset) # Iterator"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00df1141-ad73-418a-a895-d927895cc08c",
        "outputId": "992c6c50-0e78-4dea-b0e7-95ef94146d0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['introduction', 'to', 'nlp']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "next(my_iterator)"
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2608640-3123-4e65-9d04-8d7f62339b08"
      },
      "source": [
        " ### Out-of-vocabulary (OOV)\n",
        "When text data is tokenized, there may be words that are not present in the vocabulary because they are rare or unseen during the vocabulary building process. When encountering such OOV words during actual language processing tasks like text generation or language modeling, the model can use the ```<unk>``` token to represent them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b04fdec-7c65-411f-9532-d56916e00a0a"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a6c59fc-39ef-49f6-a7f2-b7912655b0c6"
      },
      "source": [
        "This code demonstrates how to fetch a tokenized sentence from an iterator, convert its tokens into indices using a provided vocabulary, and then print both the original sentence and its corresponding indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19559fc4-a848-40e0-b56f-f71b96363df8",
        "outputId": "a2b83c17-8dc9-48df-d7ad-6336406e7dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentence: ['basics', 'of', 'pytorch']\n",
            "Token Indices: [11, 15, 2]\n"
          ]
        }
      ],
      "source": [
        "def get_tokenized_sentence_and_indices(iterator):\n",
        "    tokenized_sentence = next(iterator)  # Get the next tokenized sentence\n",
        "    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices\n",
        "    return tokenized_sentence, token_indices\n",
        "\n",
        "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
        "next(my_iterator)\n",
        "\n",
        "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
        "print(\"Token Indices:\", token_indices)"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb397754-40cc-4acf-9504-b539b5b8c4da",
        "outputId": "12dbace3-338f-4084-869a-70bdd6505153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines after adding special tokens:\n",
            " [['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'saying', 'hi', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
            "Vocabulary: ['<unk>', '<pad>', '<bos>', '<eos>', '!', 'IBM', 'Special', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'saying', 'taught', 'they', 'tokenization', 'tokenizers', 'will', 'your']\n",
            "Token IDs for 'tokenization': {'will': 20, 'tokenizers': 19, 'tokenization': 18, 'taught': 16, 'your': 21, 'saying': 15, '<unk>': 0, 'and': 7, 'hi': 10, '<pad>': 1, '<bos>': 2, 'they': 17, '<eos>': 3, '!': 4, 'ready': 14, 'IBM': 5, 'are': 8, 'Special': 6, 'mind': 13, 'me': 12, 'blow': 9, 'just': 11}\n"
          ]
        }
      ],
      "source": [
        "lines = [\"IBM taught me tokenization\",\n",
        "         \"Special tokenizers are ready and they will blow your mind\",\n",
        "         \"just saying hi!\"]\n",
        "\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "tokens = []\n",
        "max_length = 0\n",
        "\n",
        "for line in lines:\n",
        "    tokenized_line = tokenizer_en(line)\n",
        "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
        "    tokens.append(tokenized_line)\n",
        "    max_length = max(max_length, len(tokenized_line))\n",
        "\n",
        "for i in range(len(tokens)):\n",
        "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
        "\n",
        "print(\"Lines after adding special tokens:\\n\", tokens)\n",
        "\n",
        "# Build vocabulary without unk_init\n",
        "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Vocabulary and Token Ids\n",
        "print(\"Vocabulary:\", vocab.get_itos())\n",
        "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df1a6cca-f667-47b9-9ff7-0304fb7e9f61",
        "outputId": "3d710629-44f4-4f69-9c7f-99209d9a4133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs for new line: [2, 0, 0, 0, 0, 7, 0, 0, 0, 3, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "new_line = \"I learned about embeddings and attention mechanisms.\"\n",
        "\n",
        "# Tokenize the new line\n",
        "tokenized_new_line = tokenizer_en(new_line)\n",
        "tokenized_new_line = ['<bos>'] + tokenized_new_line + ['<eos>']\n",
        "\n",
        "# Pad the new line to match the maximum length of previous lines\n",
        "new_line_padded = tokenized_new_line + ['<pad>'] * (max_length - len(tokenized_new_line))\n",
        "\n",
        "# Convert tokens to IDs and handle unknown words\n",
        "new_line_ids = [vocab[token] if token in vocab else vocab['<unk>'] for token in new_line_padded]\n",
        "\n",
        "# Example usage\n",
        "print(\"Token IDs for new line:\", new_line_ids)"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf76369f-941f-43c8-ba22-246ea3ed8ea3",
        "outputId": "f998616c-fc2d-4dbb-b957-c101ff630987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokens: ['Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', \"''\"]\n",
            "Time Taken: 0:00:00.001652 seconds\n",
            "\n",
            "NLTK Token Frequencies: {'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
            "\n",
            "SpaCy Tokens: ['\\n', 'Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’s', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', '\\n', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"', '\\n']\n",
            "Time Taken: 0:00:00.089144 seconds\n",
            "\n",
            "SpaCy Token Frequencies: {'\\n': 3, 'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’s': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
            "\n",
            "Bert Tokens: ['going', 'through', 'the', 'world', 'of', 'token', '##ization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'and', 'while', 'i', \"'\", 'm', 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'en', '##light', '##ening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"']\n",
            "Time Taken: 0:00:00.003467 seconds\n",
            "\n",
            "Bert Token Frequencies: {'going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'token': 1, '##ization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 3, 'meanings': 1, '.': 3, 'each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'while': 1, 'i': 1, \"'\": 1, 'm': 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'en': 1, '##light': 1, '##ening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
            "\n",
            "XLNet Tokens: ['▁Going', '▁through', '▁the', '▁world', '▁of', '▁token', 'ization', '▁has', '▁been', '▁like', '▁walking', '▁through', '▁a', '▁huge', '▁maze', '▁made', '▁of', '▁words', ',', '▁symbols', ',', '▁and', '▁meaning', 's', '.', '▁Each', '▁turn', '▁shows', '▁a', '▁bit', '▁more', '▁about', '▁the', '▁cool', '▁ways', '▁computers', '▁learn', '▁to', '▁understand', '▁our', '▁language', '.', '▁And', '▁while', '▁I', \"'\", 'm', '▁still', '▁finding', '▁my', '▁way', '▁through', '▁it', ',', '▁the', '▁journey', '’', 's', '▁been', '▁enlighten', 'ing', '▁and', ',', '▁honestly', ',', '▁a', '▁bunch', '▁of', '▁fun', '.', '▁E', 'ager', '▁to', '▁see', '▁where', '▁this', '▁learning', '▁path', '▁takes', '▁me', '▁next', '!', '\"']\n",
            "Time Taken: 0:00:00.012062 seconds\n",
            "\n",
            "XLNet Token Frequencies: {'▁Going': 1, '▁through': 3, '▁the': 3, '▁world': 1, '▁of': 3, '▁token': 1, 'ization': 1, '▁has': 1, '▁been': 2, '▁like': 1, '▁walking': 1, '▁a': 3, '▁huge': 1, '▁maze': 1, '▁made': 1, '▁words': 1, ',': 5, '▁symbols': 1, '▁and': 2, '▁meaning': 1, 's': 2, '.': 3, '▁Each': 1, '▁turn': 1, '▁shows': 1, '▁bit': 1, '▁more': 1, '▁about': 1, '▁cool': 1, '▁ways': 1, '▁computers': 1, '▁learn': 1, '▁to': 2, '▁understand': 1, '▁our': 1, '▁language': 1, '▁And': 1, '▁while': 1, '▁I': 1, \"'\": 1, 'm': 1, '▁still': 1, '▁finding': 1, '▁my': 1, '▁way': 1, '▁it': 1, '▁journey': 1, '’': 1, '▁enlighten': 1, 'ing': 1, '▁honestly': 1, '▁bunch': 1, '▁fun': 1, '▁E': 1, 'ager': 1, '▁see': 1, '▁where': 1, '▁this': 1, '▁learning': 1, '▁path': 1, '▁takes': 1, '▁me': 1, '▁next': 1, '!': 1, '\"': 1}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "text = \"\"\"\n",
        "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
        "Eager to see where this learning path takes me next!\"\n",
        "\"\"\"\n",
        "\n",
        "# Counting and displaying tokens and their frequency\n",
        "from collections import Counter\n",
        "def show_frequencies(tokens, method_name):\n",
        "    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")\n",
        "\n",
        "# NLTK Tokenization\n",
        "start_time = datetime.now()\n",
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "nltk_time = datetime.now() - start_time\n",
        "\n",
        "# SpaCy Tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "start_time = datetime.now()\n",
        "spacy_tokens = [token.text for token in nlp(text)]\n",
        "spacy_time = datetime.now() - start_time\n",
        "\n",
        "# BertTokenizer Tokenization\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "start_time = datetime.now()\n",
        "bert_tokens = bert_tokenizer.tokenize(text)\n",
        "bert_time = datetime.now() - start_time\n",
        "\n",
        "# XLNetTokenizer Tokenization\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "start_time = datetime.now()\n",
        "xlnet_tokens = xlnet_tokenizer.tokenize(text)\n",
        "xlnet_time = datetime.now() - start_time\n",
        "\n",
        "# Display tokens, time taken for each tokenizer, and token frequencies\n",
        "print(f\"NLTK Tokens: {nltk_tokens}\\nTime Taken: {nltk_time} seconds\\n\")\n",
        "show_frequencies(nltk_tokens, \"NLTK\")\n",
        "\n",
        "print(f\"SpaCy Tokens: {spacy_tokens}\\nTime Taken: {spacy_time} seconds\\n\")\n",
        "show_frequencies(spacy_tokens, \"SpaCy\")\n",
        "\n",
        "print(f\"Bert Tokens: {bert_tokens}\\nTime Taken: {bert_time} seconds\\n\")\n",
        "show_frequencies(bert_tokens, \"Bert\")\n",
        "\n",
        "print(f\"XLNet Tokens: {xlnet_tokens}\\nTime Taken: {xlnet_time} seconds\\n\")\n",
        "show_frequencies(xlnet_tokens, \"XLNet\")"
      ],
      "execution_count": 19
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "prev_pub_hash": "e2e4ac52377879aab7a3473f68f22b37f91497ed67c9b2465cec45462c28fdec",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
