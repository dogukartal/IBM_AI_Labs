{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogukartal/IBM_AI_Labs/blob/main/Generative%20AI%20Engineering%20and%20Fine-Tuning%20Transformers/HuggingFace_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd5f23dd-9f5d-49e3-af93-5bc28b0d083d"
      },
      "source": [
        "# **Loading Models and Inference with Hugging Face Inferences**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81220bb8-b961-420c-88e4-224e5fc4eaf1"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39c42069-651a-4f3c-af6d-2a5778e20632"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a727045a-3fd7-489d-9e0c-43082c7497e9"
      },
      "source": [
        "# Text classification with DistilBERT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c3e90aa-a1e5-42d0-b864-ec99b2874695"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07525832-402f-4aaf-aae0-17eb39c4da6b",
        "outputId": "386d0eb6-9381-4f49-d880-36814a0ff78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,\n",
            "          2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "text = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "print(inputs)"
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00454b6e-69f3-4087-9ce8-c8218864de09"
      },
      "outputs": [],
      "source": [
        "# Perform inference\n",
        "with torch.no_grad(): # Disable gradient calculation since gradients are not used in inference\n",
        "    outputs = model(**inputs) # Unpack a dictionary of key arguments\n",
        "\n",
        "# model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])"
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6f5f062-63a0-418f-b1ad-509d20abaff5",
        "outputId": "20e89b36-eb39-475a-d31a-029d7221c3f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "logits = outputs.logits # Raw, unnormalized predictions of the model\n",
        "logits.shape"
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQid_qqJQogI",
        "outputId": "6cd60686-4dbf-470f-d13e-19da65c93a99"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.9954,  4.3336]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b4431a7-f981-4727-8558-cb373506ab29",
        "outputId": "2861bc5d-ae6c-4db7-918c-8f5a830beff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: POSITIVE\n"
          ]
        }
      ],
      "source": [
        "# Post-process\n",
        "# Convert logits to probabilities\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# Map the predicted class to the label\n",
        "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
        "predicted_label = labels[predicted_class]\n",
        "\n",
        "print(f\"Predicted label: {predicted_label}\")"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6016f7be-074b-448d-8b57-414f01aa03a6"
      },
      "source": [
        "# Text generation with GPT-2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "665b7203-2e66-4de1-9e83-b4c2ec03b820"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f25ba2d5-acc5-4c72-ab9c-bac5affa1590",
        "outputId": "509a2668-ca7c-4b15-dac5-501f3adb9df7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Prompt\n",
        "prompt = \"Once upon a time\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b8a1693-70ef-4198-bb41-1e0ce8ed6a93",
        "outputId": "828c3f1c-b7fa-4fd6-e721-887bf76a7b82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,\n",
              "         8737,  290, 1049, 3514,   13,  383,  995,  373,  257, 1295,  286, 1049,\n",
              "         3514,   11,  290,  262,  995,  373,  257, 1295,  286, 1049, 3514,   13,\n",
              "          383,  995,  373,  257, 1295,  286, 1049, 3514,   11,  290,  262,  995,\n",
              "          373,  257]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Generate text\n",
        "output_ids = model.generate(\n",
        "    inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "output_ids"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99d7eef8-b64c-40f8-918a-16aa3150f9d2"
      },
      "source": [
        "or\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "outputs"
      ],
      "metadata": {
        "id": "ogT-C0OWRIXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cac66de-4c4d-4ec8-8cb0-11251176f3e9",
        "outputId": "9ad4f433-2d9c-40f6-c7ec-6fad6f69b131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
          ]
        }
      ],
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70eaf1ba-13ed-4c3e-8b0d-6a3953b00994"
      },
      "source": [
        "# Hugging Face `pipeline()` function\n",
        "\n",
        "## Definition\n",
        "\n",
        "```python\n",
        "transformers.pipeline(\n",
        "    task: str,\n",
        "    model: Optional = None,\n",
        "    config: Optional = None,\n",
        "    tokenizer: Optional = None,\n",
        "    feature_extractor: Optional = None,\n",
        "    framework: Optional = None,\n",
        "    revision: str = 'main',\n",
        "    use_fast: bool = True,\n",
        "    model_kwargs: Dict[str, Any] = None,\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "## Parameters\n",
        "\n",
        "- **task**: `str`\n",
        "  - The task to perform, such as \"text-classification\", \"text-generation\", \"question-answering\", etc.\n",
        "  - Example: `\"text-classification\"`\n",
        "\n",
        "- **model**: `Optional`\n",
        "  - The model to use. This can be a string (model identifier from Hugging Face model hub), a path to a directory containing model files, or a pre-loaded model instance.\n",
        "  - Example: `\"distilbert-base-uncased-finetuned-sst-2-english\"`\n",
        "\n",
        "- **config**: `Optional`\n",
        "  - The configuration to use. This can be a string, a path to a directory, or a pre-loaded config object.\n",
        "  - Example: `{\"output_attentions\": True}`\n",
        "\n",
        "- **tokenizer**: `Optional`\n",
        "  - The tokenizer to use. This can be a string, a path to a directory, or a pre-loaded tokenizer instance.\n",
        "  - Example: `\"bert-base-uncased\"`\n",
        "\n",
        "- **feature_extractor**: `Optional`\n",
        "  - The feature extractor to use for tasks that require it (e.g., image processing).\n",
        "  - Example: `\"facebook/detectron2\"`\n",
        "\n",
        "- **framework**: `Optional`\n",
        "  - The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. If not specified, it will be inferred.\n",
        "  - Example: `\"pt\"`\n",
        "\n",
        "- **revision**: `str`, default `'main'`\n",
        "  - The specific model version to use (branch, tag, or commit hash).\n",
        "  - Example: `\"v1.0\"`\n",
        "\n",
        "- **use_fast**: `bool`, default `True`\n",
        "  - Whether to use the fast version of the tokenizer if available.\n",
        "  - Example: `True`\n",
        "\n",
        "- **model_kwargs**: `Dict[str, Any]`, default `None`\n",
        "  - Additional keyword arguments passed to the model during initialization.\n",
        "  - Example: `{\"output_hidden_states\": True}`\n",
        "\n",
        "- **kwargs**: `Any`\n",
        "  - Additional keyword arguments passed to the pipeline components.\n",
        "\n",
        "## Task types\n",
        "\n",
        "The `pipeline()` function supports a wide range of NLP tasks. Here are some of the common tasks:\n",
        "\n",
        "1. **Text Classification**: `text-classification`\n",
        "   - **Purpose**: Classify text into predefined categories.\n",
        "   - **Use Cases**: Sentiment analysis, spam detection, topic classification.\n",
        "\n",
        "2. **Text Generation**: `text-generation`\n",
        "   - **Purpose**: Generate coherent text based on a given prompt.\n",
        "   - **Use Cases**: Creative writing, dialogue generation, story completion.\n",
        "\n",
        "3. **Question Answering**: `question-answering`\n",
        "   - **Purpose**: Answer questions based on a given context.\n",
        "   - **Use Cases**: Building Q&A systems, information retrieval from documents.\n",
        "\n",
        "4. **Named Entity Recognition (NER)**: `ner` (or `token-classification`)\n",
        "   - **Purpose**: Identify and classify named entities (like people, organizations, locations) in text.\n",
        "   - **Use Cases**: Extracting structured information from unstructured text.\n",
        "\n",
        "5. **Summarization**: `summarization`\n",
        "   - **Purpose**: Summarize long pieces of text into shorter, coherent summaries.\n",
        "   - **Use Cases**: Document summarization, news summarization.\n",
        "\n",
        "6. **Translation**: `translation_xx_to_yy` (e.g., `translation_en_to_fr`)\n",
        "   - **Purpose**: Translate text from one language to another.\n",
        "   - **Use Cases**: Language translation, multilingual applications.\n",
        "\n",
        "7. **Fill-Mask**: `fill-mask`\n",
        "   - **Purpose**: Predict masked words in a sentence (useful for masked language modeling).\n",
        "   - **Use Cases**: Language modeling tasks, understanding model predictions.\n",
        "\n",
        "8. **Zero-Shot Classification**: `zero-shot-classification`\n",
        "   - **Purpose**: Classify text into categories without needing training data for those categories.\n",
        "   - **Use Cases**: Flexible and adaptable classification tasks.\n",
        "\n",
        "9. **Feature Extraction**: `feature-extraction`\n",
        "   - **Purpose**: Extract hidden state features from text.\n",
        "   - **Use Cases**: Downstream tasks requiring text representations, such as clustering, similarity, or further custom model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "zYWMmzutR4bd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82791926-b4c2-4326-b0b3-3b80d8856156",
        "outputId": "73acdb39-3524-4675-8e3f-4c064b6762e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
          ]
        }
      ],
      "source": [
        "## Text Classification\n",
        "# Load a general text classification model\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Classify a sample text\n",
        "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
        "print(result)"
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29db8243-01e6-4b86-aa02-83a77f155dfc"
      },
      "source": [
        "#### Output\n",
        "\n",
        "The output will be a list of dictionaries, where each dictionary contains:\n",
        "\n",
        "- `label`: The predicted label (e.g., \"POSITIVE\" or \"NEGATIVE\").\n",
        "- `score`: The confidence score for the prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "A41EvRcoR3Es"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a91542d7-ba7e-468a-9a62-47cc3560cad8",
        "outputId": "48b45a88-d35e-482d-ce77-6e985cc71ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'fr', 'score': 0.9934879541397095}]\n"
          ]
        }
      ],
      "source": [
        "## Language Detection\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
        "result = classifier(\"Bonjour, comment ça va?\")\n",
        "print(result)"
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b884ff30-9c64-4662-a4bb-f8b721a6f2ab"
      },
      "source": [
        "#### Output\n",
        "The output will be a list of dictionaries, where each dictionary contains:\n",
        "\n",
        "- `label`: The predicted language label (e.g., \"fr\" for French).\n",
        "- `score`: The confidence score for the prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "zlMPVp5tR1Ld"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f2a805d-59e1-411a-a94d-5ddb7e6f5a62",
        "outputId": "877d6bcf-de7e-4952-e015-32125b4fb4d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "## Text Generation\n",
        "# Initialize the text generation pipeline with GPT-2\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")"
      ],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "def8b20a-38da-40e2-bc25-728e499da6ae",
        "outputId": "fadd0c61-c956-4262-a90a-af296795e876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time I tried making a movie, and when I went to the store and bought several of those it was a really bad time and he couldn't even bring back the original DVD when I went back to the cinema and started it again in\n"
          ]
        }
      ],
      "source": [
        "# Generate text based on a given prompt\n",
        "prompt = \"Once upon a time\"\n",
        "result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(result[0]['generated_text'])"
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a719290-1d94-4f57-a10e-5a0b78c9543a"
      },
      "source": [
        "#### Output\n",
        "The output will be a list of dictionaries, where each dictionary contains:\n",
        "\n",
        "- `generated_text`: The generated text based on the input prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69e4d741-d282-4d9a-af7c-b27b5c55e361"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "654f0659-e769-410c-9e08-3261ac898dc5"
      },
      "outputs": [],
      "source": [
        "## Text Generation Using T5\n",
        "# Initialize the text generation pipeline with T5\n",
        "generator = pipeline(\"text2text-generation\", model=\"t5-small\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f018e082-76ff-4966-a2e9-c435b180980e",
        "outputId": "8954db5b-cd47-49b9-9b0f-2a7e0b97df9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment êtes-vous?\n"
          ]
        }
      ],
      "source": [
        "# Generate text based on a given prompt\n",
        "prompt = \"translate English to French: How are you?\"\n",
        "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Print the generated text\n",
        "print(result[0]['generated_text'])"
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "277f3e27-3635-44fc-8b5b-4cb9f2a1ee71"
      },
      "source": [
        "#### Output\n",
        "The output will be a list of dictionaries, where each dictionary contains:\n",
        "\n",
        "- `generated_text`: The generated text based on the input prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "8AsIBnp_STWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Filling MASK Tokens\n",
        "# Initialize the fill-mask pipeline with BERT\n",
        "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "# Generate text by filling in the masked token\n",
        "prompt = \"The capital of France is [MASK].\"\n",
        "result = fill_mask(prompt)\n",
        "\n",
        "# Print the generated text\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-DjJhWoSSuT",
        "outputId": "6113597f-7102-4a7c-addf-d676aadf3ddb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.4167894423007965, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141634821891785, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.06339266151189804, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}, {'score': 0.04444744810461998, 'token': 16766, 'token_str': 'marseille', 'sequence': 'the capital of france is marseille.'}, {'score': 0.030297260731458664, 'token': 7562, 'token_str': 'tours', 'sequence': 'the capital of france is tours.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bab5162-3cf2-4312-9db3-63a84eac86c2"
      },
      "source": [
        "## Benefits of using `pipeline()`\n",
        "\n",
        "- **Reduced Boilerplate Code**: Simplifies the code required to perform NLP tasks.\n",
        "- **Improved Readability**: Makes code more readable and expressive.\n",
        "- **Time Efficiency**: Saves time by handling model loading, tokenization, inference, and post-processing automatically.\n",
        "- **Consistent API**: Provides a consistent API across different tasks, allowing for easy experimentation and quick prototyping.\n",
        "- **Automatic Framework Handling**: Automatically handles the underlying framework (TensorFlow or PyTorch).\n",
        "\n",
        "## When to use `pipeline()`\n",
        "\n",
        "- **Quick Prototyping**: When you need to quickly prototype an NLP application or experiment with different models.\n",
        "- **Simple Tasks**: When performing simple or common NLP tasks that are well-supported by the `pipeline()` function.\n",
        "- **Deployment**: When deploying NLP models in environments where simplicity and ease of use are crucial.\n",
        "\n",
        "## When to avoid `pipeline()`\n",
        "\n",
        "- **Custom Tasks**: When you need to perform highly customized tasks that are not well-supported by the `pipeline()` function.\n",
        "- **Performance Optimization**: When you need fine-grained control over the model and tokenization process for performance optimization or specific use cases.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "prev_pub_hash": "580f657bfe790433f8d5be264eaea1f8bbd0547d40345696853f98af925da491",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}