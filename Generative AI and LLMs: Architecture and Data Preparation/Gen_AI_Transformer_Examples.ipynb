{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogukartal/IBM_AI_Labs/blob/main/Generative%20AI%20and%20LLMs%3A%20Architecture%20and%20Data%20Preparation/Generative%20AI%20Architecture/Gen_ai_Transformer_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7001e56-a142-46be-9a1b-81dba89a890b"
      },
      "source": [
        "# Huggingface Transformers\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cc2d869-fe49-4006-bca9-b52d2b381b9b"
      },
      "outputs": [],
      "source": [
        "!pip install -qq tensorflow\n",
        "!pip install -qq transformers\n",
        "!pip install sentencepiece\n",
        "!pip install torch==2.0.1\n",
        "!pip install torchtext==0.15.2"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -U"
      ],
      "metadata": {
        "id": "_m2rCF0xYbFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d2fc32f-bfa0-4b99-bb71-ff04e56cd944"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"facebook/blenderbot-400M-distill\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e52ed93-6640-4301-8b48-ff05cde37c5a",
        "outputId": "7287e4c9-3687-47ff-af6b-0707a793d345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hey What is up?\n",
            "Chatbot: Not much, just got back from a long day at work. How are you doing today?\n",
            "You: I am fine thank you!\n",
            "Chatbot: I am glad to hear that. I hope you have a great rest of your day.\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Define the chat function\n",
        "def chat_with_bot():\n",
        "    while True:\n",
        "        input_text = input(\"You: \")\n",
        "\n",
        "        # Exit conditions\n",
        "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Tokenize input and generate response\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "        outputs = model.generate(inputs, max_new_tokens=150)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        print(\"Chatbot:\", response)\n",
        "\n",
        "chat_with_bot()"
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a1394b1-07b0-45c2-a6b3-ab43c0940fb9"
      },
      "outputs": [],
      "source": [
        "import sentencepiece\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f7d9ca9-2843-45d9-b87a-417918300a15",
        "outputId": "2ea5ee1e-7f5d-4144-8dfe-05018ec381f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: I need to finish my homework.\n",
            "Chatbot: I need to finish my homework.\n",
            "You: What?\n",
            "Chatbot: a syringe\n",
            "You: Okay\n",
            "Chatbot: Okay, so I'm going to be a little bit more sarcastic about this. I'm going to be a little bit more sarcastic about this. I'm going to be a little bit more sarcastic about this. I'm going to be a little bit more sarcastic about this. I'm going to be a little bit more sarcastic about this. I'm going to be a little bit more sarcastic about this. I'm going to be a little bit more sarcastic about this. I'm going to be a little bit more sarcastic about this. I'm\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "def chat_with_another_bot():\n",
        "    while True:\n",
        "        input_text = input(\"You: \")\n",
        "\n",
        "        # Exit conditions\n",
        "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Tokenize input and generate response\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "        outputs = model.generate(inputs, max_new_tokens=150)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        print(\"Chatbot:\", response)\n",
        "\n",
        "chat_with_another_bot()"
      ],
      "execution_count": 4
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "prev_pub_hash": "5c5bec06088ad96b1ecbe1871624b6f4fcc99062c9772bf4e6ad46b1d556c1b8",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
