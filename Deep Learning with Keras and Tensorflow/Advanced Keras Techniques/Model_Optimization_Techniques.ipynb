{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMS8MpsqPH7b4lR0VkAPwG+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dogukartal/IBM_AI_Labs/blob/main/Deep%20Learning%20with%20Keras%20and%20Tensorflow/Advanced%20Keras%20Techniques/Model_Optimization_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Optimization\n",
        "---"
      ],
      "metadata": {
        "id": "eikyMzPmmASr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight Initialization"
      ],
      "metadata": {
        "id": "mB1VOJPomDnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu', kernel_initializer=HeNormal()), #He initialization\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "KBoJe-XnmcVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate Scheduling\n"
      ],
      "metadata": {
        "id": "INw9A9dxmfYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = X_train.astype('float32') / 255\n",
        "x_val = X_val.astype('float32') / 255\n",
        "\n",
        "y_train = x_train.reshape(-1, 28, 28)\n",
        "x_val = x_val.reshape(-1, 28, 28)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "\n",
        "model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20, callbacks=[scheduler])"
      ],
      "metadata": {
        "id": "kznFX9GYmhpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization\n"
      ],
      "metadata": {
        "id": "qIaTg24MnnSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "gj6kcTA9no_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixed Precision Training"
      ],
      "metadata": {
        "id": "q2fqTdVHnuBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)"
      ],
      "metadata": {
        "id": "k3XmoO51nvnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Pruning"
      ],
      "metadata": {
        "id": "90WRaTmen0Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "pruning_params = {\n",
        "    'pruning_schedule':\n",
        "      tfmot.sparsity.keras.PolynomialDecay(\n",
        "          initial_sparsity=0.50,\n",
        "          final_sparsity=0.80,\n",
        "          begin_step=2000,\n",
        "          end_step=4000)}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
      ],
      "metadata": {
        "id": "lCIoX9JGn1XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization"
      ],
      "metadata": {
        "id": "tPNzVEiboITf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "72DEO-JoogmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Distillation"
      ],
      "metadata": {
        "id": "1mjiiF16sNek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import numpy as np\n",
        "\n",
        "def distillation_loss(teacher_logits, student_logits, temperature):\n",
        "    teacher_probs = tf.nn.softmax(teacher_logits / temperature)\n",
        "    student_probs = tf.nn.softmax(student_logits / temperature)\n",
        "    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(teacher_probs, student_probs))\n",
        "\n",
        "def train_student(student_model, teacher_model, x_train, y_train, x_val, y_val, batch_size=32, epochs=10, temperature=3):\n",
        "    for epoch in range(epochs):\n",
        "      num_batches = len(x_train) // batch_size\n",
        "      for batch_idx in range(num_batches):\n",
        "        x_batch = x_train[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
        "        y_batch = y_train[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
        "\n",
        "        teacher_logits = teacher_model(x_batch)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "          student_logits = student_model(x_batch)\n",
        "          loss = distillation_loss(teacher_logits, student_logits, temperature)\n",
        "\n",
        "        gradients = tape.gradient(loss, student_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n",
        "\n",
        "      if batch_idx % 100 == 0:\n",
        "        print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{num_batches}, Loss: {loss.numpy()}\")\n",
        "\n",
        "teacher_model = models.Sequential([\n",
        "    layers.Input(shape=(28, 28)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "teacher_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "teacher_model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        "# Assume the teacher model is alerady trained\n",
        "\n",
        "student_model = models.Sequential([\n",
        "    layers.Input(shape=(28, 28)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "student_model.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RuWZpVlVsQBp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}